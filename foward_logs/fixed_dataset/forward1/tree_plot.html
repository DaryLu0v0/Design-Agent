<!doctype html>
<html lang="en"> 
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#F2F0E7";
const accentCol = "#fd4578";

hljs.initHighlightingOnLoad();

const updateTargetDims = () => {
  // width is max-width of `.contentContainer` - its padding
  // return [min(windowWidth, 900 - 80), 700]
  return [windowWidth * (1 / 2), windowHeight];
};

const setCodeAndPlan = (code, plan) => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    // codeElm.innerText = code;
    codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    // planElm.innerText = plan.trim();
    planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
  }
};

windowResized = () => {
  resizeCanvas(...updateTargetDims());
  awaitingPostResizeOps = true;
};

const animEase = (t) => 1 - (1 - Math.min(t, 1.0)) ** 5;

// ---- global constants ----

const globalAnimSpeed = 1.1;
const scaleFactor = 0.57;

// ---- global vars ----

let globalTime = 0;
let manualSelection = false;

let currentElemInd = 0;

let treeStructData = {"edges": [[1, 5], [1, 8], [1, 11], [1, 9], [1, 7], [1, 10], [3, 6], [11, 12], [12, 14], [12, 13], [14, 15], [15, 16], [15, 19], [15, 17], [15, 18], [19, 20], [20, 21], [21, 22], [22, 23], [23, 24], [24, 25], [24, 29], [24, 31], [24, 26], [24, 32], [24, 27], [24, 28], [24, 30], [32, 42], [32, 49], [32, 47], [32, 33], [32, 51], [32, 48], [32, 43], [32, 39], [32, 45], [32, 46], [32, 50], [32, 41], [32, 38], [32, 40], [32, 35], [32, 36], [32, 44], [32, 34], [36, 37], [51, 52], [51, 54], [51, 55], [51, 53], [55, 73], [55, 71], [55, 59], [55, 70], [55, 84], [55, 88], [55, 72], [55, 66], [55, 79], [55, 56], [55, 57], [55, 68], [55, 61], [55, 80], [55, 67], [55, 65], [55, 76], [55, 74], [55, 63], [55, 81], [55, 64], [55, 86], [55, 60], [55, 62], [55, 58], [55, 78], [55, 75], [55, 69], [55, 85], [55, 83], [55, 87], [55, 77], [55, 82]], "layout": [[0.041095890410958895, 0.0], [0.06849315068493149, 0.0], [0.0958904109589041, 0.0], [0.1643835616438356, 0.0], [0.1917808219178082, 0.0], [0.0, 0.0714285714285714], [0.1643835616438356, 0.0714285714285714], [0.02739726027397259, 0.0714285714285714], [0.054794520547945195, 0.0714285714285714], [0.08219178082191779, 0.0714285714285714], [0.1095890410958904, 0.0714285714285714], [0.136986301369863, 0.0714285714285714], [0.136986301369863, 0.1428571428571429], [0.1232876712328767, 0.2142857142857143], [0.1506849315068493, 0.2142857142857143], [0.1506849315068493, 0.2857142857142857], [0.1095890410958904, 0.3571428571428571], [0.136986301369863, 0.3571428571428571], [0.1643835616438356, 0.3571428571428571], [0.1917808219178082, 0.3571428571428571], [0.1917808219178082, 0.4285714285714286], [0.1917808219178082, 0.5], [0.1917808219178082, 0.5714285714285714], [0.1917808219178082, 0.6428571428571428], [0.1917808219178082, 0.7142857142857143], [0.0958904109589041, 0.7857142857142857], [0.1232876712328767, 0.7857142857142857], [0.1506849315068493, 0.7857142857142857], [0.1780821917808219, 0.7857142857142857], [0.2054794520547945, 0.7857142857142857], [0.2328767123287671, 0.7857142857142857], [0.2602739726027397, 0.7857142857142857], [0.2876712328767123, 0.7857142857142857], [0.0547945205479452, 0.8571428571428572], [0.0821917808219178, 0.8571428571428572], [0.1095890410958904, 0.8571428571428572], [0.136986301369863, 0.8571428571428572], [0.136986301369863, 0.9285714285714286], [0.1643835616438356, 0.8571428571428572], [0.1917808219178082, 0.8571428571428572], [0.2191780821917808, 0.8571428571428572], [0.2465753424657534, 0.8571428571428572], [0.273972602739726, 0.8571428571428572], [0.3013698630136986, 0.8571428571428572], [0.3287671232876712, 0.8571428571428572], [0.3561643835616438, 0.8571428571428572], [0.3835616438356164, 0.8571428571428572], [0.410958904109589, 0.8571428571428572], [0.4383561643835616, 0.8571428571428572], [0.4657534246575342, 0.8571428571428572], [0.4931506849315068, 0.8571428571428572], [0.5205479452054794, 0.8571428571428572], [0.4794520547945205, 0.9285714285714286], [0.5068493150684932, 0.9285714285714286], [0.5342465753424658, 0.9285714285714286], [0.5616438356164384, 0.9285714285714286], [0.12328767123287666, 1.0], [0.15068493150684928, 1.0], [0.17808219178082188, 1.0], [0.20547945205479448, 1.0], [0.23287671232876708, 1.0], [0.26027397260273966, 1.0], [0.28767123287671226, 1.0], [0.31506849315068486, 1.0], [0.34246575342465746, 1.0], [0.36986301369863006, 1.0], [0.3972602739726027, 1.0], [0.4246575342465753, 1.0], [0.4520547945205479, 1.0], [0.4794520547945205, 1.0], [0.5068493150684932, 1.0], [0.5342465753424658, 1.0], [0.5616438356164384, 1.0], [0.589041095890411, 1.0], [0.6164383561643836, 1.0], [0.6438356164383562, 1.0], [0.6712328767123288, 1.0], [0.6986301369863014, 1.0], [0.726027397260274, 1.0], [0.7534246575342466, 1.0], [0.7808219178082192, 1.0], [0.8082191780821918, 1.0], [0.8356164383561644, 1.0], [0.863013698630137, 1.0], [0.8904109589041096, 1.0], [0.9178082191780822, 1.0], [0.9452054794520548, 1.0], [0.9726027397260274, 1.0], [1.0, 1.0]], "plan": ["We will build a straightforward feed-forward neural network with several fully\nconnected layers to map the 14D geometry inputs to the 2001D spectrum. We will\ntrain this model using the provided training data, track the mean squared error\n(MSE) on the validation set, and then evaluate on both validation and test sets.\nFinally, we will save predictions on the test set to a CSV file for submission.", "A residual MLP can help capture non-linear relationships while preserving useful\ninformation through skip connections. We first embed the 14D geometry input with\na linear layer, then pass it through several residual blocks that each contain\nfully connected layers and skip connections. Finally, we produce a 2001D output\nfor the spectrum and train using MSE loss. We evaluate on the hold-out\nvalidation set and report the MSE score. Lastly, we infer on the test set and\nsave the predictions to a submission.csv file.", "We propose a compact 1D CNN that treats the 14 geometry features as a length-14\nsequence, processes them with a few convolutional layers, and outputs the\n2001-dimensional spectrum. We will use 5-fold cross-validation on the training\nset to verify stability and then train on the full training data, evaluating on\nthe hold-out validation set and reporting MSE. Finally, we will generate the\ntest predictions and save them to submission.csv. This straightforward approach\nshould capture local dependencies in the geometry inputs while still being\ncomputationally efficient.", "We will build a minimal transformer-based regressor by embedding the 14D\ngeometry features into a modest hidden dimension, then applying a small self-\nattention mechanism to capture interactions among the input dimensions before\npredicting the 2001D spectrum. The model is trained with MSE loss and evaluated\non the validation set to monitor performance. Finally, we generate predictions\non the test set and save them to submission.csv. This approach should provide a\nfresh architecture while keeping the pipeline straightforward.", "We will treat the 14 geometry features as a short sequence of tokens and apply a\nsimple Transformer encoder to extract a latent representation, then map it to\n2001 spectrum outputs. We will conduct a brief 5-fold cross-validation on the\ntraining set to verify performance stability, then retrain on the entire\ntraining set and evaluate on the validation set to obtain the final MSE.\nFinally, we will predict on the test set and save these predictions to\n\"submission.csv\". This approach should capture interactions among the geometry\ninputs while maintaining a fairly compact model architecture.", "We will add a dropout layer inside each residual block, which should help\nprevent overfitting by randomly zeroing some neuron outputs during training.\nThis can improve generalization without substantially changing the model\u2019s\narchitecture or training procedure. The layer will have a dropout probability of\n0.2 and will be placed between the two linear layers in the residual blocks,\nensuring that some activation values are dropped at each step.", "One way to fix the error is to reshape the geometry inputs so that each of the\n14 geometry parameters is treated as a separate \"token\" for the transformer\ninput, rather than embedding the entire 14D vector as a single token.\nConcretely, we can use a linear layer from 1 \u2192 embed_dim and then reshape the\ninput from [batch_size, 14] to [batch_size, 14, 1] so the transformer receives\n[14, batch_size, embed_dim]. Then, after the transformer, we can mean-pool\nacross the 14 \"token\" dimension before the final linear regression head. This\nensures the expected embedding dimension of 64 is correctly maintained\nthroughout the transformer's forward pass.", "We can improve the residual MLP by adding batch normalization in each residual\nblock. This helps stabilize and speed up the training by normalizing\nintermediate activations, potentially reducing internal covariate shift.\nSpecifically, we insert BatchNorm1d layers after each linear layer, allowing for\nbetter gradient flow and potentially better generalization.", "One way to improve the model is to add a small weight decay term to the Adam\noptimizer, which will regularize the network parameters and help reduce\noverfitting. This involves adding an L2 penalty on the weights, encouraging the\nmodel to learn more robust representations. We will keep the same architecture\nand hyperparameters otherwise, and observe whether this additional term lowers\nthe validation MSE.", "A simple way to further improve performance is to apply a learning rate\nscheduler that reduces the learning rate after a few epochs, helping the model\nescape potential plateaus and improving convergence. Specifically, we can use a\nStepLR scheduler that halves the learning rate every five epochs. This small\naddition often yields better final performance without major modifications to\nthe existing code.", "One way to improve the solution is to replace ReLU with ELU activation functions\nin the residual blocks and the input layer. ELU can avoid issues with zero\ngradients that sometimes occur with ReLU, potentially improving convergence. We\nwill keep the rest of the architecture and hyperparameters the same to isolate\nthe effect of this change.", "We can increase the network\u2019s capacity by enlarging the hidden dimension and\nadding an additional residual block. This gives the model more parameters and\nshould improve its ability to capture complex relationships in the data.\nSpecifically, we will change the hidden dimension from 64 to 128 and the number\nof residual blocks from 3 to 4, keeping the rest of the pipeline unchanged. The\nintuition is that a larger network can learn more intricate features,\npotentially reducing the MSE on the validation set.", "One way to improve the solution is to implement early stopping, which will\nreduce overfitting and potentially speed up training. Specifically, we will\nmonitor the validation loss at each epoch and stop training if it stops\nimproving for a certain number of epochs (patience). We will keep the same\narchitecture and hyperparameters otherwise, ensuring that only the stopping\ncriterion changes. This should help us obtain a more robust final model without\nadditional complexity.", "One way to improve the solution is to apply gradient clipping during training,\nwhich helps avoid exploding gradients by limiting the norm of the gradients. We\nwill clip the gradients in each training step to a maximum norm of 1.0. This is\na simple addition that can stabilize training, potentially leading to better\nconvergence.", "One way to improve the solution is to integrate a ReduceLROnPlateau scheduler,\nwhich will dynamically reduce the learning rate when the validation MSE\nplateaus. This can encourage the model to escape suboptimal local minima and\nconverge more effectively. We will keep the same architecture, hyperparameters,\nand early stopping configuration, adding only this scheduler.", "Using the AdamW optimizer rather than traditional Adam can improve\ngeneralization by decoupling weight decay from the gradient-based updates. We\nwill keep the existing architecture, learning rate scheduling, and early\nstopping unchanged. This is a minimal yet effective change that may yield a\nlower validation MSE.", "One way to improve the solution is to use a cosine annealing warm restarts\nscheduler instead of ReduceLROnPlateau. Cosine annealing can help the model\nescape sharp local minima by periodically restarting the learning rate,\npotentially improving generalization. We will keep the same residual MLP\narchitecture, early stopping, and optimizer settings to isolate the impact of\nthe new scheduler. This change should be relatively straightforward to\nincorporate into the existing training loop.", "One way to improve the solution is to replace the current ReduceLROnPlateau\nscheduler with a OneCycleLR scheduler, which increases the learning rate from a\nlower bound to a higher bound and then decreases it again, often boosting\nperformance on many tasks. We will set the maximum learning rate to the same\nvalue (1e-3) and run this schedule over the entire training process. This helps\nthe model explore larger learning rates early on and converge more efficiently.", "One way to improve the solution is to replace the ReduceLROnPlateau scheduler\nwith a OneCycleLR scheduler. This approach linearly increases the learning rate\nto the maximum rate and then decreases it back, which frequently yields better\nconvergence and model performance. We will keep the rest of the architecture and\nhyperparameters the same, ensuring we isolate the effect of this scheduler\nchange. This is a small yet effective improvement that can be implemented by\nstepping the scheduler once per batch.", "One way to further improve performance is to train an ensemble of models with\ndifferent random seeds and average their predictions. This approach reduces\nvariance and often yields better generalization, especially for high-dimensional\noutputs. We will keep the same architecture and hyperparameters, train three\nmodels with different initializations, and then average their validation and\ntest predictions. Only this small ensemble change should help reduce the final\nMSE.", "One way to further improve performance is to increase the number of models in\nthe ensemble, reducing variance and providing more robust predictions. Instead\nof three random seeds, we will train five models, each with a different seed,\nand average their outputs. This ensemble approach typically leads to lower MSE\non the validation set by leveraging the diverse representations learned by each\nmodel.", "One potential improvement is to replace ReLU with the Swish activation function\nin the residual blocks, which can lead to smoother gradients and improved\nconvergence. Swish often outperforms ReLU in certain deep learning tasks, so we\nwill define a custom Swish layer and use it throughout our residual MLP. This\nchange is straightforward to implement, keeping the rest of the pipeline\nunmodified. We will then train and evaluate as before, and observe whether this\nyields a lower validation MSE.", "One way to improve this solution is to maintain an exponential moving average\n(EMA) of the model\u2019s weights during training. By updating a \u201cshadow\u201d copy of the\nparameters more smoothly, the EMA model often outperforms the raw model on the\nvalidation test, reducing variance and improving generalization. We will\nimplement an EMA class and update it after each optimizer step, and then we will\nuse these averaged weights at validation time and for the final predictions.", "One way to further improve performance is to increase the hidden dimension of\neach residual block, giving the network more capacity to learn complex\nrelationships. Specifically, we can raise the hidden dimension from 128 to 256.\nThis is a straightforward and isolated change that can boost representational\npower while keeping the rest of the training pipeline intact.", "One way to further improve this solution is to increase the size of the ensemble\nfrom 5 models to 10 models. Aggregating predictions from more models can reduce\nvariance and improve generalization. We will keep the same architecture, data\nprocessing, and training pipeline, and simply train ten models with different\nseeds. The final predictions will be the average of these ten models.", "One way to improve this solution is to integrate batch normalization into the\nresidual blocks. By normalizing the intermediate features, we can stabilize and\naccelerate training and potentially improve generalization. Specifically, we\nwill insert BatchNorm1d layers after each linear layer within each residual\nblock. This straightforward modification can often reduce internal covariate\nshift and help the network converge more smoothly.", "A simple yet effective improvement is to add dropout to each residual block to\nhelp prevent overfitting. We will insert a Dropout layer (e.g., p=0.2) in the\nforward pass of each block, after the first linear + activation step. This\nrandomizes neuron outputs during training, leading to more robust\nrepresentations and potentially lower MSE.", "One way to improve this solution is to add batch normalization to each residual\nblock, which can help stabilize and speed up training. Specifically, we will\ninsert batch normalization layers after each linear layer in the residual block.\nThis normalization reduces internal covariate shift and often leads to better\ngeneralization performance without introducing major complexity or overhead. We\nkeep the rest of the architecture and hyperparameters the same to isolate the\neffect of batch normalization.", "One way to improve the solution is to wrap the current AdamW optimizer with the\nLookahead method, which can help stabilize and potentially speed up convergence\nby periodically \u201clooking ahead\u201d to a slow weight copy. This refinement can lead\nto more robust updates combined with the benefits of AdamW. We will keep the\nsame architecture, EMA, and training loop, just replacing the standard optimizer\nwith a Lookahead wrapper. This is a small, targeted improvement that can be\nevaluated by its effect on the validation MSE.", "One way to improve the solution is to replace the ReduceLROnPlateau scheduler\nwith the OneCycleLR scheduler, which can often help the model converge faster\nand achieve better results by warming up and then annealing the learning rate\nover each epoch. We'll keep the architecture, optimizer, EMA, and early stopping\nlogic unchanged. The only difference will be using OneCycleLR (stepped once per\nbatch) instead of ReduceLROnPlateau. This should encourage more robust\noptimization and potentially lower the validation MSE.", "One straightforward improvement is to wrap the existing AdamW optimizer with\nLookahead, which can improve convergence stability and final performance by\nperiodically \u201clooking ahead\u201d to a slower-moving set of parameters. This change\nis easy to implement and leaves the rest of the architecture, scheduler, and EMA\nlogic intact. By averaging weights over shorter internal steps (AdamW) and\nlonger steps (Lookahead), the model may escape local minima more effectively. We\nsimply define a Lookahead class and instantiate it with the base optimizer, then\nproceed with training exactly as before.", "One way to improve the solution is to wrap the existing AdamW optimizer with\nLookahead, which can stabilize and sometimes improve convergence by periodically\nmerging the \u201cfast\u201d weights into a \u201cslow\u201d weight copy. This ensures that the\nmodel does not drift too far from a stable parameter set, potentially yielding\nbetter final performance. Everything else (model architecture, EMA, and training\nloop) remains the same, so we can isolate the effect of Lookahead.", "One way to improve this solution is to apply gradient clipping during training,\nwhich limits the maximum norm of the gradients and helps stabilize updates. This\nsimple addition can prevent potential exploding gradients and lead to more\nrobust convergence, especially for deeper architectures. We will clip gradients\nto a norm of 1.0 right after backpropagation and before the optimizer step,\nleaving all other hyperparameters and logic unchanged.", "We will replace the ReduceLROnPlateau scheduler with the OneCycleLR scheduler,\nwhich linearly increases the learning rate up to a specified maximum (in this\ncase, LEARNING_RATE) and then decreases it again. This often leads to faster and\nmore effective convergence compared to a plateau-based scheduler. We keep the\nexisting residual MLP architecture, early stopping, optimizer, and EMA\ncomponents. By stepping the scheduler once per batch, we give the model a\ndynamic learning rate schedule that typically yields lower validation MSE.", "We will wrap the current AdamW optimizer with a custom Lookahead wrapper, which\nmaintains a \u201cslow\u201d copy of the weights to stabilize updates. Specifically, we\nwill periodically merge the fast (AdamW) weights into the slow weights after a\nfixed number of optimizer steps, creating a smoothing effect. This often\nprovides better exploration and convergence, potentially lowering the validation\nMSE with minimal changes to the existing code.", "We can improve the existing solution by training in mixed precision, which can\nreduce memory usage and often speeds up convergence. Specifically, we will wrap\nthe forward pass and loss computation in torch.cuda.amp.autocast and scale the\ngradients with GradScaler. This minimally alters the code while retaining all\nother hyperparameters, architecture, and ensemble strategy.", "One way to improve the previous solution is to wrap the existing AdamW optimizer\nwith Lookahead, which maintains a slow version of the weights and periodically\nupdates it with the fast weights. This can help stabilize training and enhance\nthe final performance by combining the rapid updates of AdamW with the smoother\nslow weights. We will keep the rest of the setup (EMA, gradient clipping, etc.)\nunchanged, so that we can isolate the effect of this Lookahead modification.\nThis is a minimal, targeted improvement that can often reduce validation MSE\nfurther.", "To fix the index error with self.fast_weights, we can maintain a nested list\nstructure in Lookahead and update it in nested loops to match the parameter\ngroups and their respective parameters. This way, we avoid assigning to an out-\nof-range index. Everything else remains the same.", "One way to improve the previous solution is to replace the ReduceLROnPlateau\nscheduler with a OneCycleLR scheduler, which often converges more effectively by\nramping the learning rate up to a maximum and then decreasing it. We will keep\nthe same architecture, optimizer, early stopping, and EMA logic intact. This\nscheduler change means we will step the learning rate scheduler after each\ntraining batch instead of once per epoch. The rest of the training loop remains\nthe same, ensuring that we isolate the effect of this single modification.", "One way to improve this solution is to replace the ReduceLROnPlateau scheduler\nwith OneCycleLR, which ramps the learning rate up to a specified maximum and\nthen decreases it, often leading to more effective convergence. We will keep the\narchitecture, early stopping, EMA, and other hyperparameters unchanged to\nisolate the effect of the new scheduler. Specifically, we will compute the total\nnumber of training steps based on the data loader and use OneCycleLR at the\nbatch level, which can help the model reach a better optimum within fewer\nepochs.", "One way to improve the solution is to replace ReduceLROnPlateau with OneCycleLR,\nwhich linearly increases the learning rate up to a maximum before gradually\ndecreasing it. This often helps models converge more effectively by facilitating\na wider exploration of parameter space. We will keep the architecture,\noptimizer, and early stopping unchanged so we can focus on the impact of\nOneCycleLR. In practice, we will instantiate OneCycleLR with steps_per_epoch set\nto the length of the training loader, and we will call scheduler.step() after\neach batch.", "One way to improve the previous solution is to replace the ReduceLROnPlateau\nscheduler with the OneCycleLR scheduler, which linearly increases the learning\nrate up to a maximum and then decreases it. This modification often helps models\nconverge more effectively by allowing them to explore higher learning rates\nearly on. We will keep all other hyperparameters and the training loop the same,\nbut remove the epoch-level scheduler step and instead call scheduler.step()\nafter each batch.", "We will improve the solution by adding a Squeeze-and-Excitation (SE) mechanism\ninside each residual block. Specifically, after computing the residual mapping,\nwe will feed the output through two additional linear layers to \"squeeze\" and\nthen \"excite\" the feature dimension, and multiply by a learned gating vector.\nThis should allow the model to reweight crucial features more effectively. We\nwill keep all other hyperparameters and training loops unchanged, so that we can\nisolate the effect of adding the SE sub-block.", "We will add batch normalization to each residual block to help stabilize and\nspeed up training, often improving generalization. Specifically, we insert\nBatchNorm1d layers after each linear layer so that the activations are\nnormalized before applying Swish or the residual addition. This is a\nstraightforward improvement that can reduce internal covariate shift and lead to\nlower MSE on the validation set.", "One way to improve the solution is to replace the Swish activation function with\nthe Mish activation function. Mish often provides a smoother gradient flow and\nbetter performance compared to ReLU-like activations, potentially leading to\nlower MSE. We will define a custom Mish activation class and use it in all\nresidual blocks and the input layer, while keeping all other architecture\ndetails and hyperparameters the same. This atomic change should help us isolate\nthe effect of the Mish activation on the final validation error.", "We will replace the ReduceLROnPlateau scheduler with OneCycleLR, which gradually\nincreases the learning rate to a maximum before decreasing it again. This\napproach often enables faster convergence and sometimes avoids getting stuck at\nsuboptimal points. We will keep all other hyperparameters, early stopping, and\nEMA unchanged so we can isolate the effect of this new scheduler. By stepping\nthe scheduler after every mini-batch during training, the network can explore\nhigher learning rates early on and then anneal them to refine the minima.", "One way to improve the previous solution is to wrap the existing AdamW optimizer\nwith Lookahead. Lookahead maintains a slow copy of the weights that is\nperiodically updated based on the fast (AdamW) weights, which can stabilize\ntraining and potentially yield lower MSE. We will define a Lookahead class,\ninstantiate it with the base optimizer, and keep all other hyperparameters and\narchitecture details the same.", "One way to improve the existing solution is to replace the ReduceLROnPlateau\nscheduler with a OneCycleLR scheduler. OneCycleLR will ramp the learning rate up\nto a maximum early in training and then decrease it, which often leads to faster\nconvergence and better final performance. We will remove the previous scheduler\nlogic and call OneCycleLR.step() at each training batch, keeping all other\ncomponents like EMA, ensemble training, and early stopping unchanged to isolate\nthe effect of this new scheduler.", "One way to improve the solution is to add batch normalization to each residual\nblock. By normalizing the activations, we can help stabilize training and\npotentially achieve better validation MSE. Specifically, we will insert\nBatchNorm1d layers after each linear layer in the residual block. This is a\nsmall, localized change that keeps other hyperparameters and the training loop\nintact, so we can isolate its effect on performance.", "One way to improve the solution is to replace the existing ReduceLROnPlateau\nscheduler with OneCycleLR. We will set the maximum learning rate to the same\nvalue (LEARNING_RATE=1e-3) and ramp it up in the early training phase, then\ngradually reduce it. We will keep the rest of the architecture and training code\nthe same, stepping the OneCycleLR scheduler after each batch. This approach can\nhelp the model converge faster and potentially yield a lower MSE.", "We can replace the ReduceLROnPlateau scheduler with OneCycleLR, which often\nconverges more effectively by ramping the learning rate up to a maximum and then\nreducing it, rather than waiting for a plateau. This requires defining a\nOneCycleLR instance with the total number of steps equal to the number of\nbatches per epoch multiplied by the total epochs. Then, we call scheduler.step()\nafter each training batch rather than after each epoch. This simple change\nusually leads to better convergence and might result in lower validation MSE.", "One way to improve the solution is to incorporate layer normalization in each\nresidual block. By normalizing the hidden activations across features, we can\nhelp stabilize training and potentially improve the final MSE. This is\nstraightforward to implement by adding LayerNorm layers before each linear\nlayer. We will keep all other hyperparameters, optimizer settings, and training\nprocedures unchanged so that we isolate the effect of layer normalization.", "We will replace the existing ReduceLROnPlateau scheduler with a OneCycleLR\nscheduler, which increases the learning rate up to a maximum value before\ndecreasing it. This helps the model explore larger learning rates early in\ntraining, often leading to better convergence and a lower final MSE. We will\nkeep the same residual MLP architecture, EMA, and hyperparameters to isolate the\neffect of this scheduler change. Specifically, we will compute the total\ntraining steps based on the length of the training loader and number of epochs,\nand we will step the scheduler once per batch instead of per epoch.", "One way to improve the solution is to replace the ReduceLROnPlateau scheduler\nwith OneCycleLR. This scheduler ramps the learning rate up to a maximum over the\nfirst part of training and then decreases it, which often helps the model\nconverge more effectively. We will keep the rest of the training loop, early\nstopping, and ensemble logic the same, so we can isolate the effect of this new\nscheduler. This straightforward change should potentially yield a lower\nvalidation MSE by allowing the model to explore higher learning rates early in\ntraining.", "One way to improve the existing solution is to replace the ReduceLROnPlateau\nscheduler with the OneCycleLR scheduler, which ramps the learning rate up to a\nmaximum in the early part of training and then decreases it. This helps the\nmodel explore higher learning rates initially and can converge more effectively.\nWe will keep the model architecture, ensemble approach, and early stopping\nunchanged, only modifying the learning rate schedule steps. By calling the\nOneCycleLR scheduler once per batch, we often see improved performance and a\nlower validation MSE.", "One way to improve the solution is to incorporate a Squeeze-and-Excitation (SE)\nsub-block inside each residual block. This mechanism adaptively reweights\nfeature channels by using a gating function that learns which features are most\nimportant. We will add a small MLP that \u201csqueezes\u201d by reducing the feature\ndimension to a smaller latent space, then \u201cexcites\u201d by mapping back to the\noriginal dimension and scaling the outputs accordingly. Everything else in the\nresidual MLP, including layer normalization and Swish activation, remains\nunchanged to isolate the effect of SE. This modification can help the model\nfocus on the most relevant feature channels and potentially lower the validation\nMSE.", "One way to improve the solution is to add a dropout layer after the Squeeze-and-\nExcitation sub-block in each residual block. This helps prevent overfitting by\nrandomly zeroing out some hidden units during training, leading to more robust\nrepresentations. We will keep the rest of the architecture and hyperparameters\nunchanged so we can isolate the effect of this additional dropout.", "We can replace the ReduceLROnPlateau scheduler with OneCycleLR, which linearly\nincreases the learning rate up to a maximum and then decreases it, often\nimproving convergence and final performance. We will keep the residual MLP\narchitecture, EMA, and all other hyperparameters as before to isolate the effect\nof this change. In practice, we will calculate the total number of steps (epochs\n\u00d7 batches per epoch) and step the scheduler after each training batch. This\napproach typically helps the model explore higher learning rates early on and\nrefine the solution at lower learning rates, leading to better MSE.", "A simple yet effective improvement is to replace the ReduceLROnPlateau scheduler\nwith OneCycleLR, which ramps the learning rate up to a maximum early in training\nand then decreases it. This approach can help the model escape suboptimal local\nminima and converge more effectively. We will define OneCycleLR with a maximum\nlearning rate of 1e-3, computed over the total number of steps (based on the\ntraining loader and epochs), and call scheduler.step() after each batch. All\nother components such as the residual MLP architecture, EMA, and early stopping\nremain unchanged. This modification should provide a more dynamic learning rate\nschedule, potentially lowering the validation MSE.", "One way to improve the previous solution is to replace the ReduceLROnPlateau\nscheduler with OneCycleLR, which linearly increases the learning rate up to a\nmaximum and then decreases it. This often helps the model escape suboptimal\nlocal minima and converge more effectively. We will keep all other\nhyperparameters, architecture components (residual blocks, Swish activation, and\nEMA), and ensemble logic unchanged. By stepping the scheduler once per batch\nrather than after each epoch, we should see smoother optimization and\npotentially a lower validation MSE.", "One way to further improve this solution is to replace the existing plateau-\nbased learning rate scheduler with OneCycleLR, which ramps the learning rate up\nto a peak before gradually decreasing it. This approach can help the model\nexplore higher learning rates early on and often converge to a better local\nminimum. We will keep all other hyperparameters and the EMA/ensemble logic\nunchanged, and simply step the scheduler after each training batch. This\nmodification should lead to improved convergence and potentially a lower\nvalidation MSE.", "We will replace the existing ReduceLROnPlateau scheduler with a OneCycleLR\nscheduler that ramps the learning rate up and then decreases it. Specifically,\nwe'll calculate the total number of training steps (epochs \u00d7 steps per epoch)\nand step the scheduler after each batch. This can help the model explore higher\nlearning rates initially, often leading to improved convergence and a\npotentially lower final MSE. All other components such as the residual MLP\narchitecture, EMA, and early stopping remain unchanged so that we isolate the\neffect of the OneCycleLR scheduler.", "We will replace the ReduceLROnPlateau scheduler with OneCycleLR to give the\nmodel a cyclical learning rate schedule. This allows the network to explore\nhigher learning rates early in training and then gradually reduce them, often\nresulting in better convergence. We keep the same residual MLP design, Swish\nactivation, and EMA logic unchanged so that we can isolate the impact of this\nschedule change. Our only modifications are to remove references to\nReduceLROnPlateau and step OneCycleLR after each mini-batch for a total of\n(number of batches \u00d7 MAX_EPOCHS) steps.", "We can improve the solution by replacing the existing ReduceLROnPlateau\nscheduler with the OneCycleLR scheduler, which increases the learning rate to a\nset maximum early in training and then decreases it afterward. This approach\noften leads to better convergence by exploring higher learning rates and\nprevents the model from becoming stuck at suboptimal minima. We will keep all\nother aspects of the architecture, ensemble logic, and early stopping the same\nto clearly isolate the effect of this change.", "One way to improve the solution is to replace the existing ReduceLROnPlateau\nscheduler with OneCycleLR. This scheduler linearly increases the learning rate\nto a set maximum and then decreases it, enabling faster convergence and often\nbetter final performance. We will keep the same architecture, EMA, and training\nlogic, and only substitute the scheduler implementation so that we can isolate\nthe effect of OneCycleLR on the validation MSE.", "A straightforward improvement is to replace the existing ReduceLROnPlateau\nscheduler with OneCycleLR, which increases the learning rate up to a specified\nmaximum before decreasing it. This often helps the model explore higher learning\nrates early and converge to a better final point. We will keep the rest of the\narchitecture, optimizer, EMA, and early stopping logic unchanged, removing only\nthe epoch-level scheduler step and calling scheduler.step() after each batch. By\ndoing so, we should see more dynamic learning efficiency and potentially a\nreduced validation MSE.", "One way to improve the solution is to replace the ReduceLROnPlateau scheduler\nwith OneCycleLR. This scheduler linearly increases the learning rate up to a\nmaximum during the first part of training and then decreases it, which often\nencourages faster and more effective convergence. We will keep the overall\narchitecture, patience-based early stopping, and EMA logic unchanged, removing\nthe old scheduler and stepping OneCycleLR once per batch. This simple change may\nyield a lower validation MSE by allowing the model to explore higher learning\nrates early in training.", "One way to improve the previous solution is to replace the ReduceLROnPlateau\nscheduler with OneCycleLR, which increases the learning rate to a maximum and\nthen decreases it over the course of training. This approach can often lead to\nfaster and more effective convergence compared to waiting for a validation loss\nplateau. We will keep the same network architecture, early stopping, and\nensemble logic, only substituting the new scheduler that steps once per batch.\nBy exploring higher learning rates early, the model may escape suboptimal local\nminima and achieve a lower MSE.", "We will replace the ReduceLROnPlateau scheduler with OneCycleLR, which ramps the\nlearning rate up to a maximum and then decreases it, allowing the model to\nexplore larger rates early in training. Specifically, we will calculate the\ntotal number of steps (MAX_EPOCHS \u00d7 number of batches per epoch) and step the\nscheduler after each batch instead of waiting for a plateau. This change often\nleads to faster convergence and thereby a lower validation MSE. We will keep all\nother hyperparameters and components, such as EMA and early stopping, the same\nso we can clearly isolate the effect of this schedule change.", "One way to improve this solution is to replace the ReduceLROnPlateau scheduler\nwith OneCycleLR, which ramps the learning rate up to a specified maximum and\nthen decreases it. This approach often helps the model explore higher learning\nrates early in training, leading to faster convergence and potentially a lower\nfinal MSE. We will keep all other hyperparameters, the architecture, and the EMA\nlogic unchanged, stepping the OneCycleLR scheduler once per batch. This\nmodification lets the model benefit from a more dynamic learning rate schedule\nwithout significantly altering the training loop.", "By replacing the ReduceLROnPlateau scheduler with the OneCycleLR scheduler, we\nallow the model to explore higher learning rates early in training before\ndecaying them, which often leads to faster convergence and a better final MSE.\nWe'll remove the validation-based scheduler steps, compute the total training\nsteps as (epochs \u00d7 steps_per_epoch), and step the OneCycleLR scheduler after\neach training batch. This atomic change isolates the effect of the OneCycleLR\nschedule while retaining the same architecture, optimizer, EMA, and early\nstopping approach.", "Replacing the ReduceLROnPlateau scheduler with OneCycleLR can help the model\nexplore higher learning rates in the early stages of training and then gradually\nreduce them, often resulting in more effective convergence. We will remove the\nvalidation-based step call to the scheduler and instead step the scheduler after\neach batch during training. All other aspects of the architecture, optimizer,\nEMA, and early stopping will remain unchanged, allowing us to isolate the effect\nof this new learning rate schedule. This small, atomic change should potentially\nlead to a lower MSE on the validation set while maintaining training stability.", "One way to improve the existing solution is to replace the ReduceLROnPlateau\nscheduler with OneCycleLR. This scheduler ramps the learning rate up to a\nmaximum in the early part of training and then decreases it, often helping the\nmodel escape suboptimal local minima and converge more effectively. We will\nremove the epoch-level scheduler step and instead step the OneCycleLR scheduler\nafter each training batch, keeping the rest of the architecture and\nhyperparameters unchanged. By doing so, we can often see smoother optimization\nand potentially lower validation MSE.", "We will replace the ReduceLROnPlateau scheduler with OneCycleLR so that the\nlearning rate ramps up to a maximum early in training and then decreases. This\napproach often converges more efficiently by exploring a wider range of learning\nrates. We will keep the same architecture, ensemble approach, and other\nhyperparameters, stepping the scheduler after each training batch instead of\nonce per epoch. This single change should help the model reach a better local\nminimum and potentially lower the validation MSE.", "Replacing the ReduceLROnPlateau scheduler with OneCycleLR can help the model\nexplore higher learning rates early in training and then decrease them to refine\nconvergence. Specifically, we remove the epoch-level step call with validation\nmetrics and instead step the OneCycleLR scheduler once per batch. This keeps our\narchitecture, EMA, and early stopping logic unchanged while altering only the\nlearning rate schedule. By doing so, we can potentially achieve better final\nperformance with minimal code modifications.", "Replacing the ReduceLROnPlateau scheduler with OneCycleLR allows the model to\nexplore a higher learning rate early in training and then gradually reduce it,\npotentially escaping suboptimal minima. We will keep the same residual\narchitecture, EMA, and early stopping to isolate the effect of this change.\nSpecifically, we remove the validation-based scheduling call and switch to a\nper-batch schedule with a total step count equal to (epochs \u00d7 steps_per_epoch).", "A straightforward improvement is to replace the ReduceLROnPlateau scheduler with\nOneCycleLR, which linearly increases the learning rate up to a maximum and then\ndecreases it. This often enables faster convergence by allowing exploration of\nhigher learning rates early on. We will keep all other aspects of the model,\nincluding the architecture, ensemble approach, and early stopping, unchanged so\nthat we can isolate the effect of this revised learning rate strategy.", "We will replace the existing ReduceLROnPlateau scheduler with OneCycleLR, which\nlinearly increases the learning rate up to a peak before decreasing it. This\nmeans we remove the validation-based step call to the scheduler and instead step\nthe OneCycleLR scheduler after each training batch, using a total step count of\n(MAX_EPOCHS \u00d7 number_of_training_batches). This atomic change can help the model\nconverge more effectively by exploring higher learning rates early in training.\nWe\u2019ll keep the rest of the architecture, early stopping, and EMA logic the same\nso that we can isolate the effect of this scheduler change.", "One simple improvement is to replace the ReduceLROnPlateau scheduler with\nOneCycleLR, which ramps the learning rate up to a specified maximum and then\ndecreases it. This can help the model explore higher learning rates early in\ntraining and converge to a better local minimum. We will keep the architecture,\noptimizer, EMA, and early stopping the same so that we can isolate the effect of\nusing OneCycleLR. Specifically, we will define a one-cycle schedule over the\ntotal number of training steps and step this scheduler after each batch.", "One way to improve the solution is to replace the ReduceLROnPlateau scheduler\nwith OneCycleLR, which linearly increases the learning rate up to a maximum\nearly in training and then decreases it. We will calculate the total number of\nsteps as the product of MAX_EPOCHS and the number of batches in the training\nloader. Then, we will call scheduler.step() after each batch rather than for\neach epoch or plateau event. This cyclical learning rate often speeds up\nconvergence and may achieve lower MSE.", "One way to improve this solution is to replace the current ReduceLROnPlateau\nscheduler with OneCycleLR, which ramps the learning rate up to a specified\nmaximum before decreasing it. This helps the model explore higher learning rates\nearly in training and can lead to better convergence. We will remove the epoch-\nbased scheduler step call and instead step the OneCycleLR scheduler after each\nbatch. Everything else, including the architecture, early stopping, and EMA,\nwill remain unchanged so we can clearly isolate the effect of this revised\nlearning rate schedule.", "To further improve performance, a simple yet effective strategy is to replace\nthe ReduceLROnPlateau scheduler with OneCycleLR, which ramps the learning rate\nup to a specified maximum early in training and then decreases it. This helps\nthe model explore higher learning rates, often leading to better convergence. We\nwill keep the same architecture, early stopping, and other hyperparameters\nunchanged, only substituting the scheduler. By calling scheduler.step() after\neach training batch, we ensure that the learning rate schedule dynamically\nfollows the one-cycle policy throughout training.", "We will replace the ReduceLROnPlateau scheduler with OneCycleLR, which ramps the\nlearning rate up to a maximum before decreasing it. This lets the network\nexplore higher learning rates early and often converges more effectively than\nwaiting for a plateau. We will set the total steps to the number of mini-batches\ntimes the number of epochs, and step the scheduler after each batch instead of\nusing validation loss. All other aspects of the model, EMA, and early stopping\nlogic remain unchanged.", "One way to improve the previous solution is to replace the plateau-based\nscheduler with OneCycleLR, which ramps up the learning rate early in training\nand then decreases it. This helps the model explore higher learning rates,\npotentially converging to a better local minimum. We will remove the\nReduceLROnPlateau logic and compute the total number of steps as the product of\nepochs and training batches, then step the OneCycleLR scheduler after each\nbatch. All other architecture details, EMA, and early stopping are kept the same\nso that we can isolate the effect of the new scheduler.", "One way to improve this solution is to replace the ReduceLROnPlateau scheduler\nwith OneCycleLR. This scheduler linearly increases the learning rate up to a\nspecified maximum and then decreases it, which often helps the model converge\nmore effectively by exploring higher learning rates early in training. We will\ncompute the total number of steps based on the product of MAX_EPOCHS and the\nnumber of training batches, and then call scheduler.step() after each batch. All\nother aspects, such as the architecture, EMA, and early stopping logic, remain\nunchanged so that we can isolate the effect of this schedule change.", "One simple improvement is to replace the ReduceLROnPlateau scheduler with\nOneCycleLR. This scheduler will ramp the learning rate up to a maximum early in\ntraining and then decrease it, often achieving better convergence than waiting\nfor a plateau. We will remove the validation-based steps and call\nscheduler.step() after each batch, leaving the rest of the model architecture,\nEMA, and early stopping unchanged. This small, isolated change should enable the\nmodel to explore higher learning rates early on and potentially lower the final\nvalidation MSE.", "One way to improve this solution is to replace the ReduceLROnPlateau scheduler\nwith the OneCycleLR scheduler, which linearly increases the learning rate up to\na maximum before decreasing it. This often helps the model explore higher\nlearning rates in the early stages of training and converge more effectively. We\nwill remove the validation-based scheduler step and instead step OneCycleLR\nafter each batch. All other aspects of the architecture, ensemble training, and\nearly stopping will remain unchanged to isolate the effect of this new learning\nrate schedule.", "One way to improve the solution is to replace the ReduceLROnPlateau scheduler\nwith OneCycleLR, which often yields faster convergence by increasing the\nlearning rate up to a maximum before reducing it. We will compute the total\nnumber of steps as MAX_EPOCHS multiplied by the number of batches per epoch and\nstep the scheduler after each mini-batch. This ensures a more dynamic learning\nrate schedule that can help the model escape suboptimal minima and potentially\nlower the validation MSE.", "A simple improvement is to replace the ReduceLROnPlateau scheduler with\nOneCycleLR, which ramps the learning rate up to a maximum and then decreases it.\nThis typically helps the model explore more aggressive learning rates early in\ntraining and converge more effectively. We will keep early stopping, EMA, and\nthe rest of the code unchanged, but remove the old scheduler logic and step\nOneCycleLR after every batch. This atomic change may yield lower MSE by allowing\nmore dynamic learning rate scheduling without significantly altering the\ntraining loop."], "code": ["import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# -----------------------------\n# 1) Load data\n# -----------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\", header=0).values.astype(np.float32)\ns_train = pd.read_csv(\"./input/s_training.csv\", header=0).values.astype(np.float32)\n\ng_val = pd.read_csv(\"./input/g_validation.csv\", header=0).values.astype(np.float32)\ns_val = pd.read_csv(\"./input/s_validation.csv\", header=0).values.astype(np.float32)\n\ng_test = pd.read_csv(\"./input/test_g.csv\", header=0).values.astype(np.float32)\ns_test = pd.read_csv(\"./input/test_s.csv\", header=0).values.astype(\n    np.float32\n)  # for final MSE\n\n# -----------------------------\n# 2) Create PyTorch datasets/loaders\n# -----------------------------\ntrain_dataset = TensorDataset(torch.from_numpy(g_train), torch.from_numpy(s_train))\nval_dataset = TensorDataset(torch.from_numpy(g_val), torch.from_numpy(s_val))\ntest_dataset = TensorDataset(torch.from_numpy(g_test), torch.from_numpy(s_test))\n\nbatch_size = 64\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# -----------------------------\n# 3) Define model\n# -----------------------------\nclass SimpleMLP(nn.Module):\n    def __init__(self, input_dim=14, output_dim=2001):\n        super(SimpleMLP, self).__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, output_dim),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nmodel = SimpleMLP().to(device)\n\n# -----------------------------\n# 4) Define loss and optimizer\n# -----------------------------\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# -----------------------------\n# 5) Training loop\n# -----------------------------\nepochs = 5\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    for geom, spec in train_loader:\n        geom, spec = geom.to(device), spec.to(device)\n        optimizer.zero_grad()\n        output = model(geom)\n        loss = criterion(output, spec)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * geom.size(0)\n    train_mse = running_loss / len(train_loader.dataset)\n\n    # Validation MSE\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for geom, spec in val_loader:\n            geom, spec = geom.to(device), spec.to(device)\n            output = model(geom)\n            loss = criterion(output, spec)\n            val_loss += loss.item() * geom.size(0)\n    val_mse = val_loss / len(val_loader.dataset)\n    print(\n        f\"Epoch {epoch+1}/{epochs}, Training MSE: {train_mse:.6f}, Validation MSE: {val_mse:.6f}\"\n    )\n\n\n# -----------------------------\n# 6) Evaluate on validation and test\n# -----------------------------\ndef evaluate_mse(loader):\n    model.eval()\n    total_loss = 0.0\n    with torch.no_grad():\n        for geom, spec in loader:\n            geom, spec = geom.to(device), spec.to(device)\n            preds = model(geom)\n            loss = criterion(preds, spec)\n            total_loss += loss.item() * geom.size(0)\n    return total_loss / len(loader.dataset)\n\n\nval_mse_final = evaluate_mse(val_loader)\ntest_mse_final = evaluate_mse(test_loader)\nprint(f\"Validation MSE: {val_mse_final:.6f}\")\nprint(f\"Test MSE: {test_mse_final:.6f}\")\n\n# -----------------------------\n# 7) Generate final predictions on test set and save submission\n# -----------------------------\nmodel.eval()\nall_preds = []\nwith torch.no_grad():\n    for geom, _ in test_loader:\n        geom = geom.to(device)\n        preds = model(geom)\n        all_preds.append(preds.cpu().numpy())\nall_preds = np.concatenate(all_preds, axis=0)\n\n# Save to submission.csv\ndf_submission = pd.DataFrame(all_preds)\ndf_submission.to_csv(\"./working/submission.csv\", index=False, float_format=\"%.9f\")\n\nprint(\"Done! MSE on validation set:\", val_mse_final, \"MSE on test set:\", test_mse_final)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nEPOCHS = 10\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\n\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\n\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Model Definition: Residual MLP\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = nn.ReLU()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=64, out_dim=2001, n_blocks=3):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = torch.relu(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\nmodel = ResidualMLP().to(DEVICE)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n# -------------------------\n# Training Loop\n# -------------------------\nfor epoch in range(EPOCHS):\n    model.train()\n    for batch_X, batch_y in train_loader:\n        batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n        optimizer.zero_grad()\n        preds = model(batch_X)\n        loss = criterion(preds, batch_y)\n        loss.backward()\n        optimizer.step()\n\n# -------------------------\n# Validation\n# -------------------------\nmodel.eval()\nval_preds = []\nval_targets = []\nwith torch.no_grad():\n    for batch_X, batch_y in val_loader:\n        batch_X = batch_X.to(DEVICE)\n        out = model(batch_X).cpu().numpy()\n        val_preds.append(out)\n        val_targets.append(batch_y.numpy())\n\nval_preds = np.concatenate(val_preds, axis=0)\nval_targets = np.concatenate(val_targets, axis=0)\nval_mse = mean_squared_error(val_targets, val_preds)\nprint(f\"Validation MSE: {val_mse:.6f}\")\n\n# -------------------------\n# Inference on Test Set\n# -------------------------\nmodel.eval()\ntest_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\nwith torch.no_grad():\n    test_preds = model(test_tensor).cpu().numpy()\n\n# -------------------------\n# Save submission\n# -------------------------\nsubmission = pd.DataFrame(test_preds)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\n# =======================\n# Hyperparameters\n# =======================\nBATCH_SIZE = 128\nEPOCHS_CV = 1  # For quick 5-fold cross-validation\nEPOCHS_FINAL = 3\nLR = 1e-3\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nSEED = 42\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\n# =======================\n# Data Loading\n# =======================\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Reshape for 1D CNN: (batch, channels=1, length=14)\nX_train_1d = X_train.reshape(-1, 1, 14)\nX_val_1d = X_val.reshape(-1, 1, 14)\nX_test_1d = X_test.reshape(-1, 1, 14)\n\n\n# =======================\n# Model Definition\n# =======================\nclass Simple1DCNN(nn.Module):\n    def __init__(self):\n        super(Simple1DCNN, self).__init__()\n        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(\n            in_channels=32, out_channels=64, kernel_size=3, padding=1\n        )\n        self.conv3 = nn.Conv1d(\n            in_channels=64, out_channels=128, kernel_size=3, padding=1\n        )\n        self.flatten = nn.Flatten()\n        self.fc_out = nn.Linear(128 * 14, 2001)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        x = self.relu(self.conv3(x))\n        x = self.flatten(x)\n        return self.fc_out(x)\n\n\n# =======================\n# Training / Evaluation Helpers\n# =======================\ndef train_one_epoch(model, optimizer, X, y):\n    model.train()\n    mse_fn = nn.MSELoss()\n    idxs = np.arange(len(X))\n    np.random.shuffle(idxs)\n    total_loss = 0.0\n\n    for start in range(0, len(X), BATCH_SIZE):\n        end = start + BATCH_SIZE\n        batch_idx = idxs[start:end]\n        xb = torch.tensor(X[batch_idx], device=DEVICE)\n        yb = torch.tensor(y[batch_idx], device=DEVICE)\n\n        optimizer.zero_grad()\n        pred = model(xb)\n        loss = mse_fn(pred, yb)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * len(batch_idx)\n    return total_loss / len(X)\n\n\ndef evaluate(model, X, y):\n    model.eval()\n    mse_fn = nn.MSELoss()\n    with torch.no_grad():\n        preds = []\n        for start in range(0, len(X), BATCH_SIZE):\n            end = start + BATCH_SIZE\n            xb = torch.tensor(X[start:end], device=DEVICE)\n            pred = model(xb)\n            preds.append(pred.cpu().numpy())\n        preds = np.concatenate(preds, axis=0)\n    return mean_squared_error(y, preds)\n\n\n# =======================\n# 5-Fold Cross Validation on Training Set\n# =======================\nkf = KFold(n_splits=5, shuffle=True, random_state=SEED)\ncv_scores = []\n\nfor fold_idx, (train_idx, val_idx) in enumerate(kf.split(X_train_1d)):\n    model = Simple1DCNN().to(DEVICE)\n    optimizer = optim.Adam(model.parameters(), lr=LR)\n\n    X_tr, X_va = X_train_1d[train_idx], X_train_1d[val_idx]\n    y_tr, y_va = y_train[train_idx], y_train[val_idx]\n\n    for epoch in range(EPOCHS_CV):\n        train_one_epoch(model, optimizer, X_tr, y_tr)\n    fold_mse = evaluate(model, X_va, y_va)\n    cv_scores.append(fold_mse)\n\navg_cv_mse = np.mean(cv_scores)\n\n# =======================\n# Train Final Model on Full Training Set\n# =======================\nfinal_model = Simple1DCNN().to(DEVICE)\noptimizer = optim.Adam(final_model.parameters(), lr=LR)\n\nfor epoch in range(EPOCHS_FINAL):\n    train_one_epoch(final_model, optimizer, X_train_1d, y_train)\n\nval_mse = evaluate(final_model, X_val_1d, y_val)\n\n# =======================\n# Predict on Test Set\n# =======================\nfinal_model.eval()\ntest_preds = []\nwith torch.no_grad():\n    for start in range(0, len(X_test_1d), BATCH_SIZE):\n        end = start + BATCH_SIZE\n        xb = torch.tensor(X_test_1d[start:end], device=DEVICE)\n        preds_batch = final_model(xb).cpu().numpy()\n        test_preds.append(preds_batch)\ntest_preds = np.concatenate(test_preds, axis=0)\n\n# Save to CSV\nsubmission_df = pd.DataFrame(test_preds)\nsubmission_df.to_csv(\"./working/submission.csv\", index=False)\n\n# Print evaluation metric on the validation set and also CV results\nprint(f\"5-Fold CV MSE: {avg_cv_mse:.6f}\")\nprint(f\"Validation MSE: {val_mse:.6f}\")", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n\n# --------------------\n# 1. Data Preparation\n# --------------------\nclass SpectrumDataset(Dataset):\n    def __init__(self, g_file, s_file):\n        self.g_data = pd.read_csv(g_file).values.astype(np.float32)\n        self.s_data = pd.read_csv(s_file).values.astype(np.float32)\n        assert len(self.g_data) == len(self.s_data)\n\n    def __len__(self):\n        return len(self.g_data)\n\n    def __getitem__(self, idx):\n        x = self.g_data[idx]\n        y = self.s_data[idx]\n        return x, y\n\n\ntrain_dataset = SpectrumDataset(\"./input/g_training.csv\", \"./input/s_training.csv\")\nval_dataset = SpectrumDataset(\"./input/g_validation.csv\", \"./input/s_validation.csv\")\ntest_g = pd.read_csv(\"./input/test_g.csv\").values.astype(np.float32)\ntest_s = pd.read_csv(\"./input/test_s.csv\").values.astype(\n    np.float32\n)  # for reference MSE\n\n# DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n\n\n# --------------------\n# 2. Model Definition\n# --------------------\nclass TransformerRegressor(nn.Module):\n    def __init__(\n        self, input_dim=14, embed_dim=64, output_dim=2001, nhead=4, num_layers=2\n    ):\n        super().__init__()\n        self.embedding = nn.Linear(input_dim, embed_dim)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim, nhead=nhead, dim_feedforward=128, batch_first=False\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.fc_out = nn.Linear(embed_dim, output_dim)\n\n    def forward(self, x):\n        # x shape: (batch, input_dim)\n        # Convert to shape (seq_len=14, batch, embed_dim)\n        x = self.embedding(x).transpose(0, 1)\n        # Pass through transformer\n        x = self.transformer(x)  # shape remains (14, batch, embed_dim)\n        # Mean pool across sequence length\n        x = x.mean(dim=0)  # shape: (batch, embed_dim)\n        # Final regression head\n        return self.fc_out(x)\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = TransformerRegressor().to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n# --------------------\n# 3. Training Loop\n# --------------------\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    model.train()\n    for X_batch, y_batch in train_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        optimizer.zero_grad()\n        preds = model(X_batch)\n        loss = criterion(preds, y_batch)\n        loss.backward()\n        optimizer.step()\n\n# --------------------\n# 4. Validation\n# --------------------\nmodel.eval()\nval_losses = []\nwith torch.no_grad():\n    for X_batch, y_batch in val_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        preds = model(X_batch)\n        loss = criterion(preds, y_batch)\n        val_losses.append(loss.item())\nval_mse = np.mean(val_losses)\n\n# --------------------\n# 5. Test Evaluation\n# --------------------\n# Predict on test set (for reference MSE and submission)\nmodel.eval()\ntest_tensor = torch.from_numpy(test_g).to(device)\nwith torch.no_grad():\n    test_pred = []\n    batch_size = 64\n    for i in range(0, len(test_tensor), batch_size):\n        batch_in = test_tensor[i : i + batch_size]\n        batch_out = model(batch_in)\n        test_pred.append(batch_out.cpu().numpy())\n    test_pred = np.concatenate(test_pred, axis=0)\n\ntest_loss = np.mean((test_pred - test_s) ** 2)  # reference MSE\n\n# --------------------\n# 6. Save submission\n# --------------------\n# test_pred shape: (n_test_samples, 2001)\nsubmission_df = pd.DataFrame(test_pred)\nsubmission_df.to_csv(\"./working/submission.csv\", index=False)\n\n# Print results\nprint(\"Validation MSE:\", val_mse)\nprint(\"Test MSE:\", test_loss)", "import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\n# ----------------------\n# Data Loading\n# ----------------------\ntrain_g_path = \"./input/g_training.csv\"\ntrain_s_path = \"./input/s_training.csv\"\nval_g_path = \"./input/g_validation.csv\"\nval_s_path = \"./input/s_validation.csv\"\ntest_g_path = \"./input/test_g.csv\"\n\nX_train = pd.read_csv(train_g_path).values.astype(np.float32)\ny_train = pd.read_csv(train_s_path).values.astype(np.float32)\nX_val = pd.read_csv(val_g_path).values.astype(np.float32)\ny_val = pd.read_csv(val_s_path).values.astype(np.float32)\nX_test = pd.read_csv(test_g_path).values.astype(np.float32)\n\n\n# ----------------------\n# PyTorch Dataset\n# ----------------------\nclass GeometrySpectrumDataset(torch.utils.data.Dataset):\n    def __init__(self, g, s):\n        self.g = g\n        self.s = s\n\n    def __len__(self):\n        return len(self.g)\n\n    def __getitem__(self, idx):\n        return self.g[idx], self.s[idx]\n\n\n# ----------------------\n# Model Definition (Transformer-based)\n# ----------------------\nclass TransformerRegressor(nn.Module):\n    def __init__(\n        self, input_len=14, d_model=32, nhead=4, num_layers=2, output_dim=2001\n    ):\n        super().__init__()\n        # Each of the 14 geometry features is treated like a \"token\" of 1-dim we embed into d_model\n        self.input_embed = nn.Linear(1, d_model)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=nhead, dim_feedforward=64, dropout=0.1\n        )\n        self.transformer_encoder = nn.TransformerEncoder(\n            encoder_layer, num_layers=num_layers\n        )\n        # Final linear to produce 2001 outputs from flattened encoding\n        self.fc_out = nn.Linear(input_len * d_model, output_dim)\n\n    def forward(self, x):\n        # x shape: (batch_size, input_len)\n        # Treat each geometry feature as a separate token\n        x = x.unsqueeze(-1)  # (batch_size, input_len, 1)\n        x = self.input_embed(x)  # (batch_size, input_len, d_model)\n        # Transformer expects shape (sequence_length, batch_size, d_model)\n        x = x.permute(1, 0, 2)  # (input_len, batch_size, d_model)\n        x = self.transformer_encoder(x)\n        x = x.permute(1, 0, 2)  # (batch_size, input_len, d_model)\n        x = x.reshape(x.shape[0], -1)  # flatten\n        return self.fc_out(x)\n\n\n# ----------------------\n# Utility: Training Step\n# ----------------------\ndef train_model(model, train_loader, val_loader, device, epochs=5):\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n    model.to(device)\n\n    for epoch in range(epochs):\n        model.train()\n        for batch_x, batch_y in train_loader:\n            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n            optimizer.zero_grad()\n            pred = model(batch_x)\n            loss = criterion(pred, batch_y)\n            loss.backward()\n            optimizer.step()\n\n    # Evaluate on val set\n    model.eval()\n    val_preds = []\n    val_true = []\n    with torch.no_grad():\n        for batch_x, batch_y in val_loader:\n            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n            pred = model(batch_x)\n            val_preds.append(pred.cpu().numpy())\n            val_true.append(batch_y.cpu().numpy())\n    val_preds = np.concatenate(val_preds, axis=0)\n    val_true = np.concatenate(val_true, axis=0)\n    return mean_squared_error(val_true, val_preds)\n\n\n# ----------------------\n# 5-Fold Cross Validation on Training Set\n# ----------------------\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\ncv_scores = []\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_train)):\n    X_tr, X_va = X_train[tr_idx], X_train[va_idx]\n    y_tr, y_va = y_train[tr_idx], y_train[va_idx]\n\n    tr_dataset = GeometrySpectrumDataset(X_tr, y_tr)\n    va_dataset = GeometrySpectrumDataset(X_va, y_va)\n    tr_loader = torch.utils.data.DataLoader(tr_dataset, batch_size=64, shuffle=True)\n    va_loader = torch.utils.data.DataLoader(va_dataset, batch_size=64, shuffle=False)\n\n    fold_model = TransformerRegressor()\n    fold_mse = train_model(fold_model, tr_loader, va_loader, device, epochs=3)\n    cv_scores.append(fold_mse)\n\n# Print average CV MSE\navg_cv_mse = np.mean(cv_scores)\n\n# ----------------------\n# Final Training on Full Training Set\n# ----------------------\nfull_dataset = GeometrySpectrumDataset(X_train, y_train)\nfull_loader = torch.utils.data.DataLoader(full_dataset, batch_size=64, shuffle=True)\nval_dataset = GeometrySpectrumDataset(X_val, y_val)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n\nfinal_model = TransformerRegressor()\nfinal_val_mse = train_model(final_model, full_loader, val_loader, device, epochs=5)\n\n# Print final validation MSE\nprint(\"5-Fold CV MSE:\", avg_cv_mse)\nprint(\"Validation MSE:\", final_val_mse)\n\n# ----------------------\n# Predict on Test Set and Save Submission\n# ----------------------\nfinal_model.eval()\nfinal_model.to(device)\ntest_tensor = torch.from_numpy(X_test).float()\nbatch_size = 64\ntest_preds = []\n\nwith torch.no_grad():\n    for i in range(0, len(test_tensor), batch_size):\n        batch_x = test_tensor[i : i + batch_size].to(device)\n        pred = final_model(batch_x)\n        test_preds.append(pred.cpu().numpy())\ntest_preds = np.concatenate(test_preds, axis=0)\n\n# Save predictions to submission.csv\nsubmission_df = pd.DataFrame(test_preds)\nsubmission_df.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nEPOCHS = 10\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nDROPOUT_RATE = 0.2  # newly added dropout rate\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\n\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\n\ng_test = pd.read_csv(\"./input/test_g.csv\")\ns_test = pd.read_csv(\"./input/test_s.csv\")  # added to evaluate final test MSE\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\ny_test = s_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n# Define a test DataLoader for MSE evaluation on test set\ntest_data = torch.utils.data.TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\ntest_loader = torch.utils.data.DataLoader(\n    test_data, batch_size=BATCH_SIZE, shuffle=False\n)\n\n\n# -------------------------\n# Model Definition: Residual MLP with Dropout\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, p=0.2):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.dropout = nn.Dropout(p)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = nn.ReLU()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.dropout(out)\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=64, out_dim=2001, n_blocks=3, p=0.2):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim, p=p) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = torch.relu(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\nmodel = ResidualMLP(p=DROPOUT_RATE).to(DEVICE)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n# -------------------------\n# Training Loop\n# -------------------------\nfor epoch in range(EPOCHS):\n    model.train()\n    for batch_X, batch_y in train_loader:\n        batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n        optimizer.zero_grad()\n        preds = model(batch_X)\n        loss = criterion(preds, batch_y)\n        loss.backward()\n        optimizer.step()\n\n# -------------------------\n# Validation\n# -------------------------\nmodel.eval()\nval_preds = []\nval_targets = []\nwith torch.no_grad():\n    for batch_X, batch_y in val_loader:\n        batch_X = batch_X.to(DEVICE)\n        out = model(batch_X).cpu().numpy()\n        val_preds.append(out)\n        val_targets.append(batch_y.numpy())\n\nval_preds = np.concatenate(val_preds, axis=0)\nval_targets = np.concatenate(val_targets, axis=0)\nval_mse = mean_squared_error(val_targets, val_preds)\nprint(f\"Validation MSE: {val_mse:.6f}\")\n\n# -------------------------\n# Test Evaluation (MSE)\n# -------------------------\ntest_preds = []\ntest_targets = []\nwith torch.no_grad():\n    for batch_X, batch_y in test_loader:\n        batch_X = batch_X.to(DEVICE)\n        out = model(batch_X).cpu().numpy()\n        test_preds.append(out)\n        test_targets.append(batch_y.numpy())\n\ntest_preds = np.concatenate(test_preds, axis=0)\ntest_targets = np.concatenate(test_targets, axis=0)\ntest_mse = mean_squared_error(test_targets, test_preds)\nprint(f\"Test MSE: {test_mse:.6f}\")\n\n# -------------------------\n# Save submission to CSV\n# -------------------------\nsubmission_df = pd.DataFrame(test_preds)\nsubmission_df.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n\n# --------------------\n# 1. Data Preparation\n# --------------------\nclass SpectrumDataset(Dataset):\n    def __init__(self, g_file, s_file):\n        self.g_data = pd.read_csv(g_file).values.astype(np.float32)\n        self.s_data = pd.read_csv(s_file).values.astype(np.float32)\n        assert len(self.g_data) == len(self.s_data)\n\n    def __len__(self):\n        return len(self.g_data)\n\n    def __getitem__(self, idx):\n        x = self.g_data[idx]\n        y = self.s_data[idx]\n        return x, y\n\n\ntrain_dataset = SpectrumDataset(\"./input/g_training.csv\", \"./input/s_training.csv\")\nval_dataset = SpectrumDataset(\"./input/g_validation.csv\", \"./input/s_validation.csv\")\ntest_g = pd.read_csv(\"./input/test_g.csv\").values.astype(np.float32)\ntest_s = pd.read_csv(\"./input/test_s.csv\").values.astype(\n    np.float32\n)  # for reference MSE\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n\n\n# --------------------\n# 2. Model Definition\n# --------------------\nclass TransformerRegressor(nn.Module):\n    def __init__(\n        self, input_dim=14, embed_dim=64, output_dim=2001, nhead=4, num_layers=2\n    ):\n        super().__init__()\n        # Treat each of the 14 geometry parameters as a token of dimension 1\n        self.embedding = nn.Linear(1, embed_dim)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim, nhead=nhead, dim_feedforward=128, batch_first=False\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.fc_out = nn.Linear(embed_dim, output_dim)\n\n    def forward(self, x):\n        # x shape: (batch, 14)\n        # Reshape each sample to (batch, 14, 1), then embed -> (batch, 14, embed_dim)\n        x = x.unsqueeze(-1)\n        x = self.embedding(x)\n        # Transpose to (14, batch, embed_dim)\n        x = x.transpose(0, 1)\n        # Pass through transformer\n        x = self.transformer(x)  # shape: (14, batch, embed_dim)\n        # Mean pool across sequence length\n        x = x.mean(dim=0)  # shape: (batch, embed_dim)\n        return self.fc_out(x)\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = TransformerRegressor().to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n# --------------------\n# 3. Training Loop\n# --------------------\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    model.train()\n    for X_batch, y_batch in train_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        optimizer.zero_grad()\n        preds = model(X_batch)\n        loss = criterion(preds, y_batch)\n        loss.backward()\n        optimizer.step()\n\n# --------------------\n# 4. Validation\n# --------------------\nmodel.eval()\nval_losses = []\nwith torch.no_grad():\n    for X_batch, y_batch in val_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        preds = model(X_batch)\n        loss = criterion(preds, y_batch)\n        val_losses.append(loss.item())\nval_mse = np.mean(val_losses)\n\n# --------------------\n# 5. Test Evaluation\n# --------------------\nmodel.eval()\ntest_tensor = torch.from_numpy(test_g).to(device)\ntest_pred = []\nwith torch.no_grad():\n    batch_size = 64\n    for i in range(0, len(test_tensor), batch_size):\n        batch_in = test_tensor[i : i + batch_size]\n        batch_out = model(batch_in)\n        test_pred.append(batch_out.cpu().numpy())\ntest_pred = np.concatenate(test_pred, axis=0)\ntest_loss = np.mean((test_pred - test_s) ** 2)\n\n# --------------------\n# 6. Save submission\n# --------------------\nsubmission_df = pd.DataFrame(test_pred)\nsubmission_df.to_csv(\"./working/submission.csv\", index=False)\n\n# --------------------\n# 7. Print results\n# --------------------\nprint(\"Validation MSE:\", val_mse)\nprint(\"Test MSE:\", test_loss)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nEPOCHS = 10\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\n\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\n\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Model Definition: Residual MLP with BatchNorm\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.bn1 = nn.BatchNorm1d(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.bn2 = nn.BatchNorm1d(dim)\n        self.act = nn.ReLU()\n\n    def forward(self, x):\n        identity = x\n        out = self.fc1(x)\n        out = self.bn1(out)\n        out = self.act(out)\n        out = self.fc2(out)\n        out = self.bn2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=64, out_dim=2001, n_blocks=3):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = torch.relu(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\nmodel = ResidualMLP().to(DEVICE)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n# -------------------------\n# Training Loop\n# -------------------------\nfor epoch in range(EPOCHS):\n    model.train()\n    for batch_X, batch_y in train_loader:\n        batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n        optimizer.zero_grad()\n        preds = model(batch_X)\n        loss = criterion(preds, batch_y)\n        loss.backward()\n        optimizer.step()\n\n# -------------------------\n# Validation\n# -------------------------\nmodel.eval()\nval_preds = []\nval_targets = []\nwith torch.no_grad():\n    for batch_X, batch_y in val_loader:\n        batch_X = batch_X.to(DEVICE)\n        out = model(batch_X).cpu().numpy()\n        val_preds.append(out)\n        val_targets.append(batch_y.numpy())\n\nval_preds = np.concatenate(val_preds, axis=0)\nval_targets = np.concatenate(val_targets, axis=0)\nval_mse = mean_squared_error(val_targets, val_preds)\nprint(f\"Validation MSE: {val_mse:.6f}\")\n\n# -------------------------\n# Inference on Test Set\n# -------------------------\nmodel.eval()\ntest_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\nwith torch.no_grad():\n    test_preds = model(test_tensor).cpu().numpy()\n\n# -------------------------\n# Save submission\n# -------------------------\nsubmission = pd.DataFrame(test_preds)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nEPOCHS = 10\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\ns_test_dummy = pd.read_csv(\n    \"./input/test_s.csv\"\n)  # not used for training, just to confirm shape\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Model Definition: Residual MLP\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = nn.ReLU()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=64, out_dim=2001, n_blocks=3):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = torch.relu(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\nmodel = ResidualMLP().to(DEVICE)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n\n# -------------------------\n# Training Loop\n# -------------------------\nfor epoch in range(EPOCHS):\n    model.train()\n    for batch_X, batch_y in train_loader:\n        batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n        optimizer.zero_grad()\n        preds = model(batch_X)\n        loss = criterion(preds, batch_y)\n        loss.backward()\n        optimizer.step()\n\n# -------------------------\n# Validation\n# -------------------------\nmodel.eval()\nval_preds = []\nval_targets = []\nwith torch.no_grad():\n    for batch_X, batch_y in val_loader:\n        batch_X = batch_X.to(DEVICE)\n        out = model(batch_X).cpu().numpy()\n        val_preds.append(out)\n        val_targets.append(batch_y.numpy())\n\nval_preds = np.concatenate(val_preds, axis=0)\nval_targets = np.concatenate(val_targets, axis=0)\nval_mse = mean_squared_error(val_targets, val_preds)\nprint(f\"Validation MSE: {val_mse:.6f}\")\n\n# -------------------------\n# Inference on Test Set\n# -------------------------\ntest_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\nmodel.eval()\nwith torch.no_grad():\n    test_preds = model(test_tensor).cpu().numpy()\n\n# -------------------------\n# Save submission\n# -------------------------\nsubmission = pd.DataFrame(test_preds)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nEPOCHS = 10\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\n\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\n\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Model Definition: Residual MLP\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = nn.ReLU()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=64, out_dim=2001, n_blocks=3):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = torch.relu(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\nmodel = ResidualMLP().to(DEVICE)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n# -------------------------\n# Learning Rate Scheduler\n# -------------------------\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n\n# -------------------------\n# Training Loop\n# -------------------------\nfor epoch in range(EPOCHS):\n    model.train()\n    for batch_X, batch_y in train_loader:\n        batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n        optimizer.zero_grad()\n        preds = model(batch_X)\n        loss = criterion(preds, batch_y)\n        loss.backward()\n        optimizer.step()\n    scheduler.step()\n\n# -------------------------\n# Validation\n# -------------------------\nmodel.eval()\nval_preds = []\nval_targets = []\nwith torch.no_grad():\n    for batch_X, batch_y in val_loader:\n        batch_X = batch_X.to(DEVICE)\n        out = model(batch_X).cpu().numpy()\n        val_preds.append(out)\n        val_targets.append(batch_y.numpy())\n\nval_preds = np.concatenate(val_preds, axis=0)\nval_targets = np.concatenate(val_targets, axis=0)\nval_mse = mean_squared_error(val_targets, val_preds)\nprint(f\"Validation MSE: {val_mse:.6f}\")\n\n# -------------------------\n# Inference on Test Set\n# -------------------------\nmodel.eval()\ntest_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\nwith torch.no_grad():\n    test_preds = model(test_tensor).cpu().numpy()\n\n# -------------------------\n# Save submission\n# -------------------------\nsubmission = pd.DataFrame(test_preds)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nEPOCHS = 10\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\n\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\n\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Model Definition: Residual MLP with ELU\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = nn.ELU()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=64, out_dim=2001, n_blocks=3):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = nn.ELU()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\nmodel = ResidualMLP().to(DEVICE)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n# -------------------------\n# Training Loop\n# -------------------------\nfor epoch in range(EPOCHS):\n    model.train()\n    for batch_X, batch_y in train_loader:\n        batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n        optimizer.zero_grad()\n        preds = model(batch_X)\n        loss = criterion(preds, batch_y)\n        loss.backward()\n        optimizer.step()\n\n# -------------------------\n# Validation\n# -------------------------\nmodel.eval()\nval_preds = []\nval_targets = []\nwith torch.no_grad():\n    for batch_X, batch_y in val_loader:\n        batch_X = batch_X.to(DEVICE)\n        out = model(batch_X).cpu().numpy()\n        val_preds.append(out)\n        val_targets.append(batch_y.numpy())\n\nval_preds = np.concatenate(val_preds, axis=0)\nval_targets = np.concatenate(val_targets, axis=0)\nval_mse = mean_squared_error(val_targets, val_preds)\nprint(f\"Validation MSE: {val_mse:.6f}\")\n\n# -------------------------\n# Inference on Test Set\n# -------------------------\nmodel.eval()\ntest_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\nwith torch.no_grad():\n    test_preds = model(test_tensor).cpu().numpy()\n\n# -------------------------\n# Save submission\n# -------------------------\nsubmission = pd.DataFrame(test_preds)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nEPOCHS = 10\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\n\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\n\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Model Definition: Residual MLP\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = nn.ReLU()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=128, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = torch.relu(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\nmodel = ResidualMLP().to(DEVICE)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n# -------------------------\n# Training Loop\n# -------------------------\nfor epoch in range(EPOCHS):\n    model.train()\n    for batch_X, batch_y in train_loader:\n        batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n        optimizer.zero_grad()\n        preds = model(batch_X)\n        loss = criterion(preds, batch_y)\n        loss.backward()\n        optimizer.step()\n\n# -------------------------\n# Validation\n# -------------------------\nmodel.eval()\nval_preds = []\nval_targets = []\nwith torch.no_grad():\n    for batch_X, batch_y in val_loader:\n        batch_X = batch_X.to(DEVICE)\n        out = model(batch_X).cpu().numpy()\n        val_preds.append(out)\n        val_targets.append(batch_y.numpy())\n\nval_preds = np.concatenate(val_preds, axis=0)\nval_targets = np.concatenate(val_targets, axis=0)\nval_mse = mean_squared_error(val_targets, val_preds)\nprint(f\"Validation MSE: {val_mse:.6f}\")\n\n# -------------------------\n# Inference on Test Set\n# -------------------------\ntest_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\nwith torch.no_grad():\n    test_preds = model(test_tensor).cpu().numpy()\n\nsubmission = pd.DataFrame(test_preds)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\n\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\n\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Model Definition: Residual MLP\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = nn.ReLU()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=128, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = torch.relu(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\nmodel = ResidualMLP().to(DEVICE)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\nbest_val_mse = float(\"inf\")\nepochs_without_improvement = 0\nbest_model_state = None\n\n# -------------------------\n# Training Loop with Early Stopping\n# -------------------------\nfor epoch in range(MAX_EPOCHS):\n    model.train()\n    for batch_X, batch_y in train_loader:\n        batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n        optimizer.zero_grad()\n        preds = model(batch_X)\n        loss = criterion(preds, batch_y)\n        loss.backward()\n        optimizer.step()\n\n    # Validation\n    model.eval()\n    val_preds = []\n    val_targets = []\n    with torch.no_grad():\n        for batch_X, batch_y in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds.append(out)\n            val_targets.append(batch_y.numpy())\n    val_preds = np.concatenate(val_preds, axis=0)\n    val_targets = np.concatenate(val_targets, axis=0)\n    val_mse = mean_squared_error(val_targets, val_preds)\n\n    # Check improvement\n    if val_mse < best_val_mse:\n        best_val_mse = val_mse\n        epochs_without_improvement = 0\n        best_model_state = model.state_dict()\n    else:\n        epochs_without_improvement += 1\n\n    if epochs_without_improvement >= PATIENCE:\n        break\n\n# Load best model state\nmodel.load_state_dict(best_model_state)\n\n# Final validation MSE\nmodel.eval()\nval_preds = []\nval_targets = []\nwith torch.no_grad():\n    for batch_X, batch_y in val_loader:\n        batch_X = batch_X.to(DEVICE)\n        out = model(batch_X).cpu().numpy()\n        val_preds.append(out)\n        val_targets.append(batch_y.numpy())\nval_preds = np.concatenate(val_preds, axis=0)\nval_targets = np.concatenate(val_targets, axis=0)\nval_mse = mean_squared_error(val_targets, val_preds)\nprint(f\"Validation MSE: {val_mse:.6f}\")\n\n# -------------------------\n# Inference on Test Set\n# -------------------------\ntest_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\nwith torch.no_grad():\n    test_preds = model(test_tensor).cpu().numpy()\n\nsubmission = pd.DataFrame(test_preds)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\n\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\n\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Model Definition: Residual MLP\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = nn.ReLU()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=128, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = torch.relu(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\nmodel = ResidualMLP().to(DEVICE)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\nbest_val_mse = float(\"inf\")\nepochs_without_improvement = 0\nbest_model_state = None\n\n# -------------------------\n# Training Loop with Early Stopping and Gradient Clipping\n# -------------------------\nfor epoch in range(MAX_EPOCHS):\n    model.train()\n    for batch_X, batch_y in train_loader:\n        batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n        optimizer.zero_grad()\n        preds = model(batch_X)\n        loss = criterion(preds, batch_y)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n\n    # Validation\n    model.eval()\n    val_preds = []\n    val_targets = []\n    with torch.no_grad():\n        for batch_X, batch_y in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds.append(out)\n            val_targets.append(batch_y.numpy())\n    val_preds = np.concatenate(val_preds, axis=0)\n    val_targets = np.concatenate(val_targets, axis=0)\n    val_mse = mean_squared_error(val_targets, val_preds)\n\n    # Check improvement\n    if val_mse < best_val_mse:\n        best_val_mse = val_mse\n        epochs_without_improvement = 0\n        best_model_state = model.state_dict()\n    else:\n        epochs_without_improvement += 1\n\n    if epochs_without_improvement >= PATIENCE:\n        break\n\n# Load best model state\nmodel.load_state_dict(best_model_state)\n\n# Final validation MSE\nmodel.eval()\nval_preds = []\nval_targets = []\nwith torch.no_grad():\n    for batch_X, batch_y in val_loader:\n        batch_X = batch_X.to(DEVICE)\n        out = model(batch_X).cpu().numpy()\n        val_preds.append(out)\n        val_targets.append(batch_y.numpy())\nval_preds = np.concatenate(val_preds, axis=0)\nval_targets = np.concatenate(val_targets, axis=0)\nval_mse = mean_squared_error(val_targets, val_preds)\nprint(f\"Validation MSE: {val_mse:.6f}\")\n\n# -------------------------\n# Inference on Test Set\n# -------------------------\ntest_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\nwith torch.no_grad():\n    test_preds = model(test_tensor).cpu().numpy()\n\nsubmission = pd.DataFrame(test_preds)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\n\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\n\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Model Definition: Residual MLP\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = nn.ReLU()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=128, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = torch.relu(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\nmodel = ResidualMLP().to(DEVICE)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n# Add a ReduceLROnPlateau scheduler\nscheduler = ReduceLROnPlateau(\n    optimizer, mode=\"min\", factor=0.5, patience=2, verbose=True\n)\n\nbest_val_mse = float(\"inf\")\nepochs_without_improvement = 0\nbest_model_state = None\n\n# -------------------------\n# Training Loop with Early Stopping & Scheduler\n# -------------------------\nfor epoch in range(MAX_EPOCHS):\n    model.train()\n    for batch_X, batch_y in train_loader:\n        batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n        optimizer.zero_grad()\n        preds = model(batch_X)\n        loss = criterion(preds, batch_y)\n        loss.backward()\n        optimizer.step()\n\n    # Validation\n    model.eval()\n    val_preds = []\n    val_targets = []\n    with torch.no_grad():\n        for batch_X, batch_y in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds.append(out)\n            val_targets.append(batch_y.numpy())\n    val_preds = np.concatenate(val_preds, axis=0)\n    val_targets = np.concatenate(val_targets, axis=0)\n    val_mse = mean_squared_error(val_targets, val_preds)\n\n    # Step the scheduler\n    scheduler.step(val_mse)\n\n    # Check improvement\n    if val_mse < best_val_mse:\n        best_val_mse = val_mse\n        epochs_without_improvement = 0\n        best_model_state = model.state_dict()\n    else:\n        epochs_without_improvement += 1\n\n    if epochs_without_improvement >= PATIENCE:\n        break\n\n# Load best model state\nmodel.load_state_dict(best_model_state)\n\n# Final validation MSE\nmodel.eval()\nval_preds = []\nval_targets = []\nwith torch.no_grad():\n    for batch_X, batch_y in val_loader:\n        batch_X = batch_X.to(DEVICE)\n        out = model(batch_X).cpu().numpy()\n        val_preds.append(out)\n        val_targets.append(batch_y.numpy())\nval_preds = np.concatenate(val_preds, axis=0)\nval_targets = np.concatenate(val_targets, axis=0)\nval_mse = mean_squared_error(val_targets, val_preds)\nprint(f\"Validation MSE: {val_mse:.6f}\")\n\n# -------------------------\n# Inference on Test Set\n# -------------------------\ntest_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\nwith torch.no_grad():\n    test_preds = model(test_tensor).cpu().numpy()\n\nsubmission = pd.DataFrame(test_preds)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\n\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\n\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Model Definition: Residual MLP\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = nn.ReLU()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=128, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = torch.relu(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\nmodel = ResidualMLP().to(DEVICE)\ncriterion = nn.MSELoss()\n\n# Use AdamW with a small weight decay\noptimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n\nscheduler = ReduceLROnPlateau(\n    optimizer, mode=\"min\", factor=0.5, patience=2, verbose=True\n)\n\nbest_val_mse = float(\"inf\")\nepochs_without_improvement = 0\nbest_model_state = None\n\n# -------------------------\n# Training Loop with Early Stopping & Scheduler\n# -------------------------\nfor epoch in range(MAX_EPOCHS):\n    model.train()\n    for batch_X, batch_y in train_loader:\n        batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n        optimizer.zero_grad()\n        preds = model(batch_X)\n        loss = criterion(preds, batch_y)\n        loss.backward()\n        optimizer.step()\n\n    # Validation\n    model.eval()\n    val_preds = []\n    val_targets = []\n    with torch.no_grad():\n        for batch_X, batch_y in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds.append(out)\n            val_targets.append(batch_y.numpy())\n    val_preds = np.concatenate(val_preds, axis=0)\n    val_targets = np.concatenate(val_targets, axis=0)\n    val_mse = mean_squared_error(val_targets, val_preds)\n\n    scheduler.step(val_mse)\n\n    if val_mse < best_val_mse:\n        best_val_mse = val_mse\n        epochs_without_improvement = 0\n        best_model_state = model.state_dict()\n    else:\n        epochs_without_improvement += 1\n\n    if epochs_without_improvement >= PATIENCE:\n        break\n\n# Load best model state\nmodel.load_state_dict(best_model_state)\n\n# Final validation MSE\nmodel.eval()\nval_preds = []\nval_targets = []\nwith torch.no_grad():\n    for batch_X, batch_y in val_loader:\n        batch_X = batch_X.to(DEVICE)\n        out = model(batch_X).cpu().numpy()\n        val_preds.append(out)\n        val_targets.append(batch_y.numpy())\nval_preds = np.concatenate(val_preds, axis=0)\nval_targets = np.concatenate(val_targets, axis=0)\nval_mse = mean_squared_error(val_targets, val_preds)\nprint(f\"Validation MSE: {val_mse:.6f}\")\n\n# -------------------------\n# Inference on Test Set\n# -------------------------\ntest_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\nmodel.eval()\nwith torch.no_grad():\n    test_preds = model(test_tensor).cpu().numpy()\n\nsubmission = pd.DataFrame(test_preds)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\n\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\n\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Model Definition: Residual MLP\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = nn.ReLU()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=128, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = torch.relu(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\nmodel = ResidualMLP().to(DEVICE)\ncriterion = nn.MSELoss()\n\n# Use AdamW with a small weight decay\noptimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n\n# Use CosineAnnealingWarmRestarts instead of ReduceLROnPlateau\nscheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=1, eta_min=1e-5)\n\nbest_val_mse = float(\"inf\")\nepochs_without_improvement = 0\nbest_model_state = None\n\n# -------------------------\n# Training Loop with Early Stopping & Scheduler\n# -------------------------\nfor epoch in range(MAX_EPOCHS):\n    model.train()\n    for batch_X, batch_y in train_loader:\n        batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n        optimizer.zero_grad()\n        preds = model(batch_X)\n        loss = criterion(preds, batch_y)\n        loss.backward()\n        optimizer.step()\n\n    # Validation\n    model.eval()\n    val_preds = []\n    val_targets = []\n    with torch.no_grad():\n        for batch_X, batch_y in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds.append(out)\n            val_targets.append(batch_y.numpy())\n    val_preds = np.concatenate(val_preds, axis=0)\n    val_targets = np.concatenate(val_targets, axis=0)\n    val_mse = mean_squared_error(val_targets, val_preds)\n\n    scheduler.step(epoch)  # step the new scheduler\n\n    if val_mse < best_val_mse:\n        best_val_mse = val_mse\n        epochs_without_improvement = 0\n        best_model_state = model.state_dict()\n    else:\n        epochs_without_improvement += 1\n\n    if epochs_without_improvement >= PATIENCE:\n        break\n\n# Load best model state\nmodel.load_state_dict(best_model_state)\n\n# Final validation MSE\nmodel.eval()\nval_preds = []\nval_targets = []\nwith torch.no_grad():\n    for batch_X, batch_y in val_loader:\n        batch_X = batch_X.to(DEVICE)\n        out = model(batch_X).cpu().numpy()\n        val_preds.append(out)\n        val_targets.append(batch_y.numpy())\nval_preds = np.concatenate(val_preds, axis=0)\nval_targets = np.concatenate(val_targets, axis=0)\nval_mse = mean_squared_error(val_targets, val_preds)\nprint(f\"Validation MSE: {val_mse:.6f}\")\n\n# -------------------------\n# Inference on Test Set\n# -------------------------\ntest_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\nmodel.eval()\nwith torch.no_grad():\n    test_preds = model(test_tensor).cpu().numpy()\n\nsubmission = pd.DataFrame(test_preds)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\n\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\n\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Model Definition: Residual MLP\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = nn.ReLU()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=128, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = torch.relu(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\nmodel = ResidualMLP().to(DEVICE)\ncriterion = nn.MSELoss()\noptimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n\n# Set up the OneCycleLR scheduler\nscheduler = OneCycleLR(\n    optimizer=optimizer,\n    max_lr=LEARNING_RATE,\n    steps_per_epoch=len(train_loader),\n    epochs=MAX_EPOCHS,\n)\n\nbest_val_mse = float(\"inf\")\nepochs_without_improvement = 0\nbest_model_state = None\n\n# -------------------------\n# Training Loop with Early Stopping & OneCycleLR\n# -------------------------\nfor epoch in range(MAX_EPOCHS):\n    model.train()\n    for batch_X, batch_y in train_loader:\n        batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n        optimizer.zero_grad()\n        preds = model(batch_X)\n        loss = criterion(preds, batch_y)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n    # Validation\n    model.eval()\n    val_preds = []\n    val_targets = []\n    with torch.no_grad():\n        for batch_X, batch_y in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds.append(out)\n            val_targets.append(batch_y.numpy())\n    val_preds = np.concatenate(val_preds, axis=0)\n    val_targets = np.concatenate(val_targets, axis=0)\n    val_mse = mean_squared_error(val_targets, val_preds)\n\n    if val_mse < best_val_mse:\n        best_val_mse = val_mse\n        epochs_without_improvement = 0\n        best_model_state = model.state_dict()\n    else:\n        epochs_without_improvement += 1\n\n    if epochs_without_improvement >= PATIENCE:\n        break\n\n# Load best model state\nmodel.load_state_dict(best_model_state)\n\n# Final validation MSE\nmodel.eval()\nval_preds = []\nval_targets = []\nwith torch.no_grad():\n    for batch_X, batch_y in val_loader:\n        batch_X = batch_X.to(DEVICE)\n        out = model(batch_X).cpu().numpy()\n        val_preds.append(out)\n        val_targets.append(batch_y.numpy())\nval_preds = np.concatenate(val_preds, axis=0)\nval_targets = np.concatenate(val_targets, axis=0)\nval_mse = mean_squared_error(val_targets, val_preds)\nprint(f\"Validation MSE: {val_mse:.6f}\")\n\n# -------------------------\n# Inference on Test Set\n# -------------------------\ntest_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\nmodel.eval()\nwith torch.no_grad():\n    test_preds = model(test_tensor).cpu().numpy()\n\nsubmission = pd.DataFrame(test_preds)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom torch.optim.lr_scheduler import OneCycleLR\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\n\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\n\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Model Definition: Residual MLP\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = nn.ReLU()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=128, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = torch.relu(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\nmodel = ResidualMLP().to(DEVICE)\ncriterion = nn.MSELoss()\noptimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n\n# OneCycleLR Scheduler\nscheduler = OneCycleLR(\n    optimizer,\n    max_lr=LEARNING_RATE,\n    steps_per_epoch=len(train_loader),\n    epochs=MAX_EPOCHS,\n)\n\nbest_val_mse = float(\"inf\")\nepochs_without_improvement = 0\nbest_model_state = None\n\n# -------------------------\n# Training Loop with Early Stopping\n# -------------------------\nfor epoch in range(MAX_EPOCHS):\n    model.train()\n    for batch_X, batch_y in train_loader:\n        batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n        optimizer.zero_grad()\n        preds = model(batch_X)\n        loss = criterion(preds, batch_y)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n    # Validation\n    model.eval()\n    val_preds = []\n    val_targets = []\n    with torch.no_grad():\n        for batch_X, batch_y in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds.append(out)\n            val_targets.append(batch_y.numpy())\n    val_preds = np.concatenate(val_preds, axis=0)\n    val_targets = np.concatenate(val_targets, axis=0)\n    val_mse = mean_squared_error(val_targets, val_preds)\n\n    if val_mse < best_val_mse:\n        best_val_mse = val_mse\n        epochs_without_improvement = 0\n        best_model_state = model.state_dict()\n    else:\n        epochs_without_improvement += 1\n\n    if epochs_without_improvement >= PATIENCE:\n        break\n\n# Load best model state\nmodel.load_state_dict(best_model_state)\n\n# Final validation MSE\nmodel.eval()\nval_preds = []\nval_targets = []\nwith torch.no_grad():\n    for batch_X, batch_y in val_loader:\n        batch_X = batch_X.to(DEVICE)\n        out = model(batch_X).cpu().numpy()\n        val_preds.append(out)\n        val_targets.append(batch_y.numpy())\nval_preds = np.concatenate(val_preds, axis=0)\nval_targets = np.concatenate(val_targets, axis=0)\nval_mse = mean_squared_error(val_targets, val_preds)\nprint(f\"Validation MSE: {val_mse:.6f}\")\n\n# -------------------------\n# Inference on Test Set\n# -------------------------\ntest_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\nmodel.eval()\nwith torch.no_grad():\n    test_preds = model(test_tensor).cpu().numpy()\n\nsubmission = pd.DataFrame(test_preds)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSEEDS = [0, 1, 2]  # Ensemble with 3 seeds\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\n\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\n\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Model Definition: Residual MLP\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = nn.ReLU()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=128, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = torch.relu(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    # Define model, criterion, optimizer, scheduler\n    model = ResidualMLP().to(DEVICE)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    scheduler = ReduceLROnPlateau(\n        optimizer, mode=\"min\", factor=0.5, patience=2, verbose=False\n    )\n\n    best_val_mse = float(\"inf\")\n    best_model_state = None\n    epochs_without_improvement = 0\n\n    # Training loop with early stopping\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            optimizer.step()\n\n        # Validation\n        model.eval()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        scheduler.step(val_mse)\n\n        if val_mse < best_val_mse:\n            best_val_mse = val_mse\n            epochs_without_improvement = 0\n            best_model_state = model.state_dict()\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best model state\n    model.load_state_dict(best_model_state)\n\n    # Final validation predictions\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    return val_preds_cat, test_preds, best_val_mse\n\n\n# -------------------------\n# Ensemble Training\n# -------------------------\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\n# Average validation predictions across seeds\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\n\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\n# Average test predictions across seeds\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSEEDS = [0, 1, 2, 3, 4]  # Increased ensemble to 5 seeds\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\n\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\n\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Model Definition: Residual MLP\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = nn.ReLU()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=128, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = torch.relu(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    # Define model, criterion, optimizer, scheduler\n    model = ResidualMLP().to(DEVICE)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    scheduler = ReduceLROnPlateau(\n        optimizer, mode=\"min\", factor=0.5, patience=2, verbose=False\n    )\n\n    best_val_mse = float(\"inf\")\n    best_model_state = None\n    epochs_without_improvement = 0\n\n    # Training loop with early stopping\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            optimizer.step()\n\n        # Validation\n        model.eval()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        scheduler.step(val_mse)\n\n        if val_mse < best_val_mse:\n            best_val_mse = val_mse\n            epochs_without_improvement = 0\n            best_model_state = model.state_dict()\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best model state\n    model.load_state_dict(best_model_state)\n\n    # Final validation predictions\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    return val_preds_cat, test_preds, best_val_mse\n\n\n# -------------------------\n# Ensemble Training\n# -------------------------\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\n# Average validation predictions across seeds\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\n\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\n# Average test predictions across seeds\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSEEDS = [0, 1, 2, 3, 4]  # Ensemble with 5 seeds\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Define Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# Model Definition: Residual MLP with Swish\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=128, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    # Define model, criterion, optimizer, scheduler\n    model = ResidualMLP().to(DEVICE)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    scheduler = ReduceLROnPlateau(\n        optimizer, mode=\"min\", factor=0.5, patience=2, verbose=False\n    )\n\n    best_val_mse = float(\"inf\")\n    best_model_state = None\n    epochs_without_improvement = 0\n\n    # Training loop with early stopping\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            optimizer.step()\n\n        # Validation\n        model.eval()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        scheduler.step(val_mse)\n\n        if val_mse < best_val_mse:\n            best_val_mse = val_mse\n            epochs_without_improvement = 0\n            best_model_state = model.state_dict()\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best model state\n    model.load_state_dict(best_model_state)\n\n    # Final validation predictions\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    return val_preds_cat, test_preds, best_val_mse\n\n\n# -------------------------\n# Ensemble Training\n# -------------------------\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\n# Average validation predictions across seeds\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\n# Average test predictions across seeds\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSEEDS = [0, 1, 2, 3, 4]  # Ensemble with 5 seeds\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Define Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name, param in self.shadow.items():\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Model Definition: Residual MLP with Swish\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=128, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    # Define model, criterion, optimizer, scheduler\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    scheduler = ReduceLROnPlateau(\n        optimizer, mode=\"min\", factor=0.5, patience=2, verbose=False\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    # Training loop with early stopping\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            optimizer.step()\n            ema.update()\n\n        # Validation with EMA weights\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        scheduler.step(val_mse_ema)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best EMA weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions with best EMA\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions with best EMA\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\n# -------------------------\n# Ensemble Training\n# -------------------------\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\n# Average validation predictions across seeds\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\n# Average test predictions across seeds\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSEEDS = [0, 1, 2, 3, 4]  # Ensemble with 5 seeds\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Define Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Model Definition: Residual MLP with Swish\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    # Define model, criterion, optimizer, scheduler\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    scheduler = ReduceLROnPlateau(\n        optimizer, mode=\"min\", factor=0.5, patience=2, verbose=False\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    # Training loop with early stopping\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            optimizer.step()\n            ema.update()\n\n        # Validation with EMA weights\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        scheduler.step(val_mse_ema)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best EMA weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions with best EMA\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions with best EMA\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\n# -------------------------\n# Ensemble Training\n# -------------------------\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\n# Average validation predictions across seeds\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\n# Average test predictions across seeds\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Define Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Model Definition: Residual MLP with Swish\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    # Define model, criterion, optimizer, scheduler\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    scheduler = ReduceLROnPlateau(\n        optimizer, mode=\"min\", factor=0.5, patience=2, verbose=False\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    # Training loop with early stopping\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            optimizer.step()\n            ema.update()\n\n        # Validation with EMA weights\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        scheduler.step(val_mse_ema)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best EMA weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions with best EMA\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions with best EMA\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\n# -------------------------\n# Ensemble Training\n# -------------------------\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\n# Average validation predictions across seeds\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\n# Average test predictions across seeds\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Define Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Model Definition: Residual MLP with BatchNorm + Swish\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.bn1 = nn.BatchNorm1d(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.bn2 = nn.BatchNorm1d(dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.fc1(x)\n        out = self.bn1(out)\n        out = self.act(out)\n        out = self.fc2(out)\n        out = self.bn2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    # Define model, criterion, optimizer, scheduler\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    scheduler = ReduceLROnPlateau(\n        optimizer, mode=\"min\", factor=0.5, patience=2, verbose=False\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    # Training loop with early stopping\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            optimizer.step()\n            ema.update()\n\n        # Validation with EMA weights\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        scheduler.step(val_mse_ema)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best EMA weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions with best EMA\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions with best EMA\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\n# -------------------------\n# Ensemble Training\n# -------------------------\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\n# Average validation predictions across seeds\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\n# Average test predictions across seeds\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDROP_PROB = 0.2\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Define Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with Dropout\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, drop_prob=0.2):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n        self.dropout = nn.Dropout(drop_prob)\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.dropout(out)\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\n# -------------------------\n# Model Definition: Residual MLP with Swish + Dropout\n# -------------------------\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim, drop_prob=DROP_PROB) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    # Define model, criterion, optimizer, scheduler\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    scheduler = ReduceLROnPlateau(\n        optimizer, mode=\"min\", factor=0.5, patience=2, verbose=False\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    # Training loop with early stopping\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            optimizer.step()\n            ema.update()\n\n        # Validation with EMA weights\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        scheduler.step(val_mse_ema)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best EMA weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions with best EMA\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions with best EMA\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\n# -------------------------\n# Ensemble Training\n# -------------------------\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\n# Average validation predictions across seeds\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\n# Average test predictions across seeds\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Define Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with BatchNorm\n# -------------------------\nclass BNResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.bn1 = nn.BatchNorm1d(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.bn2 = nn.BatchNorm1d(dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.fc1(x)\n        out = self.bn1(out)\n        out = self.act(out)\n        out = self.fc2(out)\n        out = self.bn2(out)\n        return self.act(out + identity)\n\n\n# -------------------------\n# Model Definition: Residual MLP with Swish\n# -------------------------\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [BNResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    # Define model, criterion, optimizer, scheduler\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    scheduler = ReduceLROnPlateau(\n        optimizer, mode=\"min\", factor=0.5, patience=2, verbose=False\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            optimizer.step()\n            ema.update()\n\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        scheduler.step(val_mse_ema)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best EMA weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\n# -------------------------\n# Ensemble Training\n# -------------------------\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\n# Average validation predictions across seeds\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\n# Average test predictions across seeds\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n\n# -------------------------\n# Lookahead Optimizer\n# -------------------------\nclass Lookahead(optim.Optimizer):\n    def __init__(self, optimizer, alpha=0.5, k=6):\n        self.optimizer = optimizer\n        self.param_groups = self.optimizer.param_groups\n        self.alpha = alpha\n        self.k = k\n        self.step_counter = 0\n\n        # Cache the current optimizer parameters\n        for group in self.param_groups:\n            group[\"cached_params\"] = []\n            for p in group[\"params\"]:\n                cached = torch.zeros_like(p.data)\n                cached.copy_(p.data)\n                group[\"cached_params\"].append(cached)\n\n    def step(self, closure=None):\n        loss = self.optimizer.step(closure)\n        self.step_counter += 1\n\n        # Perform a Lookahead update\n        if self.step_counter % self.k == 0:\n            for group in self.param_groups:\n                for p, q in zip(group[\"params\"], group[\"cached_params\"]):\n                    if p.requires_grad:\n                        q.data.add_(self.alpha, p.data - q.data)\n                        p.data.copy_(q.data)\n        return loss\n\n    def zero_grad(self):\n        self.optimizer.zero_grad()\n\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Define Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Model Definition: Residual MLP with Swish\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    # Define model, criterion\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n\n    # Wrap AdamW with Lookahead\n    base_optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    optimizer = Lookahead(base_optimizer, alpha=0.5, k=6)\n\n    scheduler = ReduceLROnPlateau(  # keep the same scheduler\n        base_optimizer, mode=\"min\", factor=0.5, patience=2, verbose=False\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    # Training loop with early stopping\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            optimizer.step()\n            ema.update()\n\n        # Validation with EMA weights\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        scheduler.step(val_mse_ema)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best EMA weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions with best EMA\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions with best EMA\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\n# -------------------------\n# Ensemble Training\n# -------------------------\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\n# Average validation predictions across seeds\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\n# Average test predictions across seeds\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSEEDS = list(range(10))  # Ensemble with 10 seeds\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Define Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Model Definition: Residual MLP with Swish\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # OneCycleLR scheduler\n    steps_per_epoch = len(train_loader)\n    scheduler = OneCycleLR(\n        optimizer, max_lr=LEARNING_RATE, total_steps=MAX_EPOCHS * steps_per_epoch\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            optimizer.step()\n            ema.update()\n            scheduler.step()\n\n        # Validation with EMA weights\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best EMA weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions with best EMA\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions with best EMA\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\n# -------------------------\n# Ensemble Training\n# -------------------------\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n\n# -------------------------\n# Lookahead Optimizer\n# -------------------------\nclass Lookahead(optim.Optimizer):\n    def __init__(self, base_optimizer, k=5, alpha=0.5):\n        if not 0.0 <= alpha <= 1.0:\n            raise ValueError(f\"Alpha must be in [0,1], got {alpha}\")\n        if not 1 <= k:\n            raise ValueError(f\"k must be an integer >= 1, got {k}\")\n        self.base_optimizer = base_optimizer\n        self.k = k\n        self.alpha = alpha\n        self.step_counter = 0\n        self.param_groups = self.base_optimizer.param_groups\n        self.slow_weights = []\n        for group in self.param_groups:\n            slow_weights_group = []\n            for p in group[\"params\"]:\n                slow_weights_group.append(p.data.clone().detach())\n            self.slow_weights.append(slow_weights_group)\n\n    def step(self, closure=None):\n        loss = self.base_optimizer.step(closure)\n        self.step_counter += 1\n        if self.step_counter % self.k == 0:\n            for group_index, group in enumerate(self.param_groups):\n                for p_index, p in enumerate(group[\"params\"]):\n                    p.data = self.slow_weights[group_index][\n                        p_index\n                    ] * self.alpha + p.data * (1.0 - self.alpha)\n                    self.slow_weights[group_index][p_index] = p.data.clone().detach()\n        return loss\n\n    def zero_grad(self):\n        self.base_optimizer.zero_grad()\n\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Define Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Model Definition: Residual MLP with Swish\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    # Define model, criterion\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n\n    # Base optimizer and Lookahead\n    base_optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    optimizer = Lookahead(base_optimizer, k=5, alpha=0.5)\n\n    scheduler = ReduceLROnPlateau(\n        optimizer.base_optimizer, mode=\"min\", factor=0.5, patience=2, verbose=False\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    # Training loop with early stopping\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            optimizer.step()\n            ema.update()\n\n        # Validation with EMA weights\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        scheduler.step(val_mse_ema)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best EMA weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions with best EMA\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions with best EMA\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\n# -------------------------\n# Ensemble Training\n# -------------------------\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\n# Average validation predictions across seeds\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\n# Average test predictions across seeds\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSEEDS = list(range(10))  # Ensemble with 10 seeds\n\n\n# -------------------------\n# Lookahead Optimizer\n# -------------------------\nclass Lookahead(optim.Optimizer):\n    def __init__(self, optimizer, alpha=0.5, k=5):\n        if not 0.0 <= alpha <= 1.0:\n            raise ValueError(f\"Invalid alpha: {alpha}\")\n        if not 1 <= k:\n            raise ValueError(f\"Invalid k: {k}\")\n        defaults = {}\n        super().__init__(optimizer.param_groups, defaults)\n        self.optimizer = optimizer\n        self.alpha = alpha\n        self.k = k\n        self.step_counter = 0\n        # Create the slow weights\n        self.slow_weights = []\n        for group in self.optimizer.param_groups:\n            group_slow_weights = []\n            for p in group[\"params\"]:\n                if p.requires_grad:\n                    group_slow_weights.append(p.data.clone().detach())\n            self.slow_weights.append(group_slow_weights)\n\n    def step(self, closure=None):\n        loss = self.optimizer.step(closure)\n        self.step_counter += 1\n\n        # Lookahead updates\n        if self.step_counter % self.k == 0:\n            for group_idx, group in enumerate(self.optimizer.param_groups):\n                for p_idx, p in enumerate(group[\"params\"]):\n                    if p.requires_grad:\n                        slow = self.slow_weights[group_idx][p_idx]\n                        fast = p.data\n                        # Merge fast weights into slow weights\n                        self.slow_weights[group_idx][p_idx] = slow + self.alpha * (\n                            fast - slow\n                        )\n                        p.data = self.slow_weights[group_idx][p_idx]\n        return loss\n\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Define Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Model Definition: Residual MLP with Swish\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n\n    base_optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    # Wrap the base optimizer with Lookahead\n    optimizer = Lookahead(base_optimizer, alpha=0.5, k=5)\n\n    scheduler = ReduceLROnPlateau(\n        optimizer.optimizer, mode=\"min\", factor=0.5, patience=2, verbose=False\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            optimizer.step()\n            ema.update()\n\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        scheduler.step(val_mse_ema)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Define Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Model Definition: Residual MLP with Swish\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    # Define model, criterion, optimizer, scheduler\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    scheduler = ReduceLROnPlateau(\n        optimizer, mode=\"min\", factor=0.5, patience=2, verbose=False\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    # Training loop with early stopping\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n\n            # Gradient Clipping\n            clip_grad_norm_(model.parameters(), 1.0)\n\n            optimizer.step()\n            ema.update()\n\n        # Validation with EMA weights\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        scheduler.step(val_mse_ema)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best EMA weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions with best EMA\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions with best EMA\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\n# -------------------------\n# Ensemble Training\n# -------------------------\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\n# Average validation predictions across seeds\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\n# Average test predictions across seeds\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Define Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Model Definition: Residual MLP with Swish\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    # Define model, criterion, optimizer\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # Define OneCycleLR scheduler (stepped once per batch)\n    steps_per_epoch = len(train_loader)\n    scheduler = OneCycleLR(\n        optimizer,\n        max_lr=LEARNING_RATE,\n        steps_per_epoch=steps_per_epoch,\n        epochs=MAX_EPOCHS,\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n\n            # Gradient Clipping\n            clip_grad_norm_(model.parameters(), 1.0)\n\n            optimizer.step()\n            scheduler.step()\n            ema.update()\n\n        # Validation with EMA weights\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best EMA weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions with best EMA\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions with best EMA\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\n# -------------------------\n# Ensemble Training\n# -------------------------\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\n# Average validation predictions across seeds\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\n# Average test predictions across seeds\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Define Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Lookahead Optimizer\n# -------------------------\nclass Lookahead(torch.optim.Optimizer):\n    def __init__(self, optimizer, alpha=0.5, k=6):\n        if not 0.0 <= alpha <= 1.0:\n            raise ValueError(f\"Invalid alpha: {alpha}\")\n        if not k >= 1:\n            raise ValueError(f\"Invalid k: {k}\")\n        self.optimizer = optimizer\n        self.alpha = alpha\n        self.k = k\n        self.step_counter = 0\n        self.param_groups = self.optimizer.param_groups\n        self.slow_params = []\n        for group in self.param_groups:\n            sp = []\n            for p in group[\"params\"]:\n                sp.append(p.data.clone())\n            self.slow_params.append(sp)\n\n    def step(self, closure=None):\n        loss = self.optimizer.step(closure)\n        self.step_counter += 1\n        if self.step_counter % self.k == 0:\n            for group_idx, group in enumerate(self.param_groups):\n                for p_idx, p in enumerate(group[\"params\"]):\n                    if p.grad is None:\n                        continue\n                    slow = self.slow_params[group_idx][p_idx]\n                    slow += self.alpha * (p.data - slow)\n                    p.data = slow.clone()\n        return loss\n\n    def zero_grad(self):\n        self.optimizer.zero_grad()\n\n\n# -------------------------\n# Model Definition: Residual MLP with Swish\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    # Define model, criterion, base optimizer, lookahead, scheduler\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    base_optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    optimizer = Lookahead(base_optimizer, alpha=0.5, k=6)\n\n    # We'll keep using ReduceLROnPlateau here\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        base_optimizer, mode=\"min\", factor=0.5, patience=2, verbose=False\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    # Training loop with early stopping\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n\n            # Gradient Clipping\n            clip_grad_norm_(model.parameters(), 1.0)\n\n            optimizer.step()\n            ema.update()\n\n        # Validation with EMA weights\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        scheduler.step(val_mse_ema)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best EMA weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions with best EMA\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions with best EMA\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\n# -------------------------\n# Ensemble Training\n# -------------------------\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\n# Average validation predictions across seeds\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\n# Average test predictions across seeds\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\nfrom torch.cuda.amp import autocast, GradScaler\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Define Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Model Definition: Residual MLP with Swish\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    scheduler = ReduceLROnPlateau(\n        optimizer, mode=\"min\", factor=0.5, patience=2, verbose=False\n    )\n    scaler = GradScaler()\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            with autocast():\n                preds = model(batch_X)\n                loss = criterion(preds, batch_y)\n\n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(optimizer)\n            scaler.update()\n            ema.update()\n\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        scheduler.step(val_mse_ema)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Define Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# Lookahead Optimizer\n# -------------------------\nclass Lookahead:\n    def __init__(self, base_optimizer, alpha=0.5, k=5):\n        self.base_optimizer = base_optimizer\n        self.alpha = alpha\n        self.k = k\n        self.step_counter = 0\n        self.fast_weights = []\n        for group in self.base_optimizer.param_groups:\n            group_fast = []\n            for p in group[\"params\"]:\n                group_fast.append(p.data.clone())\n            self.fast_weights.append(group_fast)\n\n    def zero_grad(self):\n        self.base_optimizer.zero_grad()\n\n    def step(self):\n        self.base_optimizer.step()\n        self.step_counter += 1\n\n        # Update slow weights every k steps\n        if self.step_counter % self.k == 0:\n            idx = 0\n            for group in self.base_optimizer.param_groups:\n                for p in group[\"params\"]:\n                    p.data = self.fast_weights[idx] + self.alpha * (\n                        p.data - self.fast_weights[idx]\n                    )\n                    self.fast_weights[idx] = p.data.clone()\n                    idx += 1\n        else:\n            # Just update the fast weights\n            idx = 0\n            for group in self.base_optimizer.param_groups:\n                for p in group[\"params\"]:\n                    self.fast_weights[idx] = p.data.clone()\n                    idx += 1\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Model Definition: Residual MLP with Swish\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    # Define model, criterion, optimizer, scheduler\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    base_optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    optimizer = Lookahead(base_optimizer, alpha=0.5, k=5)\n    scheduler = ReduceLROnPlateau(\n        base_optimizer, mode=\"min\", factor=0.5, patience=2, verbose=False\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    # Training loop with early stopping\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n\n            # Gradient Clipping\n            clip_grad_norm_(model.parameters(), 1.0)\n\n            optimizer.step()\n            ema.update()\n\n        # Validation with EMA weights\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        scheduler.step(val_mse_ema)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best EMA weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions with best EMA\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions with best EMA\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\n# -------------------------\n# Ensemble Training\n# -------------------------\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\n# Average validation predictions across seeds\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\n# Average test predictions across seeds\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Define Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# Lookahead Optimizer (Bugfix with nested loop)\n# -------------------------\nclass Lookahead:\n    def __init__(self, base_optimizer, alpha=0.5, k=5):\n        self.base_optimizer = base_optimizer\n        self.alpha = alpha\n        self.k = k\n        self.step_counter = 0\n        self.fast_weights = []\n        for group in self.base_optimizer.param_groups:\n            group_fast = []\n            for p in group[\"params\"]:\n                group_fast.append(p.data.clone())\n            self.fast_weights.append(group_fast)\n\n    def zero_grad(self):\n        self.base_optimizer.zero_grad()\n\n    def step(self):\n        self.base_optimizer.step()\n        self.step_counter += 1\n\n        if self.step_counter % self.k == 0:\n            for group_idx, group in enumerate(self.base_optimizer.param_groups):\n                for p_idx, p in enumerate(group[\"params\"]):\n                    p.data = self.fast_weights[group_idx][p_idx] + self.alpha * (\n                        p.data - self.fast_weights[group_idx][p_idx]\n                    )\n                    self.fast_weights[group_idx][p_idx] = p.data.clone()\n        else:\n            for group_idx, group in enumerate(self.base_optimizer.param_groups):\n                for p_idx, p in enumerate(group[\"params\"]):\n                    self.fast_weights[group_idx][p_idx] = p.data.clone()\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Model Definition: Residual MLP with Swish\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    # Define model, criterion, optimizer, scheduler\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    base_optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    optimizer = Lookahead(base_optimizer, alpha=0.5, k=5)\n    scheduler = ReduceLROnPlateau(\n        base_optimizer, mode=\"min\", factor=0.5, patience=2, verbose=False\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n\n            # Gradient Clipping\n            clip_grad_norm_(model.parameters(), 1.0)\n\n            optimizer.step()\n            ema.update()\n\n        # Validation with EMA weights\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        scheduler.step(val_mse_ema)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best EMA weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions with best EMA\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions with best EMA\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\n# -------------------------\n# Ensemble Training\n# -------------------------\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\n# Average validation predictions across seeds\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\n# Average test predictions across seeds\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Define Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Model Definition: Residual MLP with Swish\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    # Define model, criterion, optimizer\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # Replace ReduceLROnPlateau with OneCycleLR\n    scheduler = OneCycleLR(\n        optimizer,\n        max_lr=LEARNING_RATE,\n        steps_per_epoch=len(train_loader),\n        epochs=MAX_EPOCHS,\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        # Training\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n\n            # Gradient Clipping\n            clip_grad_norm_(model.parameters(), 1.0)\n\n            optimizer.step()\n            ema.update()\n            scheduler.step()\n\n        # Validation with EMA\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        # Early stopping logic\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best EMA weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\n# -------------------------\n# Ensemble Training\n# -------------------------\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn.utils import clip_grad_norm_\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Define Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Model Definition: Residual MLP with Swish\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    # Define model, criterion, optimizer\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # OneCycleLR scheduler\n    total_steps = len(train_loader) * MAX_EPOCHS\n    scheduler = OneCycleLR(optimizer, max_lr=LEARNING_RATE, total_steps=total_steps)\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    # Training loop with early stopping\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n\n            # Gradient Clipping\n            clip_grad_norm_(model.parameters(), 1.0)\n\n            optimizer.step()\n            ema.update()\n            scheduler.step()\n\n        # Validation with EMA weights\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best EMA weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions with best EMA\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions with best EMA\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\n# -------------------------\n# Ensemble Training\n# -------------------------\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\n# Average validation predictions across seeds\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\n# Average test predictions across seeds\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Define Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Model Definition: Residual MLP with Swish\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    # Define model, criterion, optimizer\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # Define OneCycleLR scheduler\n    steps_per_epoch = len(train_loader)\n    scheduler = OneCycleLR(\n        optimizer,\n        max_lr=LEARNING_RATE,\n        steps_per_epoch=steps_per_epoch,\n        epochs=MAX_EPOCHS,\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    # Training loop with early stopping\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n\n            # Gradient Clipping\n            clip_grad_norm_(model.parameters(), 1.0)\n\n            optimizer.step()\n            ema.update()\n            scheduler.step()  # step per batch\n\n        # Validation with EMA weights\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best EMA weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions with best EMA\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions with best EMA\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\n# -------------------------\n# Ensemble Training\n# -------------------------\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\n# Average validation predictions across seeds\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\n# Average test predictions across seeds\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Define Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Model Definition: Residual MLP with Swish\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    # Define model, criterion, optimizer, scheduler\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    # OneCycleLR scheduler\n    scheduler = OneCycleLR(\n        optimizer,\n        max_lr=LEARNING_RATE,\n        epochs=MAX_EPOCHS,\n        steps_per_epoch=len(train_loader),\n        pct_start=0.3,\n        anneal_strategy=\"cos\",\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    # Training loop with early stopping\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n\n            # Gradient Clipping\n            clip_grad_norm_(model.parameters(), 1.0)\n\n            optimizer.step()\n            scheduler.step()\n            ema.update()\n\n        # Validation with EMA weights\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best EMA weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions with best EMA\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions with best EMA\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\n# -------------------------\n# Ensemble Training\n# -------------------------\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\n# Average validation predictions across seeds\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\n# Average test predictions across seeds\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Define Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# SE Residual Block\n# -------------------------\nclass SEResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n        # Squeeze-and-Excitation layers\n        squeeze_dim = max(1, dim // reduction)\n        self.se_fc1 = nn.Linear(dim, squeeze_dim)\n        self.se_fc2 = nn.Linear(squeeze_dim, dim)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        identity = x\n        # Main residual mapping\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        # Squeeze-and-Excitation\n        z = self.se_fc1(out)  # [B, squeeze_dim]\n        z = self.act(z)\n        z = self.se_fc2(z)  # [B, dim]\n        z = self.sigmoid(z)\n        out = out * z\n        out = out + identity\n        out = self.act(out)\n        return out\n\n\n# -------------------------\n# Residual MLP with SE\n# -------------------------\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [SEResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    # Define model, criterion, optimizer, scheduler\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    scheduler = ReduceLROnPlateau(\n        optimizer, mode=\"min\", factor=0.5, patience=2, verbose=False\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    # Training loop with early stopping\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n\n            # Gradient Clipping\n            clip_grad_norm_(model.parameters(), 1.0)\n\n            optimizer.step()\n            ema.update()\n\n        # Validation with EMA weights\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        scheduler.step(val_mse_ema)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best EMA weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions with best EMA\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions with best EMA\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\n# -------------------------\n# Ensemble Training\n# -------------------------\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\n# Average validation predictions across seeds\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\n# Average test predictions across seeds\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Define Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Model Definition: Residual MLP with Swish + BatchNorm\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.bn1 = nn.BatchNorm1d(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.bn2 = nn.BatchNorm1d(dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.fc1(x)\n        out = self.bn1(out)\n        out = self.act(out)\n        out = self.fc2(out)\n        out = self.bn2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.bn_in = nn.BatchNorm1d(hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.input_layer(x)\n        x = self.bn_in(x)\n        x = self.act(x)\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    # Define model, criterion, optimizer, scheduler\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    scheduler = ReduceLROnPlateau(\n        optimizer, mode=\"min\", factor=0.5, patience=2, verbose=False\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    # Training loop with early stopping\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n\n            # Gradient Clipping\n            clip_grad_norm_(model.parameters(), 1.0)\n\n            optimizer.step()\n            ema.update()\n\n        # Validation with EMA weights\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        scheduler.step(val_mse_ema)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best EMA weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions with best EMA\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions with best EMA\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\n# -------------------------\n# Ensemble Training\n# -------------------------\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\n# Average validation predictions across seeds\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\n# Average test predictions across seeds\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Define Mish Activation\n# -------------------------\nclass Mish(nn.Module):\n    def forward(self, x):\n        return x * torch.tanh(nn.functional.softplus(x))\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Model Definition: Residual MLP with Mish\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Mish()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Mish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    scheduler = ReduceLROnPlateau(\n        optimizer, mode=\"min\", factor=0.5, patience=2, verbose=False\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n\n            # Gradient Clipping\n            clip_grad_norm_(model.parameters(), 1.0)\n\n            optimizer.step()\n            ema.update()\n\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        scheduler.step(val_mse_ema)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Define Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Model Definition: Residual MLP with Swish\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    from torch.optim.lr_scheduler import OneCycleLR\n\n    set_seed(seed)\n\n    # Define model, criterion, optimizer, scheduler\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    # OneCycleLR: total_steps = len(train_loader)*MAX_EPOCHS\n    scheduler = OneCycleLR(\n        optimizer,\n        max_lr=LEARNING_RATE,\n        steps_per_epoch=len(train_loader),\n        epochs=MAX_EPOCHS,\n        anneal_strategy=\"cos\",\n        pct_start=0.1,\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    # Training loop with early stopping\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n\n            # Gradient Clipping\n            clip_grad_norm_(model.parameters(), 1.0)\n\n            optimizer.step()\n            ema.update()\n            scheduler.step()\n\n        # Validation with EMA weights\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best EMA weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions with best EMA\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions with best EMA\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\n# -------------------------\n# Ensemble Training\n# -------------------------\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\n# Average validation predictions across seeds\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\n# Average test predictions across seeds\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Define Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Lookahead Wrapper\n# -------------------------\nclass Lookahead(optim.Optimizer):\n    def __init__(self, optimizer, alpha=0.5, k=6):\n        self.optimizer = optimizer\n        self.param_groups = self.optimizer.param_groups\n        self.alpha = alpha\n        self.k = k\n        self.step_counter = 0\n        # slow weights\n        self.slow_weights = [\n            [p.clone().detach() for p in group[\"params\"]] for group in self.param_groups\n        ]\n\n    def zero_grad(self):\n        self.optimizer.zero_grad()\n\n    def step(self):\n        loss = self.optimizer.step()\n        self.step_counter += 1\n\n        if self.step_counter % self.k == 0:\n            for group_idx, group in enumerate(self.param_groups):\n                for p_idx, p in enumerate(group[\"params\"]):\n                    if p.requires_grad:\n                        slow_weight = self.slow_weights[group_idx][p_idx]\n                        diff = p.data - slow_weight\n                        slow_weight += self.alpha * diff\n                        p.data = slow_weight\n        return loss\n\n\n# -------------------------\n# Model Definition: Residual MLP with Swish\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    # Define model, criterion\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n\n    # Base optimizer\n    base_optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    # Wrap with Lookahead\n    optimizer = Lookahead(base_optimizer, alpha=0.5, k=6)\n\n    scheduler = ReduceLROnPlateau(\n        optimizer.optimizer, mode=\"min\", factor=0.5, patience=2, verbose=False\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    # Training loop with early stopping\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n\n            # Gradient Clipping\n            clip_grad_norm_(model.parameters(), 1.0)\n\n            optimizer.step()\n            ema.update()\n\n        # Validation with EMA weights\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        scheduler.step(val_mse_ema)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best EMA weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions with best EMA\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\n# -------------------------\n# Ensemble Training\n# -------------------------\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\n# Average validation predictions\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\n# Average test predictions and save\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Define Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Model Definition: Residual MLP with Swish\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    # Define model, criterion, optimizer\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # OneCycleLR: total_steps = total_batches * epochs\n    total_steps = len(train_loader) * MAX_EPOCHS\n    scheduler = OneCycleLR(\n        optimizer, max_lr=LEARNING_RATE, total_steps=total_steps, anneal_strategy=\"cos\"\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    # Training loop with early stopping\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n\n            # Gradient Clipping\n            clip_grad_norm_(model.parameters(), 1.0)\n\n            optimizer.step()\n            ema.update()\n            scheduler.step()\n\n        # Validation with EMA weights\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best EMA weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions with best EMA\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions with best EMA\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\n# -------------------------\n# Ensemble Training\n# -------------------------\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\n# Average validation predictions across seeds\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\n# Average test predictions across seeds\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSEEDS = list(range(10))  # Ensemble with 10 seeds\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Define Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# ResidualBlock with Batch Norm\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.bn1 = nn.BatchNorm1d(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.bn2 = nn.BatchNorm1d(dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.fc1(x)\n        out = self.bn1(out)\n        out = self.act(out)\n        out = self.fc2(out)\n        out = self.bn2(out)\n        return self.act(out + identity)\n\n\n# -------------------------\n# Residual MLP\n# -------------------------\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    scheduler = ReduceLROnPlateau(\n        optimizer, mode=\"min\", factor=0.5, patience=2, verbose=False\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n\n        # Validation\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        scheduler.step(val_mse_ema)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation preds\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Define Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Model Definition: Residual MLP with Swish\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    # Define model, criterion, optimizer\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # Define OneCycleLR\n    total_steps = len(train_loader) * MAX_EPOCHS\n    scheduler = OneCycleLR(optimizer, max_lr=LEARNING_RATE, total_steps=total_steps)\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    # Training loop with early stopping\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n\n            # Gradient Clipping\n            clip_grad_norm_(model.parameters(), 1.0)\n\n            optimizer.step()\n            scheduler.step()\n            ema.update()\n\n        # Validation with EMA weights\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        # Manual early stopping\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best EMA weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions with best EMA\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions with best EMA\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\n# Convert to numpy\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\n# Scale\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\n# Torch datasets\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Define Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Model Definition: Residual MLP with Swish\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    # Define model, criterion, optimizer, scheduler\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # Define OneCycleLR\n    total_steps = len(train_loader) * MAX_EPOCHS\n    scheduler = OneCycleLR(\n        optimizer,\n        max_lr=LEARNING_RATE,\n        total_steps=total_steps,\n        pct_start=0.3,\n        anneal_strategy=\"cos\",\n        div_factor=25.0,\n        final_div_factor=1e4,\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    # Training loop with early stopping\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n\n            # Gradient Clipping\n            clip_grad_norm_(model.parameters(), 1.0)\n\n            optimizer.step()\n            ema.update()\n            scheduler.step()  # step after each batch\n\n        # Validation with EMA weights\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best EMA weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions with best EMA\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions with best EMA\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\n# -------------------------\n# Ensemble Training\n# -------------------------\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\n# Average validation predictions across seeds\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\n# Average test predictions across seeds\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual MLP with LayerNorm\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    scheduler = ReduceLROnPlateau(\n        optimizer, mode=\"min\", factor=0.5, patience=2, verbose=False\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        scheduler.step(val_mse_ema)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual MLP with LayerNorm\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # Replace ReduceLROnPlateau with OneCycleLR\n    total_steps = len(train_loader) * MAX_EPOCHS\n    scheduler = OneCycleLR(optimizer, max_lr=LEARNING_RATE, total_steps=total_steps)\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n    current_step = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n\n            scheduler.step()\n            current_step += 1\n\n        # Validation\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Final test predictions\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual MLP with LayerNorm\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # Replace ReduceLROnPlateau with OneCycleLR\n    total_steps = len(train_loader) * MAX_EPOCHS\n    scheduler = OneCycleLR(optimizer, max_lr=LEARNING_RATE, total_steps=total_steps)\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n            scheduler.step()\n\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual MLP with LayerNorm\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # Define OneCycleLR\n    steps_per_epoch = len(train_loader)\n    scheduler = OneCycleLR(\n        optimizer,\n        max_lr=LEARNING_RATE,\n        steps_per_epoch=steps_per_epoch,\n        epochs=MAX_EPOCHS,\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n            scheduler.step()  # step after each batch\n\n        # Validation\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Apply best weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Inference on test data\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    scheduler = ReduceLROnPlateau(\n        optimizer, mode=\"min\", factor=0.5, patience=2, verbose=False\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        scheduler.step(val_mse_ema)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE + Dropout\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n        self.dropout = nn.Dropout(p=0.2)  # Added dropout\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        out = self.dropout(out)  # Apply dropout after SE\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    scheduler = ReduceLROnPlateau(\n        optimizer, mode=\"min\", factor=0.5, patience=2, verbose=False\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        scheduler.step(val_mse_ema)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    # Use OneCycleLR instead of ReduceLROnPlateau\n    total_steps = MAX_EPOCHS * len(train_loader)\n    scheduler = OneCycleLR(optimizer, max_lr=LEARNING_RATE, total_steps=total_steps)\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n            scheduler.step()  # Step the OneCycleLR each batch\n\n        # Validation phase\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final inference on validation\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Inference on test set\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # Replace ReduceLROnPlateau with OneCycleLR\n    # We calculate total steps based on data loader length and max epochs\n    total_steps = len(train_loader) * MAX_EPOCHS\n    scheduler = OneCycleLR(\n        optimizer,\n        max_lr=LEARNING_RATE,\n        total_steps=total_steps,\n        anneal_strategy=\"cos\",\n        pct_start=0.3,\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n            # Step the OneCycleLR scheduler after each batch\n            scheduler.step()\n\n        # Validation\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load the best shadow parameters\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # Use OneCycleLR instead of ReduceLROnPlateau\n    steps_per_epoch = len(train_loader)\n    total_steps = steps_per_epoch * MAX_EPOCHS\n    scheduler = OneCycleLR(\n        optimizer,\n        max_lr=LEARNING_RATE,\n        total_steps=total_steps,\n        pct_start=0.3,\n        anneal_strategy=\"cos\",\n        div_factor=25.0,\n        final_div_factor=10000.0,\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n            scheduler.step()  # Step the OneCycleLR after each batch\n\n        # Validation\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        # Manual early stopping\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Inference on test set\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\ns_test_dummy = pd.read_csv(\n    \"./input/test_s.csv\"\n)  # Not used for training, only for format\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with Squeeze-and-Excitation\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)\n        se = self.act(se)\n        se = self.se_up(se)\n        se = torch.sigmoid(se)\n        out = out * se\n\n        return self.act(out + identity)\n\n\n# -------------------------\n# Residual MLP\n# -------------------------\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\n# -------------------------\n# Utilities\n# -------------------------\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # Replace ReduceLROnPlateau with OneCycleLR\n    scheduler = OneCycleLR(\n        optimizer,\n        max_lr=LEARNING_RATE,\n        steps_per_epoch=len(train_loader),\n        epochs=MAX_EPOCHS,\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n            scheduler.step()\n\n        # Validation\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # OneCycleLR Scheduler\n    total_steps = len(train_loader) * MAX_EPOCHS\n    scheduler = OneCycleLR(\n        optimizer,\n        max_lr=LEARNING_RATE,\n        total_steps=total_steps,\n        pct_start=0.3,\n        anneal_strategy=\"cos\",\n        div_factor=25.0,\n        final_div_factor=100.0,\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n            scheduler.step()\n\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    from torch.optim.lr_scheduler import OneCycleLR\n\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    total_steps = len(train_loader) * MAX_EPOCHS\n    scheduler = OneCycleLR(\n        optimizer,\n        max_lr=LEARNING_RATE,\n        total_steps=total_steps,\n        pct_start=0.3,\n        anneal_strategy=\"cos\",\n        div_factor=25.0,\n        final_div_factor=1e4,\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n    step_count = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n\n            scheduler.step()  # step OneCycleLR after each batch\n            step_count += 1\n\n        # Validation\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n        if step_count >= total_steps:\n            break\n\n    # Load best weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Final test predictions\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn.utils import clip_grad_norm_\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Shape: (N, 1)\n        se = self.se_down(se)  # Shape: (N, dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Shape: (N, dim)\n        se = torch.sigmoid(se)\n        out = out * se\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    # Replace ReduceLROnPlateau with OneCycleLR\n    total_steps = len(train_loader) * MAX_EPOCHS\n    scheduler = OneCycleLR(optimizer, max_lr=LEARNING_RATE, total_steps=total_steps)\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()  # step the OneCycleLR after each batch\n            ema.update()\n\n        # Validation at the end of each epoch\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    total_steps = len(train_loader) * MAX_EPOCHS\n    scheduler = OneCycleLR(optimizer, max_lr=LEARNING_RATE, total_steps=total_steps)\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            ema.update()\n\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # Replace ReduceLROnPlateau with OneCycleLR\n    total_steps = len(train_loader) * MAX_EPOCHS\n    scheduler = OneCycleLR(optimizer, max_lr=LEARNING_RATE, total_steps=total_steps)\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n            scheduler.step()\n\n        # Validation\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best EMA weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # OneCycleLR replacing ReduceLROnPlateau\n    steps_per_epoch = len(train_loader)\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n        optimizer,\n        max_lr=LEARNING_RATE,\n        steps_per_epoch=steps_per_epoch,\n        epochs=MAX_EPOCHS,\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n            scheduler.step()\n\n        # Validation\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Restore best weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # -------------------------\n    # Replace ReduceLROnPlateau with OneCycleLR\n    # -------------------------\n    total_steps = len(train_loader) * MAX_EPOCHS\n    scheduler = optim.lr_scheduler.OneCycleLR(\n        optimizer,\n        max_lr=LEARNING_RATE,\n        total_steps=total_steps,\n        pct_start=0.3,\n        anneal_strategy=\"cos\",\n        div_factor=25.0,\n        final_div_factor=1e4,\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n            scheduler.step()  # Step the OneCycleLR after each batch\n\n        # Validation step\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        # Early stopping logic\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # Set up OneCycleLR\n    total_steps = len(train_loader) * MAX_EPOCHS\n    scheduler = OneCycleLR(optimizer, max_lr=LEARNING_RATE, total_steps=total_steps)\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()  # Step once per batch\n            ema.update()\n\n        # Validation\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best shadow and produce final predictions\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # Replace ReduceLROnPlateau with OneCycleLR\n    total_steps = len(train_loader) * MAX_EPOCHS\n    scheduler = OneCycleLR(optimizer, max_lr=LEARNING_RATE, total_steps=total_steps)\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n            scheduler.step()\n\n        # Validation\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    from torch.optim.lr_scheduler import OneCycleLR\n\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # OneCycleLR setup\n    total_steps = len(train_loader) * MAX_EPOCHS\n    scheduler = OneCycleLR(optimizer, max_lr=LEARNING_RATE, total_steps=total_steps)\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            ema.update()\n\n        # Validation MSE under EMA weights\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best EMA shadow\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Final test predictions\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # Replace ReduceLROnPlateau with OneCycleLR\n    steps_per_epoch = len(train_loader)\n    scheduler = OneCycleLR(\n        optimizer,\n        max_lr=LEARNING_RATE,\n        steps_per_epoch=steps_per_epoch,\n        epochs=MAX_EPOCHS,\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n            scheduler.step()\n\n        # Validation\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # OneCycleLR setup: total_steps = len(train_loader) * MAX_EPOCHS\n    total_steps = len(train_loader) * MAX_EPOCHS\n    scheduler = OneCycleLR(optimizer, max_lr=LEARNING_RATE, total_steps=total_steps)\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            ema.update()\n\n        # Validation\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # Replacing ReduceLROnPlateau with OneCycleLR\n    scheduler = OneCycleLR(\n        optimizer,\n        max_lr=LEARNING_RATE,\n        steps_per_epoch=len(train_loader),\n        epochs=MAX_EPOCHS,\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n            scheduler.step()\n\n        # Validation and early stopping\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    # Replace ReduceLROnPlateau with OneCycleLR\n    scheduler = OneCycleLR(\n        optimizer,\n        max_lr=LEARNING_RATE,\n        steps_per_epoch=len(train_loader),\n        epochs=MAX_EPOCHS,\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n            scheduler.step()\n\n        # Validation\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    # Replace ReduceLROnPlateau with OneCycleLR\n    total_steps = len(train_loader) * MAX_EPOCHS\n    scheduler = OneCycleLR(optimizer, max_lr=LEARNING_RATE, total_steps=total_steps)\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n            scheduler.step()  # step per batch\n\n        # Validation\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    # Replace ReduceLROnPlateau with OneCycleLR\n    scheduler = OneCycleLR(\n        optimizer,\n        max_lr=LEARNING_RATE,\n        steps_per_epoch=len(train_loader),\n        epochs=MAX_EPOCHS,\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n            scheduler.step()\n\n        # Validation and Early Stopping\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best weights and compute final predictions\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    steps_per_epoch = len(train_loader)\n    total_steps = MAX_EPOCHS * steps_per_epoch\n    scheduler = OneCycleLR(\n        optimizer,\n        max_lr=LEARNING_RATE,\n        total_steps=total_steps,\n        pct_start=0.3,\n        anneal_strategy=\"cos\",\n        div_factor=25.0,\n        final_div_factor=1e4,\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            ema.update()\n\n        # Validation\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSEEDS = list(range(10))  # ensemble with 10 seeds\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # Replace ReduceLROnPlateau with OneCycleLR\n    total_steps = len(train_loader) * MAX_EPOCHS\n    scheduler = OneCycleLR(\n        optimizer,\n        max_lr=LEARNING_RATE,\n        total_steps=total_steps,\n        pct_start=0.3,\n        anneal_strategy=\"cos\",\n        cycle_momentum=False,\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n            scheduler.step()  # step the OneCycleLR each batch\n\n        # Validation\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    # Replace ReduceLROnPlateau with OneCycleLR\n    total_steps = len(train_loader) * MAX_EPOCHS\n    scheduler = OneCycleLR(optimizer, max_lr=LEARNING_RATE, total_steps=total_steps)\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for i, (batch_X, batch_y) in enumerate(train_loader):\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n            scheduler.step()\n\n        # Evaluate on validation set using EMA weights\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best shadow and do final inference\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # Replace ReduceLROnPlateau with OneCycleLR\n    total_steps = MAX_EPOCHS * len(train_loader)\n    scheduler = OneCycleLR(\n        optimizer,\n        max_lr=LEARNING_RATE,\n        total_steps=total_steps,\n        pct_start=0.3,\n        anneal_strategy=\"cos\",\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n            scheduler.step()\n\n        # Validation\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best shadow\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    # Use OneCycleLR in place of ReduceLROnPlateau\n    total_steps = MAX_EPOCHS * len(train_loader)\n    scheduler = OneCycleLR(\n        optimizer,\n        max_lr=LEARNING_RATE,\n        total_steps=total_steps,\n        pct_start=0.3,\n        anneal_strategy=\"cos\",\n        div_factor=25.0,\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n    global_step = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n            scheduler.step()\n            global_step += 1\n\n        # Validation\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Final test predictions\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # OneCycleLR replaces ReduceLROnPlateau\n    total_steps = len(train_loader) * MAX_EPOCHS\n    scheduler = OneCycleLR(\n        optimizer,\n        max_lr=LEARNING_RATE,\n        total_steps=total_steps,\n        pct_start=0.3,\n        anneal_strategy=\"cos\",\n        div_factor=25.0,\n        final_div_factor=1e4,\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n            scheduler.step()\n\n        # Evaluate on validation\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final val predictions\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # Replace ReduceLROnPlateau with OneCycleLR\n    total_steps = MAX_EPOCHS * len(train_loader)\n    scheduler = OneCycleLR(\n        optimizer, max_lr=LEARNING_RATE, total_steps=total_steps, pct_start=0.3\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n            scheduler.step()  # Step once per batch\n\n        # Evaluate on validation after each epoch\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best weights and finalize predictions\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # Replace ReduceLROnPlateau with OneCycleLR\n    total_steps = MAX_EPOCHS * len(train_loader)\n    scheduler = OneCycleLR(\n        optimizer,\n        max_lr=LEARNING_RATE,\n        total_steps=total_steps,\n        pct_start=0.3,\n        anneal_strategy=\"cos\",\n        div_factor=25.0,\n        final_div_factor=1e4,\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n            scheduler.step()\n\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # Replace ReduceLROnPlateau with OneCycleLR\n    total_steps = len(train_loader) * MAX_EPOCHS\n    scheduler = OneCycleLR(optimizer, max_lr=LEARNING_RATE, total_steps=total_steps)\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    global_step = 0\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n            scheduler.step()\n            global_step += 1\n\n        # Validation\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best shadow and evaluate final predictions\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # Replace ReduceLROnPlateau with OneCycleLR\n    total_steps = len(train_loader) * MAX_EPOCHS\n    scheduler = OneCycleLR(\n        optimizer,\n        max_lr=LEARNING_RATE,\n        total_steps=total_steps,\n        pct_start=0.3,\n        anneal_strategy=\"cos\",\n        div_factor=25.0,\n        final_div_factor=1e4,\n    )\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n            scheduler.step()  # step OneCycleLR per batch\n\n        # Evaluate on validation set\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    # Load best weights\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    # Final validation predictions\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    # Test predictions\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # Replace ReduceLROnPlateau with OneCycleLR\n    total_steps = MAX_EPOCHS * len(train_loader)\n    scheduler = OneCycleLR(optimizer, max_lr=LEARNING_RATE, total_steps=total_steps)\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    step_count = 0\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n\n            scheduler.step()\n            step_count += 1\n\n        # Validate\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn.utils import clip_grad_norm_\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nMAX_EPOCHS = 50\nPATIENCE = 5\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEMA_DECAY = 0.999\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Ensemble with 10 seeds\nSEEDS = list(range(10))\n\n# -------------------------\n# Data Loading\n# -------------------------\ng_train = pd.read_csv(\"./input/g_training.csv\")\ns_train = pd.read_csv(\"./input/s_training.csv\")\ng_val = pd.read_csv(\"./input/g_validation.csv\")\ns_val = pd.read_csv(\"./input/s_validation.csv\")\ng_test = pd.read_csv(\"./input/test_g.csv\")\n\nX_train = g_train.values.astype(np.float32)\ny_train = s_train.values.astype(np.float32)\nX_val = g_val.values.astype(np.float32)\ny_val = s_val.values.astype(np.float32)\nX_test = g_test.values.astype(np.float32)\n\nscaler_x = StandardScaler().fit(X_train)\nX_train = scaler_x.transform(X_train)\nX_val = scaler_x.transform(X_val)\nX_test = scaler_x.transform(X_test)\n\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(X_train), torch.tensor(y_train)\n)\nval_data = torch.utils.data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# -------------------------\n# Swish Activation\n# -------------------------\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# -------------------------\n# EMA Class\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        for name, param in self.model.named_parameters():\n            self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_avg = (1.0 - self.decay) * param.data + self.decay * self.shadow[\n                    name\n                ]\n                self.shadow[name] = new_avg.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\n    def store_shadow(self):\n        return {k: v.clone() for k, v in self.shadow.items()}\n\n    def load_shadow(self, shadow_dict):\n        for name in self.shadow:\n            self.shadow[name] = shadow_dict[name].clone()\n\n\n# -------------------------\n# Residual Block with SE\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, reduction=16):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.ln2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.act = Swish()\n\n        # Squeeze-and-Excitation sub-block\n        self.se_down = nn.Linear(1, dim // reduction)\n        self.se_up = nn.Linear(dim // reduction, dim)\n\n    def forward(self, x):\n        identity = x\n        out = self.ln1(x)\n        out = self.act(self.fc1(out))\n        out = self.ln2(out)\n        out = self.fc2(out)\n\n        # Squeeze-and-Excitation\n        se = out.mean(dim=1, keepdim=True)  # Nx1\n        se = self.se_down(se)  # Nx(dim//reduction)\n        se = self.act(se)\n        se = self.se_up(se)  # Nx(dim)\n        se = torch.sigmoid(se)\n        out = out * se  # Nx(dim)\n\n        return self.act(out + identity)\n\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, in_dim=14, hidden_dim=256, out_dim=2001, n_blocks=4):\n        super().__init__()\n        self.input_layer = nn.Linear(in_dim, hidden_dim)\n        self.act = Swish()\n        self.blocks = nn.ModuleList(\n            [ResidualBlock(hidden_dim) for _ in range(n_blocks)]\n        )\n        self.output_layer = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        for block in self.blocks:\n            x = block(x)\n        x = self.output_layer(x)\n        return x\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef train_and_get_predictions(seed):\n    set_seed(seed)\n\n    model = ResidualMLP().to(DEVICE)\n    ema = EMA(model, decay=EMA_DECAY)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    # OneCycleLR setup\n    total_steps = len(train_loader) * MAX_EPOCHS\n    scheduler = OneCycleLR(optimizer, max_lr=LEARNING_RATE, total_steps=total_steps)\n\n    best_val_mse = float(\"inf\")\n    best_shadow = ema.store_shadow()\n    epochs_without_improvement = 0\n\n    for epoch in range(MAX_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            optimizer.zero_grad()\n            preds = model(batch_X)\n            loss = criterion(preds, batch_y)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            ema.update()\n            scheduler.step()\n\n        # Validation\n        model.eval()\n        ema.apply_shadow()\n        val_preds_list = []\n        val_targets_list = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X = batch_X.to(DEVICE)\n                out = model(batch_X).cpu().numpy()\n                val_preds_list.append(out)\n                val_targets_list.append(batch_y.numpy())\n        ema.restore()\n\n        val_preds_cat = np.concatenate(val_preds_list, axis=0)\n        val_targets_cat = np.concatenate(val_targets_list, axis=0)\n        val_mse_ema = mean_squared_error(val_targets_cat, val_preds_cat)\n\n        if val_mse_ema < best_val_mse:\n            best_val_mse = val_mse_ema\n            best_shadow = ema.store_shadow()\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= PATIENCE:\n            break\n\n    ema.load_shadow(best_shadow)\n    ema.apply_shadow()\n\n    model.eval()\n    val_preds_list = []\n    with torch.no_grad():\n        for batch_X, _ in val_loader:\n            batch_X = batch_X.to(DEVICE)\n            out = model(batch_X).cpu().numpy()\n            val_preds_list.append(out)\n    val_preds_cat = np.concatenate(val_preds_list, axis=0)\n\n    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    with torch.no_grad():\n        test_preds = model(test_tensor).cpu().numpy()\n\n    ema.restore()\n    return val_preds_cat, test_preds, best_val_mse\n\n\nall_val_preds = []\nall_test_preds = []\nall_val_mses = []\n\nfor sd in SEEDS:\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n    all_val_preds.append(val_preds_seed)\n    all_test_preds.append(test_preds_seed)\n    all_val_mses.append(val_mse_seed)\n\nval_preds_ensemble = np.mean(np.stack(all_val_preds, axis=0), axis=0)\nval_targets_concat = s_val.values.astype(np.float32)\nensemble_val_mse = mean_squared_error(val_targets_concat, val_preds_ensemble)\nprint(f\"Ensemble Validation MSE: {ensemble_val_mse:.6f}\")\n\ntest_preds_ensemble = np.mean(np.stack(all_test_preds, axis=0), axis=0)\nsubmission = pd.DataFrame(test_preds_ensemble)\nsubmission.to_csv(\"./working/submission.csv\", index=False)"], "term_out": ["Epoch 1/5, Training MSE: 0.019851, Validation MSE: 0.010044\nEpoch 2/5, Training MSE: 0.008939, Validation MSE: 0.008177\nEpoch 3/5, Training MSE: 0.007690, Validation MSE: 0.007427\nEpoch 4/5, Training MSE: 0.007214, Validation MSE: 0.007161\nEpoch 5/5, Training MSE: 0.006987, Validation MSE: 0.007060\nValidation MSE: 0.007060\nTest MSE: 0.007056\nDone! MSE on validation set: 0.007059542309625457 MSE on test set: 0.007056042265867012\nExecution time: 41 seconds seconds (time limit is 10 hours).", "Validation MSE: 0.004324\nExecution time: a minute seconds (time limit is 10 hours).", "5-Fold CV MSE: 0.010148\nValidation MSE: 0.008197\nExecution time: 30 seconds seconds (time limit is 10 hours).", "Traceback (most recent call last):\n  File \"runfile.py\", line 80, in <module>\n    preds = model(X_batch)\n            ^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"runfile.py\", line 59, in forward\n    x = self.transformer(x)  # shape remains (14, batch, embed_dim)\n        ^^^^^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 517, in forward\n    output = mod(\n             ^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 920, in forward\n    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 934, in _sa_block\n    x = self.self_attn(\n        ^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/site-packages/torch/nn/modules/activation.py\", line 1373, in forward\n    attn_output, attn_output_weights = F.multi_head_attention_forward(\n                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/site-packages/torch/nn/functional.py\", line 6203, in multi_head_attention_forward\n    embed_dim == embed_dim_to_check\nAssertionError: was expecting embedding dimension of 64, but got 9\nExecution time: 20 seconds seconds (time limit is 10 hours).", "5-Fold CV MSE: 0.0103236325\nValidation MSE: 0.0085159745\nExecution time: a minute seconds (time limit is 10 hours).", "Validation MSE: 0.004464\nTest MSE: 0.004466\nExecution time: a minute seconds (time limit is 10 hours).", "Validation MSE: 0.006826294142842652\nTest MSE: 0.0067966105\nExecution time: 51 seconds seconds (time limit is 10 hours).", "Validation MSE: 0.004526\nExecution time: a minute seconds (time limit is 10 hours).", "Validation MSE: 0.004739\nExecution time: a minute seconds (time limit is 10 hours).", "Validation MSE: 0.004677\nExecution time: 59 seconds seconds (time limit is 10 hours).", "Validation MSE: 0.004895\nExecution time: 59 seconds seconds (time limit is 10 hours).", "Validation MSE: 0.003333\nExecution time: a minute seconds (time limit is 10 hours).", "Validation MSE: 0.002202\nExecution time: 4 minutes seconds (time limit is 10 hours).", "Validation MSE: 0.002232\nExecution time: 4 minutes seconds (time limit is 10 hours).", "Validation MSE: 0.002125\nExecution time: 4 minutes seconds (time limit is 10 hours).", "Validation MSE: 0.002057\nExecution time: 4 minutes seconds (time limit is 10 hours).", "Validation MSE: 0.002124\nExecution time: 4 minutes seconds (time limit is 10 hours).", "Validation MSE: 0.002172\nExecution time: 4 minutes seconds (time limit is 10 hours).", "Validation MSE: 0.002116\nExecution time: 4 minutes seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001793\nExecution time: 12 minutes seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001742\nExecution time: 19 minutes seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001637\nExecution time: 21 minutes seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001605\nExecution time: 27 minutes seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: 27 minutes seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001242\nExecution time: 54 minutes seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001670\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001715\nExecution time: 56 minutes seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001670\nExecution time: an hour seconds (time limit is 10 hours).", "runfile.py:53: UserWarning: This overload of add_ is deprecated:\n\tadd_(Number alpha, Tensor other)\nConsider using one of the following signatures instead:\n\tadd_(Tensor other, *, Number alpha = 1) (Triggered internally at /pytorch/torch/csrc/utils/python_arg_parser.cpp:1661.)\n  q.data.add_(self.alpha, p.data - q.data)\nEnsemble Validation MSE: 0.001367\nExecution time: 55 minutes seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001318\nExecution time: 54 minutes seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001363\nExecution time: 55 minutes seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001245\nExecution time: 55 minutes seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001241\nExecution time: 56 minutes seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001318\nExecution time: 57 minutes seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001364\nExecution time: 57 minutes seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001241\nExecution time: an hour seconds (time limit is 10 hours).", "Traceback (most recent call last):\n  File \"runfile.py\", line 286, in <module>\n    val_preds_seed, test_preds_seed, val_mse_seed = train_and_get_predictions(sd)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"runfile.py\", line 223, in train_and_get_predictions\n    optimizer.step()\n  File \"runfile.py\", line 105, in step\n    self.fast_weights[idx] = p.data.clone()\n    ~~~~~~~~~~~~~~~~~^^^^^\nIndexError: list assignment index out of range\nExecution time: 12 seconds seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001250\nExecution time: 58 minutes seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001318\nExecution time: 56 minutes seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001318\nExecution time: 57 minutes seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001318\nExecution time: 56 minutes seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001318\nExecution time: 56 minutes seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001378\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001874\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: 58 minutes seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001360\nExecution time: 56 minutes seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001244\nExecution time: 57 minutes seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001318\nExecution time: 57 minutes seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001676\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001318\nExecution time: 57 minutes seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001318\nExecution time: 56 minutes seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001212\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001265\nExecution time: 58 minutes seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001265\nExecution time: 58 minutes seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001265\nExecution time: 58 minutes seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001208\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.004446\nExecution time: 18 minutes seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001258\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours).", "Ensemble Validation MSE: 0.001259\nExecution time: an hour seconds (time limit is 10 hours)."], "analysis": ["The model trained successfully with a straightforward 3-layer MLP architecture. It achieved a validation MSE of 0.007060 and a test MSE of 0.007056, indicating stable performance. The training process was smooth and the final results are respectable for this complex regression task.", "The script successfully trained a residual MLP model using a three-block architecture with hidden dimension 64, achieving a final validation MSE of 0.004324. The final predictions were saved for the test set as expected.", "The 1D CNN architecture was successfully trained, yielding a 5-Fold CV MSE of approximately 0.010148 and a final validation MSE of 0.008197. The code ran without errors and shows a promising performance for this regression task.", "There is a shape mismatch in the Transformer. The code expects 14 input features to be mapped to an embedding dimension of 64, but it receives only 9 dimensions instead. To fix this, one must ensure that the g_training.csv file (and other geometry CSVs) indeed has 14 columns. Alternatively, adjust the model\u2019s input dimension to match the actual number of geometry features.", "The training completed successfully with no apparent bugs. The transformer-based regressor achieved a final validation MSE of 0.008516, demonstrating good performance on this task.", "The code ran successfully, producing a Residual MLP with dropout that achieved a Validation MSE of around 0.004464 and a Test MSE of around 0.004466. The residual connections and dropout appear to help with stable training and good generalization.", "The code ran successfully without errors, achieving a validation MSE of approximately 0.0068 and a test MSE of approximately 0.0068. The transformer-based architecture appears to perform well given the short training time, indicating that the approach may be effective for this dataset.", "The code executed successfully and achieved a validation MSE of 0.004526, indicating effective performance. The final model is a Residual MLP with batch normalization and ReLU activation, trained for 10 epochs. The model\u2019s predictions were saved to submission.csv for the test set.", "The code ran successfully, producing a final validation MSE of 0.004739. This Residual MLP architecture appears effective for the given regression task, yielding low errors on the validation set.", "The code ran successfully using a residual MLP architecture with three residual blocks and achieved a final validation MSE of approximately 0.004677. All steps, including data loading, scaling, training, and evaluation, appear correct, and no bugs were found.", "The code executed successfully with no apparent errors or failures. The final model achieved a validation MSE of approximately 0.004895, indicating that the Residual MLP architecture with ELU activation performed well for this regression task.", "The model successfully trained a 4-block residual MLP with a validation MSE of 0.003333, indicating a good fit. The final predictions for the test set were output to submission.csv.", "The training script completed successfully, achieving a final validation MSE of 0.002202. The Residual MLP architecture, coupled with early stopping, appears to have performed well with no errors encountered in the code execution.", "The residual MLP training ran successfully and converged to a final validation MSE of 0.002232. This indicates the model learned a strong mapping from geometry to spectrum, achieving good predictive accuracy on the validation dataset.", "The residual MLP model trained successfully without errors. The final validation MSE was 0.002125, indicating strong predictive performance on this dataset.", "The training completed successfully with a final validation MSE of 0.002057, which indicates a good fit for this deep residual MLP model. The code includes a learning rate scheduler and early stopping mechanism, suggesting a carefully tuned solution for the regression task.", "The training completed successfully using a residual MLP with 128-unit hidden layers, achieving a validation MSE of 0.002124. The architecture search with a small number of residual blocks and tuned hyperparameters yielded strong performance and relatively fast convergence.", "The training completed successfully, achieving a validation MSE of approximately 0.002172. A residual MLP with four blocks was used, and the model converged without any apparent errors.", "The model trained successfully, achieving a final validation MSE of 0.002116. A Residual MLP architecture with four blocks was used, along with an AdamW optimizer and OneCycleLR scheduler. Early stopping halted training after no improvement in the validation metric.", "The code ran successfully and produced a final ensemble model with a validation MSE of 0.001793. Training used a residual MLP architecture with ensembles over three random seeds. This approach effectively minimized the validation error while remaining stable across runs.", "The code successfully trained a residual MLP ensemble for multivariate regression and converged to a final validation MSE of about 0.001742. The residual blocks and ensemble training approach appear to work well for this task. No bugs were observed in the training process.", "The residual MLP ensemble trained successfully and achieved a final validation MSE of 0.001637. This indicates that the chosen architecture and hyperparameters effectively captured the geometry-to-spectrum mapping. No bugs were detected in the execution.", "The code ran successfully and achieved an ensemble validation MSE of 0.001605. The residual MLP architecture with Swish activations, EMA, and an ensemble of multiple seeds appears to have effectively minimized the MSE for this 2001D regression task.", "The training proceeded successfully without errors. A ResidualMLP architecture with Swish activation was used and trained with five different seeds to form an ensemble. The final ensemble validation MSE was 0.001259, indicating a good fit for this regression task.", "The code ran successfully and produced an ensemble validation MSE of 0.001242. The residual MLP architecture with Swish activation and EMA provided strong performance for this multivariate regression task.", "The code ran successfully, yielding an ensemble validation MSE of 0.001670. The final architecture was a residual MLP with Swish activation and EMA for regularization, and an ensemble of 10 seeds was used to reduce variance.", "The code ran successfully and achieved an ensemble validation MSE of 0.001715. It trains a residual MLP with Swish activation, dropout, and EMA, and then ensembles multiple seeds to further reduce the validation MSE.", "Model trained successfully with an ensemble validation MSE of 0.001670. No implementation bugs observed.", "Training completed successfully using a residual MLP ensemble with an EMA and a Lookahead optimizer. The final ensemble validation MSE was 0.001367, indicating strong predictive performance.", "The training completed successfully with a residual MLP ensemble achieving a final validation MSE of 0.001318. The approach involved an EMA for stabilized training, and residual blocks with Swish activation proved effective for this multivariate regression task.", "The code executed successfully, producing a final ensemble validation MSE of 0.001363, which is quite low for a high-dimensional regression task. The residual MLP ensemble with EMA and Lookahead optimizers seems to work well, indicating that this approach generalizes effectively across multiple random seeds.", "The deep learning training code successfully ran without errors. The final residual MLP ensemble achieved a validation MSE of 0.001245, demonstrating strong performance on this regression task.", "Training was successful using a residual MLP ensemble with Swish activations. The final ensemble achieved an MSE of 0.001241 on the validation set, showing strong performance on the multivariate regression task.", "The code ran successfully and produced a final ensemble validation MSE of 0.001318. The residual MLP architecture with Swish activation and EMA was used, and an ensemble of models with different seeds was averaged to achieve this result.", "The code runs successfully, and the ensemble achieved a validation MSE of about 0.001364. The training strategy (with residual layers, EMA, and the Lookahead optimizer) appears to have worked effectively, as indicated by the low MSE. The final model likely provides reasonably accurate predictions.", "The code successfully trained and evaluated a residual MLP ensemble, achieving an ensemble validation MSE of 0.001241 on the validation set. The final architecture performed well, indicating that residual blocks and Swish activation likely helped model performance.", "The code encounters an index out-of-range error in the Lookahead optimizer\u2019s step method because it stores weights by param group but attempts to write them in a flattened manner. To fix this, adjust the step logic to iterate over each param group and correctly update self.fast_weights[group_idx][param_idx]. Example:\n\nfor group_idx, group in enumerate(self.base_optimizer.param_groups):\n    for param_idx, p in enumerate(group[\"params\"]):\n        self.fast_weights[group_idx][param_idx] = p.data.clone()\n\nThis ensures that each group\u2019s parameters are handled consistently with how they were initialized.", "The code ran without errors and achieved a final ensemble validation MSE of 0.00125. It uses a residual MLP architecture with Swish activations, Lookahead, and EMA, forming an ensemble across 10 different seeds. The result is stable and demonstrates solid performance on this regression task.", "The Residual MLP with Swish activation successfully trained and converged on the data. An ensemble of 10 runs achieved a validation MSE of 0.001318, indicating strong predictive performance for the 2001D spectrum regression task.", "The model ran successfully and reached a validation MSE of 0.001318 using a ResidualMLP architecture with Swish activation. An ensemble of 10 seeds was used, improving stability and performance. No bugs were observed during execution.", "The training script ran successfully. An ensemble of residual MLPs featuring Swish activations achieved a final validation MSE of 0.001318, indicating good performance on predicting the high-dimensional spectrum from the given geometry parameters.", "All training steps executed correctly. The residual MLP with Swish activation was trained using an ensemble of ten seeds, achieving a final validation MSE of 0.001318. This setup indicates successful optimization with a low error rate, suggesting strong predictive performance.", "The deep learning regression model ran successfully, producing an ensemble validation MSE of 0.001378. The final architecture uses a residual MLP with squeeze-and-excitation blocks and Swish activation, and the ensemble of 10 seeds performed well.", "Training completed successfully with a final ensemble validation MSE of 0.001874. The model used a residual MLP architecture with Swish activations, batch normalization, and an EMA training scheme. The ensemble of 10 seeds helped reduce overfitting and improve generalization.", "The script successfully trained a residual MLP model using 10-seed ensembling, yielding a best ensemble MSE of 0.001259 on the validation set. The training process appears stable, and the final architecture (Residual MLP with Mish activation) performed well for this regression task.", "The code ran successfully without bugs. The final residual MLP ensemble achieved an impressive validation MSE of 0.001360, indicating accurate predictions. Applied techniques such as EMA, early stopping, gradient clipping, and learning rate scheduling likely contributed to its stable and effective training.", "The code executed successfully using a residual MLP with Swish activation, ensemble training, and EMA/Lookahead optimizers. The model achieved a validation MSE of 0.001244, indicating effective performance for this high-dimensional regression.", "The deep learning residual MLP ensemble successfully trained on the data, achieving a final ensemble validation MSE of 0.001318. There were no apparent errors during execution, and the provided architecture and hyperparameters yielded stable and robust performance.", "The code ran successfully without errors. An ensemble approach using a residual MLP architecture with EMA led to a final validation MSE of 0.001676, indicating strong predictive performance with lower being better.", "The script ran successfully, producing a final ensemble validation MSE of 0.001318. The deep residual MLP with Swish activation and EMA performed well, and no issues or errors were identified.", "The code successfully trained a Residual MLP with Swish activation functions, employing an ensemble of 10 seeds for prediction. The final ensemble achieved a validation MSE of 0.001318, demonstrating good performance in minimizing prediction error.", "Training completed successfully with a final ensemble validation MSE of 0.001212. The Residual MLP architecture with EMA and an ensemble across 10 seeds yielded strong performance. Adjusting hidden dimensions and residual blocks, along with using the Swish activation and AdamW optimization, appear to have contributed to achieving a low MSE.", "The training completed successfully and produced a final ensemble validation MSE of 0.001265. The residual MLP ensemble approach with Swish activation and EMA appears to be effective. Overall, the implementation ran smoothly without errors.", "The Residual MLP ensemble trained successfully with no apparent bugs or errors. The final ensemble achieved an MSE of 0.001265 on the validation set, suggesting good predictive performance on this regression task.", "The training script ran successfully with a final Residual MLP ensemble model. The validation MSE reached 0.001265, indicating strong predictive performance on the validation set. No issues or exceptions were observed, and the final predictions for the test set were saved.", "The code execution was successful. The residual MLP architecture, combined with EMA and ensembling over 10 seeds, reached a validation MSE of 0.001208, indicating very good performance on this task.", "The model trained successfully using a residual MLP with ensemble over 10 seeds. Final ensemble validation MSE was 0.004446, which shows good performance on this task.", "The code ran successfully without errors. A residual MLP with 4 blocks and squeeze-and-excitation was trained and ensembled across 10 seeds. The final ensemble achieved a Validation MSE of 0.001259.", "The training and evaluation ran successfully, producing an ensemble validation MSE of 0.001259. The residual MLP with squeeze-and-excitation blocks combined with EMA, OneCycleLR, and ensembling over 10 seeds appears to have achieved stable and low error.", "The code executed successfully, producing an ensemble validation MSE of about 0.001259. This indicates a well-trained deep model with strong regression performance for the 14D-to-2001D prediction task.", "The code successfully executes an ensemble of Residual MLP models with squeeze-and-excitation blocks. The final reported validation MSE is 0.001259, suggesting strong performance on the task of predicting the 2001D spectrum from 14D geometry inputs.", "The script ran successfully and produced a 4-block Residual MLP ensemble with SE blocks. The final model achieved an ensemble validation MSE of 0.001259, indicating a strong performance in predicting the electromagnetic spectrum from the geometry inputs.", "The model successfully trained a residual MLP ensemble and achieved a final validation MSE of 0.001259. No errors were encountered, and the experimental results indicate strong performance on this regression task.", "The code appears to run successfully with no errors. A residual MLP ensemble was trained, achieving a final validation MSE of 0.001259. This indicates the model is performing effectively for this regression task.", "The code ran successfully with a final ensemble validation MSE of 0.001259. This indicates strong predictive performance for the Residual MLP architecture. The training procedure completed without any issues.", "The code ran successfully and produced an ensemble MSE of 0.001259 on validation. The residual MLP architecture with squeeze-and-excitation and EMA demonstrated good performance. Hyperparameters worked well, and the final ensemble gave strong results with minimal overfitting.", "Training completed successfully, yielding an ensemble validation MSE of approximately 0.001259. The residual MLP architecture, combined with an ensemble of multiple seeds, appears to perform well on this regression task.", "This training script successfully completed without errors. An ensemble of 10 Residual MLP models with Squeeze-and-Excitation blocks achieved a validation MSE of 0.001259. The final architecture and hyperparameters appear to be working well, and the test predictions were saved to submission.csv.", "The code ran successfully using an ensemble of residual MLP models with squeeze-and-excitation blocks. The final ensemble achieved a validation MSE of 0.001259, demonstrating that the model largely succeeds at predicting the spectrum. The results suggest the chosen architecture and hyperparameter settings are effective for this task.", "The residual MLP ensemble trained successfully and achieved a final validation MSE of 0.001259. By averaging the predictions from multiple seeds, the final model appears to perform well on this regression task involving predicting a 2001D spectrum from 14D geometry parameters.", "The run completed successfully, producing an ensemble validation MSE of 0.001259. The final model is a Residual MLP with Squeeze-and-Excitation blocks, demonstrating effective performance in predicting the 2001D output from the given 14D geometry inputs.", "No bugs were encountered. The ensemble of 10 models using a Residual MLP architecture successfully trained, achieving a reported validation MSE of 0.001259.", "The code successfully trains a residual MLP architecture with squeeze-and-excitation blocks, achieving strong performance. The final ensemble yielded a validation MSE of 0.001259, indicating good predictive accuracy with no observed issues or bugs.", "The code successfully trained an ensemble of residual MLP models with squeeze-and-excitation blocks, achieving a final ensemble validation MSE of 0.001259. The approach involved architecture tuning, hyperparameter optimization (OneCycleLR, early stopping), and creating an ensemble across multiple seeds for better performance.", "The code successfully trained an ensemble of Residual MLP models with a Squeeze-and-Excitation mechanism, converging to an ensemble validation MSE of 0.001259. This indicates strong predictive performance on the spectrum regression task.", "The code ran successfully and produced a final ensemble validation MSE of 0.001259. A residual MLP architecture with EMA was used, indicating stable performance with no reported errors.", "The script ran successfully, producing a final ensemble validation MSE of 0.001259. The approach used a residual MLP with SE blocks and an ensemble over 10 seeds, achieving good performance on the validation set with early stopping and OneCycleLR scheduling.", "The training script successfully concluded with an ensemble validation MSE of 0.001259. The Residual MLP architecture with Swish activation and Squeeze-and-Excitation blocks, combined with an ensemble of multiple seeds, provided robust performance for this regression task.", "The script ran successfully, building an MLP with residual SE blocks and an EMA for parameter averaging. The final ensemble on the validation set achieved an MSE of 0.001258, indicating a strong predictive performance.", "Training finished successfully, producing a final ensemble MSE of 0.001259. The ensemble-based Residual MLP approach with EMA and hyperparameter tuning appears to be effective for this 14D-to-2001D regression task.", "The code executed successfully, producing an ensemble validation MSE of approximately 0.001259. A residual MLP with squeeze-and-excitation blocks was used, providing strong performance in this multivariate regression task.", "This training script ran successfully. The final ensemble achieved a validation MSE of 0.001259, indicating strong predictive performance on the validation set. No significant issues were found in the implementation.", "The code ran successfully with no apparent bugs. A residual MLP architecture with squeeze-and-excitation blocks and an EMA strategy was used, yielding an ensemble validation MSE of 0.001259. This suggests that the chosen architecture and hyperparameters achieved good predictive accuracy on the 2001D regression task.", "A Residual MLP with Squeeze-and-Excitation sub-blocks and ensemble training achieved a final validation MSE of 0.001259, indicating strong predictive performance. The architecture search and one-cycle learning rate scheduler provided effective optimization, as demonstrated by the low validation error.", "The code ran successfully on the ensemble of 10 seeds, yielding a final validation MSE of 0.001259. The final model is a residual MLP with squeeze-and-excitation blocks, trained with OneCycleLR scheduling. Hyperparameter adjustments and seed ensembling further stabilized the performance.", "The training script ran successfully, producing a final ensemble MSE on the validation set of 0.001259. An ensemble of Residual MLPs with EMA and OneCycleLR was used, and the results suggest that the chosen architecture and hyperparameters yielded good performance.", "The code ran successfully, producing a final ensemble validation MSE of 0.001259. It used a deep residual MLP with Squeeze-and-Excitation blocks and OneCycleLR scheduling, and all appears to function as intended.", "The code executed successfully without errors. A 4-block residual MLP with ensemble averaging across 10 seeds achieved a final validation MSE of 0.001259, indicating strong predictive performance for the regression task.", "The training completed successfully without errors. The final ensemble validation MSE was 0.001259, indicating strong performance for this regression task. The residual MLP architecture with squeeze-and-excitation blocks, combined with ensemble averaging, appears effective in this setup."], "exp_name": "2-fierce-griffin-of-action", "metrics": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}

let lastClick = 0;
let firstFrameTime = undefined;

let nodes = [];
let edges = [];

let lastScrollPos = 0;

setup = () => {
  canvas = createCanvas(...updateTargetDims());
};

class Node {
  x;
  y;
  size;
  xT;
  yT;
  xB;
  yB;
  treeInd;
  color;
  relSize;
  animationStart = Number.MAX_VALUE;
  animationProgress = 0;
  isStatic = false;
  hasChildren = false;
  isRootNode = true;
  isStarred = false;
  selected = false;
  renderSize = 10;
  edges = [];
  bgCol;

  constructor(x, y, relSize, treeInd) {
    const minSize = 35;
    const maxSize = 60;

    const maxColor = 10;
    const minColor = 125;

    this.relSize = relSize;
    this.treeInd = treeInd;
    this.size = minSize + (maxSize - minSize) * relSize;
    this.color = minColor + (maxColor - minColor) * relSize;
    this.bgCol = Math.round(Math.max(this.color / 2, 0));

    this.x = x;
    this.y = y;
    this.xT = x;
    this.yT = y - this.size / 2;
    this.xB = x;
    this.yB = y + this.size / 2;

    nodes.push(this);
  }

  startAnimation = (offset = 0) => {
    if (this.animationStart == Number.MAX_VALUE)
      this.animationStart = globalTime + offset;
  };

  child = (node) => {
    let edge = new Edge(this, node);
    this.edges.push(edge);
    edges.push(edge);
    this.hasChildren = true;
    node.isRootNode = false;
    return node;
  };

  render = () => {
    if (globalTime - this.animationStart < 0) return;

    const mouseXlocalCoords = (mouseX - width / 2) / scaleFactor;
    const mouseYlocalCoords = (mouseY - height / 2) / scaleFactor;
    const isMouseOver =
      dist(mouseXlocalCoords, mouseYlocalCoords, this.x, this.y) <
      this.renderSize / 1.5;
    if (isMouseOver) cursor(HAND);
    if (isMouseOver && mouseIsPressed) {
      nodes.forEach((n) => (n.selected = false));
      this.selected = true;
      setCodeAndPlan(
        treeStructData.code[this.treeInd],
        treeStructData.plan[this.treeInd],
      );
      manualSelection = true;
    }

    this.renderSize = this.size;
    if (!this.isStatic) {
      this.animationProgress = animEase(
        (globalTime - this.animationStart) / 1000,
      );
      if (this.animationProgress >= 1) {
        this.isStatic = true;
      } else {
        this.renderSize =
          this.size *
          (0.8 +
            0.2 *
              (-3.33 * this.animationProgress ** 2 +
                4.33 * this.animationProgress));
      }
    }

    fill(this.color);
    if (this.selected) {
      fill(accentCol);
    }

    noStroke();
    square(
      this.x - this.renderSize / 2,
      this.y - this.renderSize / 2,
      this.renderSize,
      10,
    );

    noStroke();
    textAlign(CENTER, CENTER);
    textSize(this.renderSize / 2);
    fill(255);
    // fill(lerpColor(color(accentCol), color(255), this.animationProgress))
    text("{ }", this.x, this.y - 1);
    // DEBUG PRINT:
    // text(round(this.relSize, 2), this.x, this.y - 1)
    // text(this.treeInd, this.x, this.y + 15)

    const dotAnimThreshold = 0.85;
    if (this.isStarred && this.animationProgress >= dotAnimThreshold) {
      let dotAnimProgress =
        (this.animationProgress - dotAnimThreshold) / (1 - dotAnimThreshold);
      textSize(
        ((-3.33 * dotAnimProgress ** 2 + 4.33 * dotAnimProgress) *
          this.renderSize) /
          2,
      );
      if (this.selected) {
        fill(0);
        stroke(0);
      } else {
        fill(accentCol);
        stroke(accentCol);
      }
      strokeWeight((-(dotAnimProgress ** 2) + dotAnimProgress) * 2);
      text("*", this.x + 20, this.y - 11);
      noStroke();
    }

    if (!this.isStatic) {
      fill(bgCol);
      const progressAnimBaseSize = this.renderSize + 5;
      rect(
        this.x - progressAnimBaseSize / 2,
        this.y -
          progressAnimBaseSize / 2 +
          progressAnimBaseSize * this.animationProgress,
        progressAnimBaseSize,
        progressAnimBaseSize * (1 - this.animationProgress),
      );
    }
    if (this.animationProgress >= 0.9) {
      this.edges
        .sort((a, b) => a.color() - b.color())
        .forEach((e, i) => {
          e.startAnimation((i / this.edges.length) ** 2 * 1000);
        });
    }
  };
}

class Edge {
  nodeT;
  nodeB;
  animX = 0;
  animY = 0;
  animationStart = Number.MAX_VALUE;
  animationProgress = 0;
  isStatic = false;
  weight = 0;

  constructor(nodeT, nodeB) {
    this.nodeT = nodeT;
    this.nodeB = nodeB;
    this.weight = 2 + nodeB.relSize * 1;
  }

  color = () => this.nodeB.color;

  startAnimation = (offset = 0) => {
    if (this.animationStart == Number.MAX_VALUE)
      this.animationStart = globalTime + offset;
  };

  render = () => {
    if (globalTime - this.animationStart < 0) return;

    if (!this.isStatic) {
      this.animationProgress = animEase(
        (globalTime - this.animationStart) / 1000,
      );
      if (this.animationProgress >= 1) {
        this.isStatic = true;
        this.animX = this.nodeB.xT;
        this.animY = this.nodeB.yT;
      } else {
        this.animX = bezierPoint(
          this.nodeT.xB,
          this.nodeT.xB,
          this.nodeB.xT,
          this.nodeB.xT,
          this.animationProgress,
        );

        this.animY = bezierPoint(
          this.nodeT.yB,
          (this.nodeT.yB + this.nodeB.yT) / 2,
          (this.nodeT.yB + this.nodeB.yT) / 2,
          this.nodeB.yT,
          this.animationProgress,
        );
      }
    }
    if (this.animationProgress >= 0.97) {
      this.nodeB.startAnimation();
    }

    strokeWeight(this.weight);
    noFill();
    stroke(
      lerpColor(color(bgCol), color(accentCol), this.nodeB.relSize * 1 + 0.7),
    );
    bezier(
      this.nodeT.xB,
      this.nodeT.yB,
      this.nodeT.xB,
      (this.nodeT.yB + this.nodeB.yT) / 2,
      this.animX,
      (this.nodeT.yB + this.nodeB.yT) / 2,
      this.animX,
      this.animY,
    );
  };
}

draw = () => {
  cursor(ARROW);
  frameRate(120);
  if (!firstFrameTime && frameCount <= 1) {
    firstFrameTime = millis();
  }
  // ---- update global animation state ----
  const initialSpeedScalingEaseIO =
    (cos(min((millis() - firstFrameTime) / 8000, 1.0) * PI) + 1) / 2;
  const initialSpeedScalingEase =
    (cos(min((millis() - firstFrameTime) / 8000, 1.0) ** (1 / 2) * PI) + 1) / 2;
  const initAnimationSpeedFactor = 1.0 - 0.4 * initialSpeedScalingEaseIO;
  // update global scaling-aware clock
  globalTime += globalAnimSpeed * initAnimationSpeedFactor * deltaTime;

  if (nodes.length == 0) {
    const spacingHeight = height * 1.3;
    const spacingWidth = width * 1.3;
    treeStructData.layout.forEach((lay, index) => {
      new Node(
        spacingWidth * lay[0] - spacingWidth / 2,
        20 + spacingHeight * lay[1] - spacingHeight / 2,
        1 - treeStructData.metrics[index],
        index,
      );
    });
    treeStructData.edges.forEach((ind) => {
      nodes[ind[0]].child(nodes[ind[1]]);
    });
    nodes.forEach((n) => {
      if (n.isRootNode) n.startAnimation();
    });
    nodes[0].selected = true;
    setCodeAndPlan(
      treeStructData.code[0],
      treeStructData.plan[0],
    )
  }

  const staticNodes = nodes.filter(
    (n) => n.isStatic || n.animationProgress >= 0.7,
  );
  if (staticNodes.length > 0) {
    const largestNode = staticNodes.reduce((prev, current) =>
      prev.relSize > current.relSize ? prev : current,
    );
    if (!manualSelection) {
      if (!largestNode.selected) {
        setCodeAndPlan(
          treeStructData.code[largestNode.treeInd],
          treeStructData.plan[largestNode.treeInd],
        );
      }
      staticNodes.forEach((node) => {
        node.selected = node === largestNode;
      });
    }
  }
  background(bgCol);
  // global animation transforms
  translate(width / 2, height / 2);
  scale(scaleFactor);

  
  // ---- fg render ----
  edges.forEach((e) => e.render());
  nodes.forEach((n) => n.render());
  
};

    </script>
    <title>AIDE Run Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
        overflow: scroll;
      }
      body {
        background-color: #f2f0e7;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 40vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
    </style>
  </head>
  <body>
    <pre
      id="text-container"
    ><div id="plan"></div><hr><code id="code" class="language-python"></code></pre>
  </body>
</html>
