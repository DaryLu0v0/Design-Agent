<!doctype html>
<html lang="en"> 
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#F2F0E7";
const accentCol = "#fd4578";

hljs.initHighlightingOnLoad();

const updateTargetDims = () => {
  // width is max-width of `.contentContainer` - its padding
  // return [min(windowWidth, 900 - 80), 700]
  return [windowWidth * (1 / 2), windowHeight];
};

const setCodeAndPlan = (code, plan) => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    // codeElm.innerText = code;
    codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    // planElm.innerText = plan.trim();
    planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
  }
};

windowResized = () => {
  resizeCanvas(...updateTargetDims());
  awaitingPostResizeOps = true;
};

const animEase = (t) => 1 - (1 - Math.min(t, 1.0)) ** 5;

// ---- global constants ----

const globalAnimSpeed = 1.1;
const scaleFactor = 0.57;

// ---- global vars ----

let globalTime = 0;
let manualSelection = false;

let currentElemInd = 0;

let treeStructData = {"edges": [[1, 5], [1, 7], [1, 6], [1, 8], [1, 10], [1, 9], [10, 12], [10, 14], [10, 11], [10, 13], [14, 15], [15, 16], [16, 17], [17, 18], [17, 19], [19, 21], [19, 23], [19, 20], [19, 24], [19, 22], [24, 35], [24, 31], [24, 25], [24, 27], [24, 29], [24, 33], [24, 28], [24, 26], [24, 30], [31, 32], [33, 34], [35, 37], [35, 36], [37, 38], [37, 54], [37, 49], [37, 45], [37, 39], [37, 48], [37, 55], [37, 43], [37, 51], [37, 52], [37, 53], [37, 40], [37, 44], [37, 41], [37, 46], [37, 50], [37, 47], [37, 42], [55, 65], [55, 63], [55, 78], [55, 66], [55, 60], [55, 67], [55, 59], [55, 57], [55, 64], [55, 77], [55, 68], [55, 70], [55, 76], [55, 71], [55, 69], [55, 79], [55, 75], [55, 62], [55, 61], [55, 74], [55, 72], [55, 58], [55, 73], [55, 56], [79, 88], [79, 80], [79, 87], [79, 90], [79, 84], [79, 83], [79, 89], [79, 85], [79, 81], [79, 86], [79, 91], [79, 82], [91, 92], [92, 93], [93, 94], [94, 95], [95, 96], [96, 97], [97, 98], [98, 100], [98, 99], [100, 101], [100, 102], [102, 103], [103, 104], [104, 105], [105, 108], [105, 109], [105, 106], [105, 110], [105, 107], [110, 112], [110, 111], [110, 113], [113, 118], [113, 114], [113, 115], [113, 122], [113, 116], [113, 120], [113, 119], [113, 117], [113, 121], [122, 123], [122, 124], [124, 130], [124, 128], [124, 127], [124, 129], [124, 125], [124, 131], [124, 126], [131, 134], [131, 137], [131, 135], [131, 138], [131, 139], [131, 133], [131, 136], [131, 132]], "layout": [[0.027551020408163266, 0.0], [0.04591836734693878, 0.0], [0.0642857142857143, 0.0], [0.0826530612244898, 0.0], [0.10102040816326531, 0.0], [0.0, 0.033333333333333326], [0.018367346938775512, 0.033333333333333326], [0.036734693877551024, 0.033333333333333326], [0.05510204081632653, 0.033333333333333326], [0.07346938775510205, 0.033333333333333326], [0.09183673469387756, 0.033333333333333326], [0.0642857142857143, 0.06666666666666665], [0.0826530612244898, 0.06666666666666665], [0.10102040816326531, 0.06666666666666665], [0.11938775510204082, 0.06666666666666665], [0.11938775510204082, 0.09999999999999998], [0.11938775510204082, 0.1333333333333333], [0.11938775510204082, 0.16666666666666663], [0.11020408163265306, 0.19999999999999996], [0.1285714285714286, 0.19999999999999996], [0.09183673469387756, 0.23333333333333328], [0.11020408163265306, 0.23333333333333328], [0.1285714285714286, 0.23333333333333328], [0.1469387755102041, 0.23333333333333328], [0.1653061224489796, 0.23333333333333328], [0.09081632653061225, 0.2666666666666667], [0.10918367346938776, 0.2666666666666667], [0.12755102040816327, 0.2666666666666667], [0.14591836734693878, 0.2666666666666667], [0.16428571428571428, 0.2666666666666667], [0.1826530612244898, 0.2666666666666667], [0.2010204081632653, 0.2666666666666667], [0.2010204081632653, 0.30000000000000004], [0.21938775510204084, 0.2666666666666667], [0.21938775510204084, 0.30000000000000004], [0.2469387755102041, 0.2666666666666667], [0.23775510204081635, 0.30000000000000004], [0.25612244897959185, 0.30000000000000004], [0.1, 0.33333333333333337], [0.11836734693877551, 0.33333333333333337], [0.13673469387755102, 0.33333333333333337], [0.15510204081632653, 0.33333333333333337], [0.17346938775510204, 0.33333333333333337], [0.19183673469387755, 0.33333333333333337], [0.21020408163265308, 0.33333333333333337], [0.2285714285714286, 0.33333333333333337], [0.2469387755102041, 0.33333333333333337], [0.2653061224489796, 0.33333333333333337], [0.2836734693877551, 0.33333333333333337], [0.30204081632653057, 0.33333333333333337], [0.3204081632653061, 0.33333333333333337], [0.3387755102040816, 0.33333333333333337], [0.35714285714285715, 0.33333333333333337], [0.37551020408163266, 0.33333333333333337], [0.39387755102040817, 0.33333333333333337], [0.4122448979591837, 0.33333333333333337], [0.20102040816326527, 0.3666666666666667], [0.21938775510204078, 0.3666666666666667], [0.2377551020408163, 0.3666666666666667], [0.2561224489795918, 0.3666666666666667], [0.2744897959183673, 0.3666666666666667], [0.2928571428571428, 0.3666666666666667], [0.3112244897959183, 0.3666666666666667], [0.32959183673469383, 0.3666666666666667], [0.3479591836734694, 0.3666666666666667], [0.3663265306122449, 0.3666666666666667], [0.3846938775510204, 0.3666666666666667], [0.4030612244897959, 0.3666666666666667], [0.42142857142857143, 0.3666666666666667], [0.43979591836734694, 0.3666666666666667], [0.45816326530612245, 0.3666666666666667], [0.47653061224489796, 0.3666666666666667], [0.49489795918367346, 0.3666666666666667], [0.5132653061224489, 0.3666666666666667], [0.5316326530612244, 0.3666666666666667], [0.5499999999999999, 0.3666666666666667], [0.5683673469387756, 0.3666666666666667], [0.5867346938775511, 0.3666666666666667], [0.6051020408163266, 0.3666666666666667], [0.6234693877551021, 0.3666666666666667], [0.5224489795918367, 0.4], [0.5408163265306122, 0.4], [0.5591836734693877, 0.4], [0.5775510204081633, 0.4], [0.5959183673469388, 0.4], [0.6142857142857143, 0.4], [0.6326530612244898, 0.4], [0.6510204081632653, 0.4], [0.6693877551020408, 0.4], [0.6877551020408164, 0.4], [0.7061224489795919, 0.4], [0.7244897959183674, 0.4], [0.7244897959183674, 0.43333333333333335], [0.7244897959183674, 0.4666666666666667], [0.7244897959183674, 0.5], [0.7244897959183674, 0.5333333333333333], [0.7244897959183674, 0.5666666666666667], [0.7244897959183674, 0.6], [0.7244897959183674, 0.6333333333333333], [0.7153061224489796, 0.6666666666666667], [0.7336734693877551, 0.6666666666666667], [0.7244897959183674, 0.7], [0.7428571428571429, 0.7], [0.7428571428571429, 0.7333333333333334], [0.7428571428571429, 0.7666666666666666], [0.7428571428571429, 0.8], [0.7061224489795919, 0.8333333333333334], [0.7244897959183674, 0.8333333333333334], [0.7428571428571429, 0.8333333333333334], [0.7612244897959184, 0.8333333333333334], [0.7795918367346939, 0.8333333333333334], [0.7612244897959184, 0.8666666666666667], [0.7795918367346939, 0.8666666666666667], [0.7979591836734694, 0.8666666666666667], [0.7244897959183674, 0.9], [0.7428571428571429, 0.9], [0.7612244897959184, 0.9], [0.7795918367346939, 0.9], [0.7979591836734694, 0.9], [0.8163265306122449, 0.9], [0.8346938775510204, 0.9], [0.8530612244897959, 0.9], [0.8714285714285714, 0.9], [0.8622448979591837, 0.9333333333333333], [0.8806122448979592, 0.9333333333333333], [0.8255102040816327, 0.9666666666666667], [0.8438775510204082, 0.9666666666666667], [0.8622448979591837, 0.9666666666666667], [0.8806122448979592, 0.9666666666666667], [0.8989795918367347, 0.9666666666666667], [0.9173469387755102, 0.9666666666666667], [0.9357142857142857, 0.9666666666666667], [0.8714285714285714, 1.0], [0.889795918367347, 1.0], [0.9081632653061225, 1.0], [0.926530612244898, 1.0], [0.9448979591836735, 1.0], [0.963265306122449, 1.0], [0.9816326530612245, 1.0], [1.0, 1.0]], "plan": ["We will use a simple fully-connected network with two hidden layers of 512 units\nand ReLU activations, standardizing the 14\u2011dim input features. The model is\ntrained on the provided g_training/s_training pairs for 50 epochs with MSE loss\nand Adam optimizer, and evaluated by computing the mean squared error on the\nvalidation set. After training, we generate predictions for test_g, save them in\n\u201cworking/submission.csv\u201d, and print the validation MSE. The evaluation metric is\nthe MSE on the hold\u2011out validation set.", "I propose a residual MLP with three hidden layers of sizes [256, 512, 256], each\nfollowed by batch normalization and ReLU, with a dropout layer in the middle to\nprevent overfitting. We standardize the 14\u2011dimensional geometry inputs, train\nthe network with the Adam optimizer (lr=1e-3) and MSE loss for 50 epochs, then\nevaluate on the provided validation set to compute the MSE. Finally, we generate\npredictions on the test set and save them to `submission.csv`.", "We standardize the 14-dimensional inputs and feed them into a small 1D\nconvolutional network with two Conv1D layers (channels [16,32], kernel size 3)\nfollowed by a fully-connected output layer predicting the 2001-dimensional\nspectrum. The model is trained with MSE loss and Adam optimizer for 50 epochs,\nand we select the best validation MSE. After training, we generate test\npredictions saved to submission.csv. We report the final validation MSE as the\nevaluation metric.", "I propose to standardize the 14-dimensional geometry inputs and train a 4\u2011layer\nfully-connected network with hidden sizes [256,128,64,32], each followed by ReLU\nactivations and dropout (p=0.2) to prevent overfitting. We will use the Adam\noptimizer (learning rate 1e-3) and mean squared error loss, training for 50\nepochs with batch size 64. After training, we compute the MSE on the provided\nvalidation set and then generate predictions on the test set. The validation MSE\nwill be printed, and test predictions saved to \u201csubmission.csv\u201d in the working\ndirectory.", "We standardize the geometry inputs using z\u2011score normalization and train a\n4\u2011layer fully connected network with hidden sizes [512,256,128,64], ReLU\nactivations, and 0.1 dropout to predict the 2001\u2011dimensional spectrum. We use\nthe Adam optimizer (learning rate 1e\u20113), batch size 64, and train for 40 epochs,\nmonitoring validation MSE on the provided hold\u2011out set. After training we print\nthe final validation MSE and generate predictions on the test set, saving them\nto `working/submission.csv`.", "I propose adding L2 regularization by setting `weight_decay=1e-4` in the Adam\noptimizer to penalize large weights and improve generalization. This change is\natomic and keeps the residual MLP architecture and training regime unchanged. We\nwill train for 50 epochs, evaluate the final model on the validation set, and\nprint the validation MSE. Finally, we will save test predictions to\n`submission.csv`.", "I propose to integrate a ReduceLROnPlateau learning rate scheduler that monitors\nthe validation MSE and reduces the learning rate by a factor of 0.5 after 5\nepochs without improvement, which should help the model refine its weights and\nconverge to a lower error. During training we will evaluate the model on the\nvalidation set at the end of each epoch, step the scheduler based on validation\nMSE, and keep track of the best model weights. After training we will load the\nbest weights before computing the final validation MSE and generating test\npredictions. This atomic change should improve generalization and reduce the\nfinal validation MSE.", "I suggest integrating a ReduceLROnPlateau learning rate scheduler that monitors\nthe validation MSE and reduces the learning rate by a factor of 0.5 after 5\nepochs without improvement. We will evaluate on the validation set at the end of\neach epoch, step the scheduler based on the validation loss, and keep track of\nthe best model weights. After training, we load the best weights, compute the\nfinal validation MSE, and generate test predictions saved to `submission.csv`.", "We add L2 weight decay to the Adam optimizer (weight_decay=1e-4) to penalize\nlarge weights and improve generalization, keeping all other training settings\nunchanged. The ResMLP architecture, data loading, and training loop remain the\nsame\u2014including 50 epochs, batch size 128, and lr=1e-3\u2014so we can directly compare\nthe impact of regularization. After training we compute the validation MSE and\nsave test predictions to `submission.csv`, expecting a reduction in validation\nloss.", "I propose to integrate a OneCycleLR learning rate scheduler to dynamically\nadjust the learning rate throughout training, which often accelerates\nconvergence and yields a better final generalization. We will configure\nOneCycleLR with the same max_lr as our base learning rate and step it on every\nbatch. All other training settings, including model architecture, optimizer, and\nepochs, remain unchanged for a direct comparison. This atomic change should\nreduce the final validation MSE by encouraging a more robust optimization\ntrajectory.", "I propose to add validation-based model checkpointing: after each epoch we\nevaluate the model on the hold-out validation set, track and save the weights\nthat yield the lowest validation MSE, and at the end of training load these best\nweights for final evaluation and test prediction. This ensures we use the most\ngeneralizable model rather than the last epoch, potentially reducing\noverfitting. All other settings (architecture, optimizer, epochs) remain\nunchanged.", "I propose to replace the ReLU activations in the residual MLP with SiLU (Swish)\nactivations, which often improve regression performance by providing smoother\ngradients and better feature learning. This atomic change preserves the network\narchitecture, training loop, and hyperparameters, allowing direct comparison of\nits effect. We will retrain the model for 50 epochs with the same Adam optimizer\nand checkpointing, then evaluate on the validation set and save test\npredictions.", "I propose ensembling five ResMLP models via 5\u2011fold cross\u2011validation on the\ncombined training and validation data. For each fold we fit a StandardScaler on\nthe fold\u2019s training split, train the ResMLP with validation\u2011based checkpointing\nfor 50 epochs, then predict on the fold\u2019s validation and on the test set. We\naverage the five test predictions for submission and compute the out\u2011of\u2011fold MSE\nacross all samples as our evaluation metric.", "I propose to integrate a CosineAnnealingLR scheduler that smoothly decays the\nlearning rate from its initial value to a small minimum over the course of\ntraining, which often improves convergence and generalization. This change is\natomic and preserves the existing Residual MLP architecture, optimizer, and\ncheckpointing logic. We will instantiate CosineAnnealingLR with T_max equal to\nthe total number of epochs and eta_min set to 1e-5, stepping the scheduler at\nthe end of each epoch. Finally, we retrain for 50 epochs, evaluate the best\ncheckpoint on the validation set, and save test predictions to `submission.csv`.", "I will replace all BatchNorm1d layers in the residual MLP with LayerNorm, which\nnormalizes features per sample rather than per batch and may provide more stable\ntraining on these geometry inputs. This change is atomic and keeps the network\narchitecture, optimizer settings, and training loop identical for direct\ncomparison. We train for 50 epochs using the Adam optimizer (lr=1e-3) with MSE\nloss and validation\u2010based checkpointing. Finally, we print the best validation\nMSE and save test predictions to `./working/submission.csv`.", "I propose ensembling five ResMLP models via 5\u2011fold cross\u2011validation on the\ncombined training and validation data. For each fold, we fit a StandardScaler on\nthe fold\u2019s training split, train the ResMLP with LayerNorm, dropout, and\nvalidation\u2011based checkpointing for 50 epochs, then predict on both the fold\u2019s\nvalidation split and the test set. We store out\u2011of\u2011fold predictions to compute\nthe overall CV MSE and average the five test predictions for submission. This\nensemble should lower variance and improve generalization.", "I propose switching the optimizer from Adam to AdamW with a small weight decay\n(e.g., 1e-4) to add L2 regularization, which should improve generalization. All\nother aspects\u2014including the ResMLP architecture with LayerNorm, 5\u2011fold CV,\ncheckpointing, and training for 50 epochs\u2014remain unchanged. We then retrain and\nevaluate via CV and save the averaged test predictions.", "I propose to standardize the 2001\u2011dimensional target spectra using a\nStandardScaler per fold, training the ResMLP on these normalized targets and\nthen inverse\u2010transforming predictions to the original scale before computing\nMSE. This ensures each output dimension has zero mean and unit variance during\ntraining, improving gradient stability and convergence. All other\nsettings\u2014including the ResMLP architecture, AdamW optimizer, weight decay, and\n5\u2011fold CV\u2014remain unchanged. We then compute and print the overall OOF MSE on the\noriginal scale and save the averaged test predictions.", "I propose integrating a OneCycleLR scheduler that warm\u2011starts the learning rate\nto a peak and then anneals it over the course of training, which can improve\nboth convergence speed and final generalization. We keep the ResMLP\narchitecture, 5\u2011fold cross\u2011validation, AdamW optimizer, and target\nstandardization unchanged, and simply add OneCycleLR with max_lr=1e-3 over all\nbatches and epochs. The scheduler steps each batch, ensuring a dynamic learning\nrate trajectory. We expect this dynamic schedule to reduce the overall OOF MSE.", "I propose to integrate Stochastic Weight Averaging (SWA) into the training loop\nby wrapping the ResMLP in an AveragedModel and starting SWA updates at epoch\u00a025,\nusing SWALR to anneal the learning rate during SWA. After training, we will\nupdate batch\u2010norm statistics on the averaged model and then use this SWA model\nfor final validation and test predictions. This single change should yield a\nmore robust final model and reduce generalization error without altering data\nprocessing or model architecture.", "I propose adding gradient norm clipping with a maximum norm of 1.0 before each\noptimizer step to stabilize training and prevent exploding gradients, which can\nimprove convergence and generalization. We retain the existing ResMLP\narchitecture, AdamW optimizer, SWA averaging and K\u2011fold CV, and simply insert\n`clip_grad_norm_` in the training loop. All other settings including learning\nrates, weight decay, and epochs remain unchanged to isolate this effect. This\natomic change should yield a lower OOF MSE.", "I propose replacing all ReLU activations in the ResMLP with the smoother Mish\nactivation function to improve gradient flow and generalization. This change is\natomic and keeps the existing architecture, optimizer, SWA training loop, and\ncross\u2010validation setup unchanged. We will retrain with Mish for 50 epochs per\nfold, compute the out\u2010of\u2010fold MSE, and save averaged test predictions to\n`submission.csv`.", "I propose integrating gradient norm clipping with a maximum norm of 1.0 into the\ntraining loop to stabilize gradient updates and prevent exploding gradients.\nSpecifically, after computing gradients via `loss.backward()`, we apply\n`clip_grad_norm_` before each optimizer step. This atomic change should improve\nconvergence stability and generalization when combined with SWA, without\naltering other hyperparameters or the model architecture.", "I propose adding gradient norm clipping with a maximum norm of 1.0 in the\ntraining loop to stabilize gradient updates and prevent exploding gradients.\nThis change is atomic and retains the existing ResMLP architecture, optimizer,\nSWA, and cross\u2010validation setup, allowing us to directly measure its effect on\ngeneralization. By clipping gradients before each optimizer step, we expect more\nstable convergence and a potential reduction in validation MSE.", "I propose augmenting the 14\u2010dimensional geometry inputs with second\u2010order\npolynomial features (squares and pairwise products) using sklearn\u2019s\nPolynomialFeatures. This will allow the network to learn explicit feature\ninteractions without changing the model architecture or training regime. We\ngenerate these polynomial features within each CV fold to avoid leakage, then\nstandardize and train the same ResMLP+SWA pipeline as before. Finally, we\ncompute fold and OOF MSE and save the averaged test predictions.", "I propose adding small Gaussian noise augmentation to the input features during\ntraining, which will act as regularization and help reduce overfitting.\nSpecifically, for each training batch, we add noise via x_noisy = x +\ntorch.randn_like(x) * 0.01 before passing it through the network. All other\ncomponents including polynomial feature expansion, target standardization,\nResMLP architecture, AdamW optimizer, SWA, and 5-fold cross-validation remain\nunchanged. This simple augmentation often improves generalization and should\nlower the validation MSE.", "I propose adding Gaussian noise augmentation to the input features during\ntraining: for each batch, we add noise `x_noisy = x + torch.randn_like(x) *\n0.01`. This regularizes the model and can reduce overfitting without changing\nthe existing architecture, CV, or SWA pipeline. We keep all other steps\n(polynomial expansion, scaling, SWA, etc.) unchanged to directly measure the\neffect of this augmentation. We expect a lower OOF and validation MSE due to\nimproved generalization.", "I propose to add Gaussian noise augmentation to the input features during\ntraining: before each forward pass, we replace the batch inputs\nwith\u00a0x_noisy\u00a0=\u00a0x\u00a0+\u00a0torch.randn_like(x)\u00a0*\u00a00.01. This regularizes the model and\ncan reduce overfitting without changing the existing architecture, CV, or SWA\npipeline. We keep all other steps (polynomial expansion, scaling, SWA, etc.)\nunchanged to directly measure the effect of this augmentation.", "I propose integrating mixup data augmentation into the training loop by blending\npairs of samples and their targets with a Beta(\u03b1,\u03b1) distribution (\u03b1=0.2). This\nwill regularize the model on polynomial\u2010expanded inputs and scaled targets,\nencouraging smoother mappings and reducing overfitting. All other aspects of the\npipeline\u2014including SWA, 5\u2010fold CV, target standardization, and the ResMLP\narchitecture\u2014remain unchanged, allowing us to directly assess the effect of\nmixup on OOF MSE.", "I propose adding a CosineAnnealingLR scheduler to the pre\u2010SWA phase: we decay\nthe learning rate from 1e\u20113 down to 1e\u20115 over the first 25 epochs, stepping the\nscheduler each epoch; after epoch 25 we begin SWA updates with SWALR as before.\nThis dynamic schedule often improves convergence and yields a lower validation\nMSE without changing the existing data pipeline or model architecture. The rest\nof the 5\u2011fold CV, polynomial feature expansion, target scaling, and SWA pipeline\nremain unchanged.", "I propose enhancing the feature expansion by increasing the polynomial degree\nfrom 2 to 3, enabling the model to capture cubic interactions among geometry\nparameters. This change preserves the existing ResMLP architecture, AdamW\noptimizer, SWA pipeline, and 5\u2011fold cross\u2011validation setup, allowing a direct\ncomparison of its effect on performance. All preprocessing, target scaling, and\ntraining loops remain unchanged except for the higher-degree polynomial\nfeatures.", "I propose increasing the polynomial feature expansion from degree\u00a02 to degree\u00a03\nto capture higher\u2011order interactions among geometry parameters. This change is\natomic and should enrich the feature representation without altering the model\narchitecture, optimizer, SWA pipeline, or CV setup. We will simply set\n`PolynomialFeatures(degree=3)` and retrain the ResMLP with AdamW, SWA, and\n5\u2011fold cross\u2011validation to evaluate the impact on OOF MSE. The rest of the\npipeline\u2014including scaling, model, training loop, and test prediction\nsaving\u2014remains unchanged.", "I will ensure all dataset tensors are created on the CPU and only moved to the\nGPU inside the training loop, with DataLoader using zero worker processes to\navoid CUDA initialization in worker processes. This change prevents the previous\nruntime error and allows proper training and evaluation. The script performs\n5\u2010fold cross\u2010validation, prints per\u2010fold and overall validation MSE, and saves\naveraged test predictions to `./working/submission.csv`.", "I propose to add weight normalization to all linear layers in the ResMLP\narchitecture, which often improves generalization by decoupling weight magnitude\nand direction. This change is atomic and preserves the existing data\npreprocessing, polynomial feature expansion, AdamW optimizer, SWA training, and\n5\u2011fold cross\u2011validation pipeline. We will wrap every `nn.Linear` in\n`torch.nn.utils.weight_norm` and then retrain for 50 epochs per fold, finally\nprinting the overall OOF MSE and saving averaged test predictions in\n`submission.csv`. This should yield a lower validation error without altering\nany other components.", "We can avoid the SWA deepcopy error by removing PyTorch\u2019s `weight_norm`\nwrappers, since they introduce non-leaf tensors that can\u2019t be deep\u2010copied. Here\nwe redefine our ResMLP without weight normalization, keeping LayerNorm and\ndropout, and apply SWA normally. The rest of the pipeline (polynomial features,\nscaling, 5\u2010fold CV, SWA training, OOF evaluation, and test prediction averaging)\nremains the same, and we print out the final OOF MSE and save `submission.csv`.", "I propose replacing the ReLU activations in the ResMLP with GELU activations to\nprovide smoother gradients and potentially improve regression performance. This\nchange is atomic and preserves the existing network architecture, preprocessing\n(polynomial features, scaling), SWA training, and 5\u2011fold cross\u2011validation setup,\nallowing direct evaluation of its impact. We will retrain for 50 epochs per\nfold, compute the overall out\u2011of\u2011fold MSE, and save averaged test predictions to\n`./working/submission.csv`.", "I propose to add an additional dropout layer (p=0.2) after the third hidden\nblock in the ResMLP to further regularize the network and reduce overfitting.\nThis atomic change preserves the existing polynomial feature expansion, SWA\ntraining, and 5\u2011fold CV setup, allowing us to directly measure its impact on\nvalidation and OOF MSE. All other hyperparameters and the training pipeline\nremain unchanged.", "I propose adding small Gaussian noise augmentation to the input features during\ntraining: for each training batch, we replace `xb` with `xb_noisy = xb +\ntorch.randn_like(xb) * 0.01` before forwarding through the model. This atomic\nchange regularizes the model without altering the architecture, data\npreprocessing, or SWA pipeline, and should reduce overfitting and lower the\nvalidation MSE. All other parts of the training routine remain unchanged.", "I propose adding Dropout(p=0.1) after the first and third hidden blocks in the\nResMLP to further regularize the network. We keep polynomial expansion (degree\n2), Gaussian input noise augmentation, AdamW+SWA training, and 5\u2011fold CV\nunchanged to isolate this change. This should reduce overfitting and improve the\noverall OOF MSE.", "I propose adding a CosineAnnealingLR scheduler over the first SWA warming\u2010up\nepochs to smoothly decay the learning rate from its initial value to a small\nminimum before starting SWA updates. This scheduler is stepped at the end of\neach pre\u2010SWA epoch, then we switch to the existing SWALR for the remainder of\ntraining. This atomic change should refine the optimization trajectory and\nfurther reduce the validation and OOF MSE.", "I propose to apply test-time augmentation (TTA) by averaging model predictions\nover multiple noisy copies of each sample during validation and test inference.\nSpecifically, after SWA and BN update, we will generate e.g. 5 Gaussian\u2011noisy\nversions of each input batch, average the predictions in the original scale, and\nuse these smoothed outputs to compute validation MSE and test predictions. This\nTTA should reduce prediction variance and yield a lower MSE without altering\ntraining.", "I propose to integrate test-time augmentation (TTA) by averaging predictions\nover multiple small Gaussian\u2010noisy copies of each validation and test input.\nAfter updating batch\u2010norm stats on the SWA model, we generate, say, five noisy\nversions of each sample at inference, average their scaled predictions, then\ninverse\u2010transform and compute MSE. This smoothing should reduce predictive\nvariance and lower validation and OOF MSE without altering training. The rest of\nthe 5\u2011fold CV, polynomial expansion, scaling, ResMLP+SWA pipeline remains\nunchanged.", "I propose to integrate test\u2011time augmentation (TTA) by averaging model\npredictions over multiple small Gaussian\u2010noisy copies of each sample during\nvalidation and test inference. After SWA and BN update, we generate, e.g., five\nnoisy versions of each input batch, average their predictions in the original\nscale, and compute the MSE on these smoothed outputs. This should reduce\nprediction variance and yield a lower validation and OOF MSE without altering\nthe training pipeline.", "I propose adding test-time augmentation (TTA) by averaging predictions over\nmultiple small Gaussian-noisy copies of each input during validation and test\ninference. After SWA and BN update, we generate a fixed number of noisy versions\nof each batch (e.g., 5 copies), average their predictions in the original scale,\nand compute MSE on these smoothed outputs. This smoothing should reduce\nprediction variance and yield a lower final validation MSE without altering the\ntraining pipeline.", "I propose to integrate test-time augmentation (TTA) by averaging predictions\nover multiple small Gaussian\u2010noisy copies of each input during validation and\ntest inference. After updating batch\u2010norm statistics on the SWA model, we\ngenerate several noisy versions of each sample (e.g., 5 copies), average their\npredictions in the normalized target space, then inverse\u2010transform and compute\nMSE. This simple change often reduces prediction variance and lowers the final\nvalidation and OOF MSE without altering training.", "I propose to add test-time augmentation (TTA) by averaging model predictions\nover multiple small Gaussian\u2010noisy copies of each input during validation and\ntest inference. Specifically, after updating batch\u2010norm statistics on the\nSWA\u2010averaged model, we generate a fixed number of noisy versions of each sample\n(e.g., 5 copies), average their predictions in the standardized target space,\nand then inverse\u2010transform to compute the final outputs. This smoothing should\nreduce prediction variance and lower the validation MSE without altering the\ntraining pipeline.", "I propose to add a CosineAnnealingLR scheduler over the pre\u2011SWA phase to\nsmoothly anneal the learning rate from 1e\u20113 down to 1e\u20115 across the first 25\nepochs, then switch to the existing SWALR for the SWA updates. This atomic\nchange refines the optimization trajectory before averaging and should lower the\nvalidation MSE. All other components\u2014including polynomial feature expansion,\nSWA, and 5\u2011fold CV\u2014remain unchanged.", "I propose integrating test-time augmentation (TTA) by averaging model\npredictions over multiple small Gaussian\u2010noisy copies of each validation and\ntest input. After updating batch\u2010norm statistics on the SWA model, we generate\nfive noisy versions of each input batch, average their predictions in the\nstandardized target space, and then inverse\u2010transform to compute the final\noutputs. This should reduce prediction variance and yield a lower validation MSE\nwithout altering the training pipeline.", "I propose integrating test-time augmentation by averaging model predictions over\nmultiple small Gaussian\u2010noisy copies of each validation and test input. After\nupdating batch\u2010norm statistics on the SWA\u2010averaged model, we will generate\nseveral noisy versions (e.g., 5 copies) of each sample at inference, average\ntheir scaled predictions, then inverse\u2010transform to compute final outputs. This\nsmoothing reduces prediction variance and should lower the validation MSE\nwithout altering the training pipeline.", "I propose to apply PCA dimensionality reduction on the 2001\u2011dimensional target\nspectra in each fold: we fit a PCA with 100 components on the standardized\ntraining targets, train the ResMLP to predict these components, and then\ninverse\u2011transform back to the original spectral space before computing MSE. This\nreduces noise and output dimensionality, improving model regularization without\naltering the existing data pipeline, SWA, or model architecture. All other\npreprocessing and training steps remain unchanged.", "I propose replacing the LayerNorm layers with BatchNorm1d so that SWA\u2019s\n`update_bn` can properly refresh batch\u2010norm statistics after model averaging.\nAll other components\u2014including polynomial feature expansion, input noise\naugmentation, AdamW optimizer, SWA scheduling, and 5\u2011fold CV\u2014remain unchanged to\nisolate this effect. This should improve calibration of the final SWA model and\nslightly lower the validation and OOF MSE.", "I propose to enhance the current TTA strategy by applying Monte Carlo Dropout at\ninference: after SWA and BN updates, we set the model to train mode to activate\ndropout, add small Gaussian noise to inputs, perform multiple stochastic forward\npasses (e.g. 10), and average these predictions. This Monte Carlo Dropout +\nnoise augmentation at test time should reduce predictive variance and further\nlower the validation MSE. All other training and preprocessing steps remain\nunchanged.", "I propose to add a PCA dimensionality reduction step on the polynomial\u2010expanded\ninput features in each CV fold, reducing them from ~119 to 64 components. This\ndenoises and compresses redundant interactions, improving generalization while\nkeeping the ResMLP+SWA pipeline unchanged. We fit PCA on the training split,\ntransform validation and test inputs accordingly, then proceed with scaling,\ntraining, and evaluation as before.", "I propose to integrate test-time augmentation (TTA) by averaging predictions\nover multiple small Gaussian\u2010noisy copies of each input during validation and\ntest inference. After updating batch\u2010norm statistics on the SWA\u2010averaged model,\nwe generate several noisy versions of each validation and test sample (e.g., 5\ncopies), average their predictions in the normalized target space, and then\ninverse\u2010transform to obtain the final outputs. This smoothing reduces prediction\nvariance and should lower the validation and out-of-fold MSE without altering\nthe training pipeline. We then save the averaged test predictions to\n`submission.csv`.", "I suggest integrating test-time augmentation (TTA) by averaging model\npredictions over multiple small Gaussian-noisy copies of each input during\nvalidation and test inference. After updating BN statistics on the SWA-averaged\nmodel, we generate several noisy versions (e.g., 5 copies) of each batch,\naverage their predictions in the scaled target space, and then inverse-transform\nto compute the final spectrum outputs. This smoothing reduces prediction\nvariance and should lower both validation and overall OOF MSE without altering\nthe training pipeline.", "I propose replacing the explicit polynomial feature expansion with a small\nTabTransformer\u2010style encoder on the raw 14\u2010dimensional inputs. Each feature is\nembedded into a learnable vector, passed through two layers of self\u2010attention to\ncapture interactions, then flattened and fed into the existing residual MLP\nhead. We keep all other training components\u2014standard scaling, 5\u2010fold CV,\nAdamW+SWA, Gaussian input noise, and SWA BN update\u2014unchanged to directly measure\nthe benefit of this learned feature interaction module.", "We integrate test-time augmentation by averaging predictions over 5\nGaussian\u2010noisy copies of each input during validation and test inference. After\nupdating batch\u2010norm statistics on the SWA\u2010averaged model, we generate five noisy\nversions of each validation and test batch, average their predictions in the\nnormalized target space, and then inverse\u2010transform to compute final outputs.\nThis smoothing reduces prediction variance and should lower validation MSE\nwithout altering the training pipeline.", "I propose adding test-time augmentation (TTA) by averaging multiple noisy\npredictions for each sample during inference to reduce variance and improve\ngeneralization. Specifically, after SWA and batch\u2010norm update, we generate a\nfixed number (e.g.\u00a05) of Gaussian\u2010noisy copies of each validation and test input\n(std=0.01), average their predictions in the normalized target space, then\ninverse\u2010transform to compute final outputs. This atomic change does not require\nretraining or architectural modifications and should lower the validation MSE.\nBelow is the updated script implementing TTA.", "I propose to integrate test\u2010time augmentation (TTA) by averaging predictions\nover multiple small Gaussian\u2010noisy copies of each input during validation and\ntest inference. After updating batch\u2010norm statistics on the SWA\u2010averaged model,\nwe generate several noisy versions (e.g., 5 copies) of the validation and test\ninputs, average their predictions in the normalized target space, and then\ninverse\u2010transform to obtain the final outputs. This smoothing reduces prediction\nvariance and should lower both validation and overall OOF MSE without altering\nthe training pipeline.", "I propose to enhance test-time augmentation by enabling Monte Carlo Dropout\nduring inference and increasing the number of noisy forward passes to ten. After\nSWA and batch\u2010norm update, we set the model to train mode so dropout remains\nactive, then average predictions over Gaussian\u2010noisy inputs and stochastic\ndropout. This smoothing reduces prediction variance and should lower the\nvalidation MSE without altering training. All other components\u2014including the\nTabTransformer architecture, SWA, and 5\u2011fold cross\u2011validation\u2014remain unchanged.", "I propose integrating test-time augmentation (TTA) by averaging predictions over\nmultiple small Gaussian\u2011noisy copies of each input during validation and test\ninference. After SWA and batch\u2011norm updates, we generate, say, five noisy\nversions of each batch, average their predictions in the normalized target\nspace, then inverse\u2011transform to compute final outputs. This simple change can\nreduce prediction variance without altering the training pipeline, potentially\nlowering the validation and OOF MSE.", "I will integrate test-time augmentation (TTA) by averaging predictions over five\nGaussian\u2010noisy copies of each input during validation and test inference. After\nupdating the batch\u2010norm statistics on the SWA model, we generate five noisy\nversions of each scaled input (std=0.01), average their predictions in the\nstandardized target space, then inverse\u2010transform before computing MSE. This\nsmoothing should reduce prediction variance and yield a lower validation error\nwithout modifying the training pipeline. All other components (5\u2011fold CV, SWA,\noptimizer settings) remain unchanged.", "I propose to integrate mixup data augmentation into the training loop by\nblending pairs of samples and their targets with a Beta(\u03b1,\u03b1) distribution\n(\u03b1=0.2). This regularizes the model by encouraging smoother mappings between\ngeometry inputs and spectrum outputs without altering the existing architecture,\nSWA, or cross\u2010validation pipeline. We apply mixup (input and target\ninterpolation) before adding Gaussian noise, then proceed as before. This atomic\nchange should improve generalization and reduce the validation MSE.", "I propose integrating test-time augmentation (TTA) by averaging predictions over\nmultiple small Gaussian\u2010noisy copies of each input during validation and test\ninference. After updating batch\u2010norm statistics on the SWA\u2010averaged model, we\ngenerate, e.g., five noisy versions of each input batch (std=0.01), average\ntheir standardized predictions, then inverse\u2010transform to compute final outputs.\nThis atomic change will reduce prediction variance and should lower the\nvalidation and OOF MSE without altering the training pipeline. Below is the\nupdated script with TTA implemented.", "I propose to improve generalization by performing Monte Carlo Dropout during\ninference: after SWA batch\u2010norm updates, we will switch the model to train mode\nto keep dropout active, run multiple stochastic forward passes on both the\nvalidation and test inputs, average these predictions in the normalized target\nspace, and then inverse\u2010transform to compute the final outputs. This Monte Carlo\nDropout Test\u2010Time Augmentation (MC\u2010TTA) reduces predictive variance and should\nlower the validation MSE. Below is the updated script.", "I propose adding test-time augmentation (TTA) by averaging predictions over\nmultiple small Gaussian\u2010noisy copies of each input during validation and test\ninference. This change is atomic and does not affect training: after SWA and BN\nupdate, we generate several noisy versions (e.g.\u00a05 copies) of each validation\nand test batch, average their model outputs in the standardized target space,\nand then inverse\u2010transform to compute final predictions. This smoothing reduces\nprediction variance and should lower the validation MSE. All other training and\nmodel components remain unchanged.", "I propose adding test-time augmentation (TTA) by averaging model predictions\nover multiple small Gaussian-noisy copies of each input during validation and\ntest inference. After updating batch\u2010norm statistics on the SWA\u2010averaged model,\nwe generate several noisy versions (e.g., 5 copies) of each validation and test\nbatch, average their predictions in the standardized target space, and then\ninverse\u2010transform to obtain final outputs. This smoothing reduces prediction\nvariance and should lower the validation and overall OOF MSE without altering\nthe training pipeline.", "I propose adding test-time augmentation (TTA) by averaging model predictions\nover multiple small Gaussian\u2010noisy copies of each input during validation and\ntest inference. After updating batch\u2010norm statistics on the SWA\u2010averaged model,\nwe generate several noisy versions (e.g.\u00a05 copies) of each sample, average their\npredictions in the standardized target space, and then inverse\u2010transform to\ncompute final outputs. This smoothing should reduce prediction variance and\nlower validation and overall OOF MSE without changing training. Below is the\nupdated script with TTA implemented.", "I propose to integrate test-time augmentation (TTA) by averaging predictions\nover multiple small Gaussian-noisy copies of each input during validation and\ntest inference. After updating batch\u2010norm statistics on the SWA\u2010averaged model,\nwe generate several noisy versions (e.g., 5 copies) of each validation and test\nbatch, average their predictions in the normalized target space, and then\ninverse\u2010transform to compute the final outputs. This smoothing reduces\nprediction variance and should lower the validation and overall OOF MSE without\naltering the training pipeline.", "I propose integrating Monte Carlo Dropout Test\u2011Time Augmentation by switching\nthe SWA\u2011averaged model to train mode (to enable dropout) and performing multiple\nstochastic forward passes with small Gaussian noise added to the inputs. We then\naverage these predictions for both validation and test inference, which should\nreduce prediction variance and lower the MSE. This change is atomic and keeps\nthe existing architecture, SWA pipeline, and cross\u2011validation unchanged.", "I propose adding test-time augmentation by averaging model predictions over\nmultiple small Gaussian\u2010noisy copies of each input during validation and test\ninference. After SWA and batch\u2010norm updates, we generate several noisy versions\n(e.g., 5 copies) of each batch, average their scaled outputs, then\ninverse\u2010transform to obtain final predictions. This simple change can reduce\nprediction variance and lower the MSE without altering the training pipeline.", "I propose adding test-time augmentation (TTA) by averaging model predictions\nover multiple small Gaussian-noisy copies of each validation and test input.\nAfter SWA and batch\u2010norm updates, we generate, for example, five noisy versions\nof each sample (std=0.01), average their predictions in the standardized target\nspace, then inverse\u2010transform to compute final outputs. This smoothing reduces\nprediction variance and should lower validation and OOF MSE without altering the\ntraining pipeline.", "I propose adding test-time augmentation (TTA) by averaging predictions over\nmultiple small Gaussian\u2011noisy copies of each validation and test input. After\nupdating batch\u2011norm statistics on the SWA model, we will generate a fixed number\nof noisy versions (e.g.\u00a05 copies) of each batch, average their predictions in\nthe standardized target space, and then inverse\u2011transform to compute final\noutputs. This smoothing should reduce prediction variance and lower the\nvalidation and OOF MSE without altering the training pipeline. Below is the\nupdated script implementing TTA.", "I propose to integrate mixup augmentation into the training loop by sampling a\nmixing coefficient from a Beta(\u03b1,\u03b1) distribution (\u03b1=0.2), shuffling each batch,\nand linearly combining inputs and their targets before adding Gaussian noise.\nThis regularizes the model by encouraging smoother mappings between geometry\nparameters and spectra without altering the existing TabTransformer+SWA pipeline\nor cross\u2010validation setup. All other components\u2014including polynomial feature\nexpansion, target scaling, SWA, and 5\u2011fold CV\u2014remain unchanged. Below is the\nupdated script implementing mixup data augmentation and printing the final OOF\nMSE.", "I propose integrating test-time augmentation (TTA) by averaging predictions over\nmultiple small Gaussian\u2011noisy copies of each input during validation and test\ninference. After SWA and batch\u2011norm updates, we will generate a fixed number\n(e.g., 5) of noisy versions of each validation and test batch (std=0.01),\naverage their standardized predictions, and then inverse\u2011transform to obtain the\nfinal outputs. This smoothing reduces prediction variance and should lower both\nthe fold-wise and overall OOF MSE without altering the training pipeline. Below\nis the updated script implementing Gaussian\u2011noise TTA at inference.", "I propose integrating test-time augmentation (TTA) by averaging predictions over\nmultiple small Gaussian\u2010noisy copies of each input during validation and test\ninference. After updating batch\u2010norm statistics on the SWA model, we generate\nfive noisy versions of each validation and test batch, average their\nstandardized predictions, and then inverse\u2010transform to compute the final\nspectrum outputs. This atomic change does not require retraining and should\nreduce prediction variance and lower the validation MSE. We keep the existing\nTabTransformer architecture, SWA pipeline, and 5\u2010fold cross\u2010validation\nunchanged.", "I propose to integrate test\u2010time augmentation (TTA) by averaging predictions\nover multiple small Gaussian\u2010noisy copies of each input during validation and\ntest inference. After updating batch\u2010norm statistics on the SWA model, we\nperform, e.g., five forward passes with noise (`x_noisy = x +\ntorch.randn_like(x)*0.01`) and average the outputs before inverse\u2010transforming\nand computing MSE. This atomic change does not alter training but should reduce\nprediction variance and lower the validation MSE. Below is the updated script\nimplementing TTA.", "I propose adding Monte Carlo test\u2010time augmentation by averaging model\npredictions over multiple small Gaussian\u2010noisy copies of each input during\nvalidation and test inference. After updating batch\u2010norm statistics on the\nSWA\u2010averaged model, we switch to train mode to enable dropout and perform, for\nexample, five stochastic forward passes with input noise (std=0.01), averaging\nthe standardized outputs before inverse\u2010transforming to compute final\npredictions. This simple change does not alter training or architecture but\nshould reduce prediction variance and lower both fold\u2010wise and overall OOF MSE.\nBelow is the updated script implementing this TTA approach.", "I propose to integrate test-time augmentation (TTA) by averaging predictions\nover multiple small Gaussian\u2010noisy copies of each input during validation and\ntest inference. After updating batch\u2010norm stats on the SWA model, we perform\nseveral forward passes adding noise (`x + \u03b5` with \u03b5\u223cN(0,0.01)) and average the\nscaled outputs before inverse\u2010transforming to compute final predictions. This\nsimple atomic change requires no retraining and often reduces variance, leading\nto a lower validation MSE. Below is the updated script with TTA implemented.", "I propose adding a spectral smoothness regularization term to the loss by\npenalizing the second\u2010order differences of the 2001\u2010dimensional output, which\nencourages the model to produce smoother spectra and can reduce overfitting. We\nintegrate this penalty with a small weight (\u03bb=1e-4) into the existing SWA\ntraining loop and keep all other aspects\u2014TabTransformerReg architecture,\nGaussian input noise, SWA, 5-fold CV and inference\u2014unchanged. This single change\nshould lower the validation and OOF MSE by enforcing smoother predictions.", "I propose adding test-time augmentation by averaging predictions over multiple\nGaussian-noisy copies of each input during validation and test inference. After\nupdating batch\u2010norm statistics on the SWA model, we perform 5 stochastic forward\npasses with `x + \u03b5` (\u03b5\u223cN(0,0.01)) and average the outputs before\ninverse\u2010transforming. This smoothing does not require retraining and can reduce\nprediction variance and lower the validation MSE.", "I propose to integrate test-time augmentation by averaging predictions over\nmultiple small Gaussian-noisy copies of each validation and test input. After\nupdating batch\u2010norm stats on the SWA\u2010averaged model, we will perform N=5\nstochastic forward passes per sample adding noise (std=0.01) to the inputs and\naverage their standardized outputs before inverse\u2010transforming to compute final\npredictions. This smoothing will reduce prediction variance and should yield a\nlower validation and OOF MSE without altering the training pipeline.", "I propose adding mixup data augmentation to the training loop by sampling a\nmixing coefficient from a Beta(\u03b1,\u03b1) distribution (\u03b1=0.2), shuffling each batch,\nand linearly combining inputs and their targets before adding Gaussian noise.\nThis will regularize the model by encouraging smoother mappings and reduce\noverfitting. All other components\u2014including the TabTransformer+ResMLP\narchitecture, smoothness penalty, SWA, and 5-fold CV\u2014remain unchanged so we can\nisolate the effect of mixup on validation MSE.", "I propose adding test-time augmentation by averaging predictions over five small\nGaussian\u2010noisy copies of each input during validation and test inference. After\nupdating batch\u2010norm statistics on the SWA model, we generate five noisy versions\nof each batch (`x + \u03b5`, \u03b5\u223cN(0,0.01)), average their outputs in the normalized\ntarget space, and then inverse\u2010transform to compute final predictions. This\nsmoothing reduces prediction variance and should lower both fold\u2010wise and\noverall OOF MSE without altering training or model architecture.", "I propose integrating test-time augmentation (TTA) by averaging predictions over\nmultiple small Gaussian\u2011noisy copies of each input during validation and test\ninference. After updating batch\u2011norm statistics on the SWA model in each fold,\nwe perform N=5 noisy forward passes (\u03c3=0.01) and average the outputs before\ninverse\u2011transforming to compute the final spectra. This smoothing reduces\nprediction variance and should lower the MSE without changing the training\nroutine.", "I propose applying test\u2011time augmentation by averaging predictions over multiple\nsmall Gaussian\u2011noised copies of each validation and test input. This simple\nchange reduces variance in predictions without retraining. We implement N=5 TTA\nat inference and compute both fold\u2011wise and overall OOF MSE.", "I propose to integrate test-time augmentation (TTA) by averaging model\npredictions over multiple small Gaussian-noisy copies of each input during\nvalidation and test inference. After updating batch\u2010norm statistics on the\nSWA\u2010averaged model, we perform N=5 stochastic forward passes adding noise\n(\u03c3=0.01) to the inputs and average the standardized outputs before\ninverse\u2010transforming to compute the final predictions. This smoothing reduces\nprediction variance and should lower the validation MSE without altering the\ntraining pipeline. The rest of the cross\u2010validation, SWA, scaling, and model\narchitecture remain unchanged.", "I propose to integrate Monte Carlo Dropout test\u2011time augmentation by enabling\ndropout and input noise at inference: after SWA and BN update, we perform\nmultiple stochastic forward passes (e.g.\u00a05) with small Gaussian noise added to\ninputs, average the predictions, then inverse\u2011transform and compute the MSE.\nThis smoothing reduces prediction variance without altering the training\npipeline. Below is the updated script implementing MC\u2011Dropout TTA in validation\nand test inference.", "I propose to integrate test-time augmentation (TTA) by averaging model\npredictions over multiple small Gaussian-noisy copies of each input during\nvalidation and test inference. After updating batch\u2010norm statistics on the SWA\nmodel, we generate a fixed number of noisy versions (e.g., 5 copies) of each\nvalidation and test batch (`x + \u03b5`, \u03b5\u223cN(0,0.01)), average their outputs in the\nstandardized target space, then inverse\u2010transform to compute final predictions.\nThis atomic change requires no retraining and should reduce prediction variance\nand lower the validation MSE.", "I propose enhancing the inference stage with Monte Carlo Dropout and\nGaussian\u2010noise test\u2010time augmentation: after updating batch\u2010norm statistics on\nthe SWA model, we switch to train mode (to keep dropout active) and perform\nseveral stochastic forward passes with small input noise, averaging their\npredictions to reduce variance and improve generalization. This change is purely\natomic at inference and should lower the validation MSE without retraining. The\nrest of the TabTransformer + ResMLP, SWA training, and 5\u2011fold CV pipeline\nremains unchanged.", "I propose to integrate mixup data augmentation into the training loop: for each\nbatch we sample a mixing coefficient from a Beta(\u03b1,\u03b1) distribution (\u03b1=0.2),\nshuffle the batch, and form convex combinations of inputs and their targets.\nThis regularizes the model by encouraging smoother mappings and helps reduce\noverfitting without altering the existing architecture, SWA, or smoothing\npenalty. All other components\u2014TabTransformerReg, SWA scheduling, smoothness\npenalty, and 5-fold CV\u2014remain unchanged for a direct comparison of mixup\u2019s\neffect.", "I propose increasing the transformer embedding dimension from 32 to 64 and\ndoubling the number of attention heads to 8 in the TabTransformerReg module.\nThis atomic change enhances the model\u2019s capacity to capture complex feature\ninteractions while keeping all other pipeline components (smoothness penalty,\nSWA, input noise, and 5\u2011fold CV) unchanged. We will retrain under the same\nsettings and report the resulting OOF MSE as well as save test predictions.", "I propose to integrate test-time augmentation (TTA) by averaging model\npredictions over multiple small Gaussian\u2010noisy copies of each input during\nvalidation and test inference. After SWA and batch\u2010norm updates, we perform N=5\nstochastic forward passes per sample with noise (\u03c3=0.01), average the outputs in\nthe standardized target space, and then inverse\u2010transform to compute final\nspectra. This smoothing reduces prediction variance and should lower the\nvalidation MSE without altering the training pipeline.", "I propose increasing the TabTransformer\u2019s capacity by doubling the embedding\ndimension from 64 to 128 and adding an extra transformer layer (from 2 to 3),\nwhich should allow the model to capture more complex interactions among the 14\ngeometry features. This change is atomic and retains the existing SWA,\nsmoothness penalty, TTA, and 5\u2011fold CV pipeline for direct comparison. All other\nhyperparameters and training settings remain unchanged to isolate the effect of\nthis architectural enhancement. Below is the updated script implementing this\nmodification and printing fold-wise and overall OOF MSE.", "I propose enabling Monte Carlo Dropout during test\u2010time augmentation by\nswitching the SWA model into train mode at inference, so dropout layers remain\nactive while we average predictions over noisy inputs. This requires no\nretraining and should further reduce prediction variance and lower the\nvalidation MSE. All other aspects of the training, SWA, and TTA pipeline remain\nunchanged.", "I propose adding a CosineAnnealingLR scheduler to decay the learning rate from\n1e-3 down to the SWA learning rate (5e-4) over the first swa_start epochs,\nstepping each epoch before SWA begins. This pre-SWA schedule refines the\noptimization trajectory before model averaging, while preserving the existing\narchitecture, SWALR, and other hyperparameters. After swa_start epochs, we\nswitch to the SWALR scheduler as before. This atomic change often yields\nsmoother convergence and a lower validation MSE.", "I propose to further stabilize and improve inference by increasing the number of\ntest\u2010time augmentations (TTA) from 5 to 10 and reducing the Gaussian noise\namplitude from 0.01 to 0.005. Averaging more stochastic predictions with smaller\nperturbations can reduce variance and yield a lower validation MSE without\nretraining the model. All other components (model, SWA, CV, training loop)\nremain unchanged.", "I propose increasing test-time augmentation by doubling the number of noisy\nforward passes from 10 to 20 and reducing the noise scale from 0.005 to 0.003.\nThis will further reduce prediction variance and improve generalization without\naltering the training pipeline, model architecture, or hyperparameters such as\nSWA and cosine annealing. All other components remain the same for a direct\ncomparison of this atomic change.", "I propose to reduce the complexity of predicting a 2001\u2010dimensional spectrum by\nfirst applying PCA to compress targets to 100 principal components, training the\nTabTransformer\u2011ResMLP model to predict these components, and then reconstructing\nthe full spectrum via inverse PCA and standard scaling. This targets\u2010PCA step\nregularizes the output space, reduces noise, and should improve generalization.\nAll other parts of the pipeline\u2014including SWA, cosine learning rate scheduling,\ninput noise augmentation, smoothness penalty, TTA, and 5\u2011fold\ncross\u2010validation\u2014remain unchanged. The only atomic change is the addition of\ntarget PCA with 100 components.", "I propose increasing the number of PCA components used to compress the spectral\ntargets from 100 to 200, which allows the model to capture finer spectral\ndetails and reduces reconstruction error. This change is atomic\u2014everything else\n(TabTransformerReg, SWA, smoothing penalty, CosineAnnealing, TTA, 5\u2011fold CV)\nremains unchanged\u2014so we can directly measure its impact on the OOF MSE. We will\nrerun 5\u2011fold CV, print per\u2011fold and overall validation MSE, and save averaged\ntest predictions.", "I propose to refine the inference smoothing by increasing the number of\ntest\u2011time augmentations from 20 to 50 while reducing the Gaussian noise level to\n0.002. This change will yield more stable averaged predictions and lower\nvariance without altering the training pipeline. All other components\u2014PCA, SWA,\nCosineAnnealing, and the TabTransformer+ResMLP architecture\u2014remain unchanged.", "I propose increasing the number of PCA components for the target compression\nfrom 100 to 150, which should capture more spectral variance and reduce\nreconstruction error. This atomic change leaves the TabTransformerReg\narchitecture, SWA training, smoothness penalty, cosine annealing, and TTA setup\nunchanged, allowing us to directly measure the effect of a richer PCA\nrepresentation on validation MSE. We will rerun 5\u2011fold CV with pca_dims=150,\nprint the per\u2011fold and overall OOF MSE, and save averaged test predictions to\n`submission.csv`.", "I propose adding gradient norm clipping with a maximum norm of 1.0 to the\ntraining loop by calling `torch.nn.utils.clip_grad_norm_` after\n`loss.backward()`. This atomic change stabilizes gradient updates, prevents\nexploding gradients, and should improve convergence and generalization without\naltering the rest of the TabTransformer+ResMLP+SWA+PCA+TTA pipeline. No other\nparts of the code are modified so we can directly measure the impact on the\nvalidation MSE.", "We double the transformer embedding dimension from 128 to 256 and increase the\nnumber of attention heads from 8 to 16 in the TabTransformerReg, boosting the\nmodel\u2019s capacity to learn complex interactions among the 14 geometry parameters.\nAll other aspects of the pipeline\u2014including PCA target reduction (100\ncomponents), SWA, CosineAnnealingLR, input noise augmentation, smoothness\npenalty, gradient clipping, and 5\u00d7 TTA\u2014remain unchanged to isolate this change.\nWe retrain with 5\u2011fold CV, print per\u2011fold and overall OOF MSE, and save averaged\ntest predictions to `./working/submission.csv`.", "I propose to replace the GELU activations in the MLP head (ResMLP) with SiLU\n(Swish) activations, which often yield smoother gradients and better\ngeneralization. This atomic change preserves the TabTransformer architecture,\nSWA training, PCA on targets, smoothing penalty, and test\u2010time augmentation,\nallowing direct measurement of its effect on validation MSE. We will rerun the\n5\u2010fold cross\u2010validation pipeline with these new activations and print per\u2010fold\nand overall out\u2010of\u2010fold MSE.", "I propose to increase the number of test\u2010time augmentation passes from 50 to 100\nto average over more noisy predictions and further reduce prediction variance\nduring inference, without altering the training or model architecture. This\natomic change is expected to yield a lower validation MSE by smoothing out\nrandomness more effectively. All other components of the pipeline remain\nunchanged.", "I propose to increase the smoothness penalty weight from 1e-4 to 5e-4 in the\nloss, enforcing stronger regularization on the second\u2010order differences of the\npredicted spectra. This change is atomic and retains the existing\nTabTransformerReg architecture, PCA target reduction, SWA training, TTA, and\nother hyperparameters. The stronger smoothness constraint should reduce\noverfitting to high\u2010frequency noise and lower the validation MSE.", "I propose increasing the smoothness penalty weight from 1e-4 to 1e-3 in the loss\nfunction to more strongly penalize high-frequency fluctuations in the predicted\nspectra. This single atomic change leaves the TabTransformer + ResMLP\narchitecture, SWA scheduling, PCA target reduction, CosineAnnealingLR, and\ntest\u2010time augmentation pipeline unchanged. We will rerun the 5\u2011fold\ncross\u2011validation and report per\u2011fold and overall OOF MSE to assess the effect of\nstronger smoothness regularization.", "I propose to further reduce prediction variance at inference by increasing the\nnumber of test\u2010time augmentations from 100 to 200 while halving the Gaussian\nnoise level from 0.002 to 0.001. This change will smooth out randomness more\neffectively without altering the training pipeline or model architecture. The\nrest of the training, SWA, PCA target reduction, and CosineAnnealing scheduling\nremain unchanged.", "I propose integrating mixup data augmentation into the training loop with \u03b1=0.2:\nfor each batch we draw \u03bb\u223cBeta(\u03b1,\u03b1) and mix both inputs (after scaling) and\nPCA\u2011compressed targets before adding Gaussian noise. This single change\nregularizes the model by encouraging smoother mappings without altering SWA, PCA\ndims, or TTA settings. We keep all other hyperparameters and pipeline steps\nunchanged, then report fold MSEs and overall OOF MSE and save test predictions\nas before.", "I propose increasing the dropout rate in the ResMLP head from 0.2 to 0.3 to\nfurther regularize the network and reduce overfitting. This atomic change\npreserves the TabTransformerReg architecture, PCA target compression, SWA\ntraining, learning rate schedule, and test\u2010time augmentation pipeline. We\nretrain using 5\u2010fold CV, then evaluate and report the fold\u2010wise and overall\nout\u2010of\u2010fold MSE, and save averaged test predictions to `submission.csv`.", "We increase the dropout rate in the TransformerEncoderLayer from the default\u00a00.1\nto\u00a00.3 to further regularize feature interactions in the TabTransformer. This\natomic change only affects the transformer\u2019s internal dropout, leaving all other\ncomponents (PCA on targets, SWA, smoothness penalty, learning rate scheduling,\nMC\u2010TTA, etc.) unchanged. We retrain using the same 5\u2010fold CV configuration and\nprint the fold\u2010wise and overall OOF MSE, finally saving test predictions to\n`./working/submission.csv`.", "I propose increasing the spectral smoothness regularization strength by setting\nthe smoothness penalty weight (`lambda_smooth`) to 5e-4 instead of 1e-4. This\nshould more strongly penalize high\u2011frequency fluctuations in the predicted PCA\ncomponents, yielding smoother reconstructed spectra and reducing overfitting.\nAll other parts of the SWA + PCA + TTA pipeline remain unchanged.", "I propose increasing the number of PCA components for the target spectral\ncompression from 100 to 150, enabling the model to capture more spectral\nvariance and reduce reconstruction error. This change is atomic and keeps the\nTabTransformerReg architecture, SWA training, smoothness penalty, and TTA\nsettings unchanged, so we can directly measure its impact on validation MSE. We\nthen perform 5\u2011fold CV, print fold-wise and overall OOF MSE, and save averaged\ntest predictions to `submission.csv`.", "I propose to start Stochastic Weight Averaging earlier in training (at epoch 15\ninstead of 25) so that more model snapshots are averaged, yielding a smoother\noptimization trajectory and potentially better generalization. This atomic\nchange keeps all other hyperparameters and the architecture unchanged, allowing\ndirect comparison of its impact. We retrain with the adjusted `swa_start = 15`,\nperform 5-fold CV, and print out the OOF validation MSE, then save the averaged\ntest predictions.", "I propose to start stochastic weight averaging earlier at epoch\u00a015 instead of\n25, allowing the model to average more diverse weight snapshots and potentially\nimprove generalization. This atomic change simply updates `swa_start` and the\ncorresponding CosineAnnealingLR schedule, leaving all other architecture, PCA,\nSWA, and TTA settings unchanged. We then run 5\u2011fold CV, print per\u2011fold and\noverall OOF MSE, and save averaged test predictions.", "I propose doubling the number of test\u2010time augmentations to 200 while reducing\nthe Gaussian noise amplitude from 0.002 to 0.001, further smoothing predictions\nvia Monte Carlo noise+dropout at inference without changing the training\npipeline. This will reduce prediction variance and should lower the hold\u2010out\nvalidation MSE. All other training, SWA, PCA, and cross\u2010validation settings\nremain unchanged.", "I propose to apply a simple 1D Gaussian smoothing filter over the final\npredicted spectra at inference time, which will reduce high\u2011frequency noise and\nimprove MSE. After we inverse\u2011transform the PCA and scaler on both validation\nand test outputs, we convolve each spectrum with a small Gaussian kernel\n(sigma=1) before computing the MSE and before writing out `submission.csv`. This\nis an atomic change that requires no retraining and should further lower the\nhold\u2011out error.", "I propose starting stochastic weight averaging (SWA) earlier in training at\nepoch\u00a015 instead of 25. Averaging more diverse model checkpoints should further\nstabilize the final weights and improve generalization. We leave all other\ncomponents\u2014TabTransformerReg architecture, PCA target reduction, smoothness\npenalty, CosineAnnealingLR, input noise, gradient clipping, TTA, and 5\u2011fold\nCV\u2014unchanged to isolate this effect.", "I propose increasing the number of PCA components used to compress the\n2001\u2010dimensional target spectra from 150 to 200. This atomic change lets the\nmodel reconstruct finer spectral details, reducing reconstruction error without\naltering the TabTransformerReg architecture, SWA training, smoothness penalty,\ngradient clipping, or test\u2010time augmentation. We rerun the 5\u2010fold\ncross\u2010validation with `pca_dims=200`, print per\u2010fold and overall OOF validation\nMSE, and save the averaged test predictions to `./working/submission.csv`.", "I propose to apply a lightweight 1D smoothing filter over the final predicted\nspectra at inference time to reduce high-frequency noise and improve MSE,\nwithout retraining. Specifically, after inverse\u2010transforming predictions on both\nvalidation and test sets, we convolve each spectrum with a small triangular\nkernel ([1,2,1]/4) before computing the validation MSE and saving the test\nsubmission. This post\u2010processing step enforces local smoothness and can lower\noverall error with zero additional training cost.", "I propose adding a lightweight 1D triangular smoothing filter (kernel [1,2,1]/4)\nto the predicted spectra at inference time on both validation and test outputs.\nThis post\u2011processing step smooths out high\u2011frequency noise and should lower the\nMSE without any retraining or architectural changes. The rest of the pipeline\n(TabTransformer, SWA, PCA, TTA, etc.) remains unchanged.", "I propose to replace the training loss from MSE to Smooth L1 Loss (Huber) so the\nmodel is less sensitive to outlier spectral points and can generalize better.\nThis is an atomic change\u2014only the loss criterion is updated\u2014while keeping all\nother architectures, hyperparameters, SWA, PCA, and TTA settings unchanged. We\nthen retrain the same 5\u2011fold cross\u2011validation pipeline and report the validation\nMSE under this new robust loss.", "I propose to post-process the predicted spectra with a lightweight triangular\nsmoothing filter at inference time, convolving each spectrum with the kernel\n[1,2,1]/4. This smoothing step will reduce high-frequency noise without changing\nthe training pipeline, and is applied to both validation and test predictions\nbefore computing MSE and saving the submission. All other aspects of the\nTabTransformerReg+PCA+SWA+TTA pipeline remain unchanged.", "I propose increasing the number of PCA components used to compress the target\nspectra from 150 to 200, which should reduce reconstruction error and allow the\nmodel to better capture spectral details. This change is atomic and leaves the\nTabTransformerReg architecture, SWA training, smoothness penalty, and test\u2010time\naugmentation pipeline unchanged. We rerun 5\u2011fold cross\u2010validation with\n`pca_dims=200`, print fold-wise and overall OOF MSE, and save averaged test\npredictions to `./working/submission.csv`.", "I propose replacing the Smooth L1 (Huber) loss with the Mean Squared Error loss\nto directly optimize the same metric used for evaluation. Aligning the training\nobjective with the evaluation MSE should yield tighter convergence toward\nminimizing squared errors. All other components including PCA, SWA, TTA, and\nsmoothness regularization remain unchanged to isolate this effect.", "I propose applying a lightweight triangular smoothing filter to the predicted\nspectra at inference time to reduce high-frequency noise and lower MSE without\nretraining. After inverse-transforming both validation and test predictions, we\nconvolve each spectrum with the kernel [1,2,1]/4 along the spectral dimension.\nThis post\u2011processing step is atomic, preserves the existing TabTransformer+SWA\npipeline, and should reduce prediction variance and error.", "I propose applying a lightweight triangular smoothing filter (kernel [1,2,1]/4)\nto the predicted spectra at inference time on both validation and test outputs.\nAfter inverse\u2010transforming predictions back to the original scale, we convolve\neach spectrum with this kernel along the spectral dimension to suppress\nhigh\u2011frequency noise before computing MSE and saving submissions. This\npost\u2010processing is atomic and requires no retraining, yet can reduce the final\nerror by enforcing local smoothness. All other parts of the TabTransformer+SWA\npipeline remain unchanged.", "I propose adding a lightweight triangular smoothing filter ([1,2,1]/4) to the\npredicted spectra at inference time to suppress high\u2011frequency noise and improve\nMSE. After inverse\u2010transforming the PCA outputs for both the validation and test\nsets, we convolve each spectrum with this kernel by applying boundary\u2011aware\naveraging before computing the MSE and saving the submission. This change is\npurely a post\u2011processing step and does not affect training.", "I will apply a lightweight 1D triangular smoothing filter ([1, 2, 1]/4) to the\npredicted spectra at inference time for both validation and test outputs. This\npost\u2011processing step suppresses high\u2011frequency noise and can reduce the MSE\nwithout retraining or changing the model architecture. I integrate this\nsmoothing just before computing the validation metric and saving the submission.", "I propose increasing the number of PCA components for the target spectra from\n200 to 300, which allows the model to reconstruct more spectral variance and\nreduce reconstruction error. This atomic change leaves all other\naspects\u2014TabTransformerReg architecture, SWA training, smoothness penalty, and\nTTA pipeline\u2014unchanged for direct comparison. We will rerun the same 5-fold CV\nwith `pca_dims=300`, print the overall OOF MSE, and save test predictions.", "We increase the number of Transformer encoder layers in the TabTransformer from\n3 to 4 to allow the model to capture more complex interactions among the\ngeometry features. This change is isolated and leaves all other\naspects\u2014including PCA compression, SWA, learning\u2011rate scheduling, smoothness\nregularization, gradient clipping, and test\u2011time augmentation\u2014unchanged. We\nrerun 5\u2011fold cross\u2011validation, print each fold\u2019s validation MSE and the overall\nOOF MSE, then save averaged test predictions to `./working/submission.csv`.", "I propose applying a lightweight triangular smoothing filter to the predicted\nspectra at inference time to suppress high-frequency noise and improve MSE.\nAfter inverse\u2010transforming the PCA-based predictions back to the original\nspectral space, we convolve each spectrum with a [1, 2, 1]/4 kernel by padding\nedges. This post\u2010processing is applied to both validation predictions (before\ncomputing MSE) and test ensemble predictions (before saving), requiring no\nretraining yet reducing variance and lowering the validation error.", "I propose to apply a lightweight triangular smoothing filter over the final\npredicted spectra at inference time, convolving each spectrum with kernel\n[1,2,1]/4. This post\u2010processing reduces high\u2011frequency noise in the outputs\nwithout changing training or model architecture. We integrate this smoothing\nimmediately before computing each fold\u2019s validation MSE and again on the final\naveraged test predictions before saving the submission.", "I propose increasing the TabTransformer embedding dimension from 256 to 512\nwhile keeping the rest of the pipeline unchanged (4 transformer layers, 16\nheads, PCA=200, SWA, cosine annealing, TTA, etc.). This atomic change gives the\nmodel greater representational capacity to capture feature interactions and\nshould reduce the validation MSE. Below is the updated script implementing this\nimprovement.", "I propose increasing the number of test\u2010time augmentation (TTA) passes from 100\nto 200 while reducing the Gaussian noise amplitude from 0.002 to 0.001. This\nchange at inference will average over more stochastic predictions with finer\nperturbations, reducing variance and improving the MSE without altering the\ntraining pipeline. We keep the existing TabTransformerReg architecture, PCA\ncompression on targets, SWA training, and 5\u2011fold CV unchanged. The script below\nimplements this by updating `num_tta` and `tta_noise`, then rerunning CV and\nsaving the submission.", "I propose applying a lightweight triangular smoothing filter ([1,2,1]/4) to the\npredicted spectra at inference time, convolving each spectrum with this kernel\nto suppress high\u2010frequency noise and improve the MSE without retraining. This\npost\u2010processing step is integrated immediately before computing the validation\nerror and again on the final test predictions before saving the submission.", "I propose adding a lightweight 1D Gaussian smoothing filter (kernel\n[1,4,6,4,1]/16) to the final predicted spectra at inference time for both\nvalidation and test outputs. This post\u2011processing step will suppress\nhigh\u2011frequency noise without modifying the training pipeline or model\narchitecture, and should further lower the MSE. All other components of the\nTabTransformer+PCA+SWA+TTA pipeline remain unchanged. Below is the updated\nscript implementing Gaussian smoothing at inference.", "I propose replacing the Smooth L1 (Huber) loss with the Mean Squared Error loss\nduring training so that the model directly optimizes the same metric used for\nevaluation. This atomic change aligns the training objective with the validation\nMSE and should yield lower test\u2010time squared error. All other aspects of the\npipeline (PCA, SWA, TTA, etc.) remain unchanged.", "I propose applying a lightweight triangular smoothing filter ([1,2,1]/4) to the\npredicted spectra at inference time. After inverse\u2010transforming both validation\nand test predictions back to the original scale, each spectrum is convolved with\nthis kernel to suppress high\u2010frequency noise. This post\u2010processing is applied\nimmediately before computing the validation MSE and again before saving the test\nsubmission, reducing prediction variance with zero retraining cost."], "code": ["import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.preprocessing import StandardScaler\n\n# Paths\ntrain_g_path = \"./input/g_training.csv\"\ntrain_s_path = \"./input/s_training.csv\"\nval_g_path = \"./input/g_validation.csv\"\nval_s_path = \"./input/s_validation.csv\"\ntest_g_path = \"./input/test_g.csv\"\nsubmission_path = \"./working/submission.csv\"\n\n# Set random seeds\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load data\nX_train = pd.read_csv(train_g_path, header=None).values.astype(np.float32)\ny_train = pd.read_csv(train_s_path, header=None).values.astype(np.float32)\nX_val = pd.read_csv(val_g_path, header=None).values.astype(np.float32)\ny_val = pd.read_csv(val_s_path, header=None).values.astype(np.float32)\nX_test = pd.read_csv(test_g_path, header=None).values.astype(np.float32)\n\n# Standardize inputs\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)\n\n# Prepare datasets\nbatch_size = 128\ntrain_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\nval_ds = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n\n\n# Define model\nclass MLP(nn.Module):\n    def __init__(self, in_dim=14, hidden=512, out_dim=2001):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(in_dim, hidden),\n            nn.ReLU(),\n            nn.Linear(hidden, hidden),\n            nn.ReLU(),\n            nn.Linear(hidden, out_dim),\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n\nmodel = MLP().to(device)\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# Training loop\nepochs = 50\nfor epoch in range(epochs):\n    model.train()\n    for xb, yb in train_loader:\n        xb, yb = xb.to(device), yb.to(device)\n        pred = model(xb)\n        loss = criterion(pred, yb)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n# Validation\nmodel.eval()\nval_losses = []\nwith torch.no_grad():\n    for xb, yb in val_loader:\n        xb, yb = xb.to(device), yb.to(device)\n        pred = model(xb)\n        val_losses.append(criterion(pred, yb).item() * xb.size(0))\nmse_val = sum(val_losses) / len(val_ds)\nprint(f\"Validation MSE: {mse_val:.6e}\")\n\n# Predict on test and save submission\nmodel.eval()\nwith torch.no_grad():\n    X_test_t = torch.from_numpy(X_test).to(device)\n    preds = model(X_test_t).cpu().numpy()\npd.DataFrame(preds).to_csv(submission_path, index=False, header=False)", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Seed for reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Standardize inputs\nscaler = StandardScaler()\ng_train = scaler.fit_transform(g_train)\ng_val = scaler.transform(g_val)\ng_test = scaler.transform(g_test)\n\n# Convert to torch tensors\nX_train = torch.tensor(g_train, dtype=torch.float32)\ny_train = torch.tensor(s_train, dtype=torch.float32)\nX_val = torch.tensor(g_val, dtype=torch.float32)\ny_val = torch.tensor(s_val, dtype=torch.float32)\nX_test = torch.tensor(g_test, dtype=torch.float32)\n\n# DataLoader\ntrain_ds = TensorDataset(X_train, y_train)\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n\n# Residual MLP definition\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.BatchNorm1d(256), nn.ReLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# Setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = ResMLP(14, 2001).to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.MSELoss()\n\n# Training loop\nepochs = 50\nfor epoch in range(1, epochs + 1):\n    model.train()\n    total_loss = 0.0\n    for xb, yb in train_loader:\n        xb, yb = xb.to(device), yb.to(device)\n        preds = model(xb)\n        loss = criterion(preds, yb)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * xb.size(0)\n    avg_train_loss = total_loss / len(train_ds)\n    if epoch % 10 == 0 or epoch == 1:\n        print(f\"Epoch {epoch}/{epochs}, Train MSE: {avg_train_loss:.6f}\")\n\n# Validation evaluation\nmodel.eval()\nwith torch.no_grad():\n    val_preds = model(X_val.to(device)).cpu().numpy()\nval_mse = np.mean((val_preds - s_val) ** 2)\nprint(f\"Validation MSE: {val_mse:.6f}\")\n\n# Test predictions and save submission\nmodel.eval()\nwith torch.no_grad():\n    test_preds = model(X_test.to(device)).cpu().numpy()\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds).to_csv(\"./working/submission.csv\", index=False, header=False)\nprint(f\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.preprocessing import StandardScaler\n\n# Data paths\ntrain_g_path = \"./input/g_training.csv\"\ntrain_s_path = \"./input/s_training.csv\"\nval_g_path = \"./input/g_validation.csv\"\nval_s_path = \"./input/s_validation.csv\"\ntest_g_path = \"./input/test_g.csv\"\n\n# Load data\ng_train = pd.read_csv(train_g_path, header=None).values.astype(np.float32)\ns_train = pd.read_csv(train_s_path, header=None).values.astype(np.float32)\ng_val = pd.read_csv(val_g_path, header=None).values.astype(np.float32)\ns_val = pd.read_csv(val_s_path, header=None).values.astype(np.float32)\ng_test = pd.read_csv(test_g_path, header=None).values.astype(np.float32)\n\n# Standardize geometry inputs\nscaler = StandardScaler().fit(g_train)\ng_train_std = scaler.transform(g_train)\ng_val_std = scaler.transform(g_val)\ng_test_std = scaler.transform(g_test)\n\n# Create DataLoaders\nbatch_size = 64\ntrain_ds = TensorDataset(torch.from_numpy(g_train_std), torch.from_numpy(s_train))\nval_ds = TensorDataset(torch.from_numpy(g_val_std), torch.from_numpy(s_val))\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n\n\n# Model definition\nclass CNNReg(nn.Module):\n    def __init__(self):\n        super(CNNReg, self).__init__()\n        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, padding=1)\n        self.relu = nn.ReLU()\n        self.fc = nn.Linear(32 * 14, 2001)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)  # shape (batch,1,14)\n        x = self.relu(self.conv1(x))  # shape (batch,16,14)\n        x = self.relu(self.conv2(x))  # shape (batch,32,14)\n        x = x.view(x.size(0), -1)  # shape (batch,32*14)\n        return self.fc(x)  # shape (batch,2001)\n\n\n# Instantiate model, loss, optimizer\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = CNNReg().to(device)\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# Training loop\nbest_val_mse = float(\"inf\")\nfor epoch in range(1, 51):\n    model.train()\n    for xb, yb in train_loader:\n        xb, yb = xb.to(device), yb.to(device)\n        preds = model(xb)\n        loss = criterion(preds, yb)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    # Validation\n    model.eval()\n    with torch.no_grad():\n        val_losses = []\n        for xb, yb in val_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            preds = model(xb)\n            val_losses.append(criterion(preds, yb).item())\n        val_mse = np.mean(val_losses)\n    if val_mse < best_val_mse:\n        best_val_mse = val_mse\n    print(f\"Epoch {epoch:02d}: val MSE = {val_mse:.6f}\")\n\n# Final validation MSE\nprint(f\"Final validation MSE: {best_val_mse:.6f}\")\n\n# Predict on test set and save submission\nmodel.eval()\nwith torch.no_grad():\n    X_test = torch.from_numpy(g_test_std).to(device)\n    preds_test = model(X_test).cpu().numpy()\n# Save without header/index\nos.makedirs(\"working\", exist_ok=True)\npd.DataFrame(preds_test).to_csv(\"working/submission.csv\", index=False, header=False)", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.preprocessing import StandardScaler\n\n# Set seeds for reproducibility\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n# Load datasets\nX_train = pd.read_csv(\"./input/g_training.csv\", header=None).values.astype(np.float32)\ny_train = pd.read_csv(\"./input/s_training.csv\", header=None).values.astype(np.float32)\nX_val = pd.read_csv(\"./input/g_validation.csv\", header=None).values.astype(np.float32)\ny_val = pd.read_csv(\"./input/s_validation.csv\", header=None).values.astype(np.float32)\nX_test = pd.read_csv(\"./input/test_g.csv\", header=None).values.astype(np.float32)\n\n# Standardize features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)\n\n# Prepare DataLoaders\ntrain_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\nval_ds = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=64, shuffle=False)\n\n\n# Define the MLP model\nclass MLP(nn.Module):\n    def __init__(self, input_dim=14, output_dim=2001):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(32, output_dim),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Setup device, model, optimizer, and loss\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = MLP().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.MSELoss()\n\n# Training loop\nmodel.train()\nfor epoch in range(50):\n    total_loss = 0.0\n    for xb, yb in train_loader:\n        xb, yb = xb.to(device), yb.to(device)\n        optimizer.zero_grad()\n        preds = model(xb)\n        loss = criterion(preds, yb)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * xb.size(0)\n    total_loss /= len(train_loader.dataset)\n\n# Validation\nmodel.eval()\nval_preds, val_targets = [], []\nwith torch.no_grad():\n    for xb, yb in val_loader:\n        xb = xb.to(device)\n        preds = model(xb).cpu().numpy()\n        val_preds.append(preds)\n        val_targets.append(yb.numpy())\nval_preds = np.vstack(val_preds)\nval_targets = np.vstack(val_targets)\nval_mse = np.mean((val_preds - val_targets) ** 2)\nprint(f\"Validation MSE: {val_mse:.6f}\")\n\n# Generate test predictions and save submission\nwith torch.no_grad():\n    test_tensor = torch.from_numpy(X_test).to(device)\n    test_out = model(test_tensor).cpu().numpy()\npd.DataFrame(test_out).to_csv(\"./working/submission.csv\", header=False, index=False)", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# Paths\nTRAIN_G = \"./input/g_training.csv\"\nTRAIN_S = \"./input/s_training.csv\"\nVAL_G = \"./input/g_validation.csv\"\nVAL_S = \"./input/s_validation.csv\"\nTEST_G = \"./input/test_g.csv\"\nSUBMISSION_PATH = \"working/submission.csv\"\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load data\ng_train = pd.read_csv(TRAIN_G, header=None).values.astype(np.float32)\ns_train = pd.read_csv(TRAIN_S, header=None).values.astype(np.float32)\ng_val = pd.read_csv(VAL_G, header=None).values.astype(np.float32)\ns_val = pd.read_csv(VAL_S, header=None).values.astype(np.float32)\ng_test = pd.read_csv(TEST_G, header=None).values.astype(np.float32)\n\n# Standardize inputs\nmean = g_train.mean(axis=0, keepdims=True)\nstd = g_train.std(axis=0, keepdims=True) + 1e-6\ng_train_std = (g_train - mean) / std\ng_val_std = (g_val - mean) / std\ng_test_std = (g_test - mean) / std\n\n# Create datasets and loaders\ntrain_ds = TensorDataset(torch.from_numpy(g_train_std), torch.from_numpy(s_train))\nval_ds = TensorDataset(torch.from_numpy(g_val_std), torch.from_numpy(s_val))\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=2)\n\n\n# Define model\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(14, 512),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(64, 2001),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nmodel = MLP().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.MSELoss()\n\n# Training loop\nepochs = 40\nfor epoch in range(1, epochs + 1):\n    model.train()\n    for Xb, yb in train_loader:\n        Xb, yb = Xb.to(device), yb.to(device)\n        pred = model(Xb)\n        loss = criterion(pred, yb)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n# Validation\nmodel.eval()\nwith torch.no_grad():\n    Xv = torch.from_numpy(g_val_std).to(device)\n    pred_val = model(Xv).cpu().numpy()\nval_mse = np.mean((pred_val - s_val) ** 2)\nprint(f\"Validation MSE: {val_mse:.6f}\")\n\n# Test predictions\nwith torch.no_grad():\n    Xt = torch.from_numpy(g_test_std).to(device)\n    pred_test = model(Xt).cpu().numpy()\n\n# Save submission\nos.makedirs(\"working\", exist_ok=True)\npd.DataFrame(pred_test).to_csv(SUBMISSION_PATH, index=False, header=False)", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Seed for reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Standardize inputs\nscaler = StandardScaler()\ng_train = scaler.fit_transform(g_train)\ng_val = scaler.transform(g_val)\ng_test = scaler.transform(g_test)\n\n# Convert to torch tensors\nX_train = torch.tensor(g_train, dtype=torch.float32)\ny_train = torch.tensor(s_train, dtype=torch.float32)\nX_val = torch.tensor(g_val, dtype=torch.float32)\ny_val = torch.tensor(s_val, dtype=torch.float32)\nX_test = torch.tensor(g_test, dtype=torch.float32)\n\n# DataLoader\ntrain_ds = TensorDataset(X_train, y_train)\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n\n# Residual MLP definition\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.BatchNorm1d(256), nn.ReLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# Setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = ResMLP(14, 2001).to(device)\n\n# Use L2 regularization\noptimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\ncriterion = nn.MSELoss()\n\n# Training loop\nepochs = 50\nfor epoch in range(1, epochs + 1):\n    model.train()\n    total_loss = 0.0\n    for xb, yb in train_loader:\n        xb, yb = xb.to(device), yb.to(device)\n        preds = model(xb)\n        loss = criterion(preds, yb)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * xb.size(0)\n    avg_train_loss = total_loss / len(train_ds)\n    if epoch == 1 or epoch % 10 == 0:\n        print(f\"Epoch {epoch}/{epochs}, Train MSE: {avg_train_loss:.6f}\")\n\n# Validation evaluation\nmodel.eval()\nwith torch.no_grad():\n    val_preds = model(X_val.to(device)).cpu().numpy()\nval_mse = np.mean((val_preds - s_val) ** 2)\nprint(f\"Validation MSE: {val_mse:.6f}\")\n\n# Test predictions and save submission\nmodel.eval()\nwith torch.no_grad():\n    test_preds = model(X_test.to(device)).cpu().numpy()\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds).to_csv(\"./working/submission.csv\", index=False, header=False)\nprint(f\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Seed for reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Standardize inputs\nscaler = StandardScaler()\ng_train = scaler.fit_transform(g_train)\ng_val = scaler.transform(g_val)\ng_test = scaler.transform(g_test)\n\n# Convert to torch tensors\nX_train = torch.tensor(g_train, dtype=torch.float32)\ny_train = torch.tensor(s_train, dtype=torch.float32)\nX_val = torch.tensor(g_val, dtype=torch.float32)\ny_val = torch.tensor(s_val, dtype=torch.float32)\nX_test = torch.tensor(g_test, dtype=torch.float32)\n\n# DataLoader\ntrain_ds = TensorDataset(X_train, y_train)\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n\n# Residual MLP definition\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.BatchNorm1d(256), nn.ReLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# Setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = ResMLP(14, 2001).to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.MSELoss()\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode=\"min\", factor=0.5, patience=5, verbose=True\n)\n\n# Training loop with validation and scheduler\nepochs = 50\nbest_val_mse = float(\"inf\")\nbest_weights = None\nX_val_device = X_val.to(device)\ny_val_device = y_val.to(device)\n\nfor epoch in range(1, epochs + 1):\n    model.train()\n    total_loss = 0.0\n    for xb, yb in train_loader:\n        xb, yb = xb.to(device), yb.to(device)\n        preds = model(xb)\n        loss = criterion(preds, yb)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * xb.size(0)\n    avg_train_loss = total_loss / len(train_ds)\n\n    # Validation\n    model.eval()\n    with torch.no_grad():\n        val_preds = model(X_val_device)\n        val_loss = criterion(val_preds, y_val_device).item()\n    scheduler.step(val_loss)\n\n    # Save best model\n    if val_loss < best_val_mse:\n        best_val_mse = val_loss\n        best_weights = model.state_dict()\n\n    if epoch % 10 == 0 or epoch == 1:\n        print(\n            f\"Epoch {epoch}/{epochs} - Train MSE: {avg_train_loss:.6f}, Val MSE: {val_loss:.6f}\"\n        )\n\n# Load best weights\nmodel.load_state_dict(best_weights)\n\n# Final validation MSE\nmodel.eval()\nwith torch.no_grad():\n    final_val_preds = model(X_val_device).cpu().numpy()\nfinal_mse = np.mean((final_val_preds - s_val) ** 2)\nprint(f\"Best Validation MSE: {final_mse:.6f}\")\n\n# Test predictions and save submission\nwith torch.no_grad():\n    test_preds = model(X_test.to(device)).cpu().numpy()\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds).to_csv(\"./working/submission.csv\", index=False, header=False)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Seed for reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Standardize inputs\nscaler = StandardScaler()\ng_train = scaler.fit_transform(g_train)\ng_val = scaler.transform(g_val)\ng_test = scaler.transform(g_test)\n\n# Convert to torch tensors\nX_train = torch.tensor(g_train, dtype=torch.float32)\ny_train = torch.tensor(s_train, dtype=torch.float32)\nX_val = torch.tensor(g_val, dtype=torch.float32)\ny_val = torch.tensor(s_val, dtype=torch.float32)\nX_test = torch.tensor(g_test, dtype=torch.float32)\n\n# DataLoader\ntrain_ds = TensorDataset(X_train, y_train)\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n\n# Residual MLP definition\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.BatchNorm1d(256), nn.ReLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# Setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = ResMLP(14, 2001).to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.MSELoss()\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode=\"min\", factor=0.5, patience=5, verbose=True\n)\n\nbest_val_mse = float(\"inf\")\nbest_state = None\nepochs = 50\n\nfor epoch in range(1, epochs + 1):\n    # Training\n    model.train()\n    train_loss = 0.0\n    for xb, yb in train_loader:\n        xb, yb = xb.to(device), yb.to(device)\n        preds = model(xb)\n        loss = criterion(preds, yb)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * xb.size(0)\n    avg_train_loss = train_loss / len(train_ds)\n\n    # Validation\n    model.eval()\n    with torch.no_grad():\n        val_preds = model(X_val.to(device)).cpu().numpy()\n    val_mse = np.mean((val_preds - s_val) ** 2)\n\n    # Step scheduler\n    scheduler.step(val_mse)\n\n    # Track best model\n    if val_mse < best_val_mse:\n        best_val_mse = val_mse\n        best_state = model.state_dict()\n\n    if epoch % 10 == 0 or epoch == 1:\n        print(\n            f\"Epoch {epoch}/{epochs}, Train MSE: {avg_train_loss:.6f}, Val MSE: {val_mse:.6f}\"\n        )\n\n# Load best model weights\nmodel.load_state_dict(best_state)\n\n# Final validation evaluation\nmodel.eval()\nwith torch.no_grad():\n    final_val_preds = model(X_val.to(device)).cpu().numpy()\nfinal_val_mse = np.mean((final_val_preds - s_val) ** 2)\nprint(f\"Final Validation MSE: {final_val_mse:.6f}\")\n\n# Test predictions and save submission\nwith torch.no_grad():\n    test_preds = model(X_test.to(device)).cpu().numpy()\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds).to_csv(\"./working/submission.csv\", index=False, header=False)\nprint(f\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Seed for reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Standardize inputs\nscaler = StandardScaler()\ng_train = scaler.fit_transform(g_train)\ng_val = scaler.transform(g_val)\ng_test = scaler.transform(g_test)\n\n# Convert to torch tensors\nX_train = torch.tensor(g_train, dtype=torch.float32)\ny_train = torch.tensor(s_train, dtype=torch.float32)\nX_val = torch.tensor(g_val, dtype=torch.float32)\ny_val = torch.tensor(s_val, dtype=torch.float32)\nX_test = torch.tensor(g_test, dtype=torch.float32)\n\n# DataLoader\ntrain_ds = TensorDataset(X_train, y_train)\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n\n# Residual MLP definition\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.BatchNorm1d(256), nn.ReLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# Setup device, model, optimizer with weight decay, and loss\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = ResMLP(14, 2001).to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\ncriterion = nn.MSELoss()\n\n# Training loop\nepochs = 50\nfor epoch in range(1, epochs + 1):\n    model.train()\n    total_loss = 0.0\n    for xb, yb in train_loader:\n        xb, yb = xb.to(device), yb.to(device)\n        preds = model(xb)\n        loss = criterion(preds, yb)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * xb.size(0)\n    avg_train_loss = total_loss / len(train_ds)\n    if epoch % 10 == 0 or epoch == 1:\n        print(f\"Epoch {epoch}/{epochs}, Train MSE: {avg_train_loss:.6f}\")\n\n# Validation evaluation\nmodel.eval()\nwith torch.no_grad():\n    val_preds = model(X_val.to(device)).cpu().numpy()\nval_mse = np.mean((val_preds - s_val) ** 2)\nprint(f\"Validation MSE: {val_mse:.6f}\")\n\n# Test predictions and save submission\nmodel.eval()\nwith torch.no_grad():\n    test_preds = model(X_test.to(device)).cpu().numpy()\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds).to_csv(\"./working/submission.csv\", index=False, header=False)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Seed for reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Standardize inputs\nscaler = StandardScaler()\ng_train = scaler.fit_transform(g_train)\ng_val = scaler.transform(g_val)\ng_test = scaler.transform(g_test)\n\n# Convert to torch tensors\nX_train = torch.tensor(g_train, dtype=torch.float32)\ny_train = torch.tensor(s_train, dtype=torch.float32)\nX_val = torch.tensor(g_val, dtype=torch.float32)\ny_val = torch.tensor(s_val, dtype=torch.float32)\nX_test = torch.tensor(g_test, dtype=torch.float32)\n\n# DataLoader\nbatch_size = 128\ntrain_ds = TensorDataset(X_train, y_train)\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n\n\n# Residual MLP definition\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.BatchNorm1d(256), nn.ReLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# Setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = ResMLP(14, 2001).to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n# OneCycleLR scheduler: steps per epoch and total epochs\nepochs = 50\nscheduler = optim.lr_scheduler.OneCycleLR(\n    optimizer, max_lr=1e-3, steps_per_epoch=len(train_loader), epochs=epochs\n)\ncriterion = nn.MSELoss()\n\n# Training loop with OneCycleLR\nfor epoch in range(1, epochs + 1):\n    model.train()\n    total_loss = 0.0\n    for xb, yb in train_loader:\n        xb, yb = xb.to(device), yb.to(device)\n        preds = model(xb)\n        loss = criterion(preds, yb)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        total_loss += loss.item() * xb.size(0)\n    avg_train_loss = total_loss / len(train_ds)\n    if epoch % 10 == 0 or epoch == 1:\n        print(f\"Epoch {epoch}/{epochs}, Train MSE: {avg_train_loss:.6f}\")\n\n# Validation evaluation\nmodel.eval()\nwith torch.no_grad():\n    val_preds = model(X_val.to(device)).cpu().numpy()\nval_mse = np.mean((val_preds - s_val) ** 2)\nprint(f\"Validation MSE: {val_mse:.6f}\")\n\n# Test predictions and save submission\nmodel.eval()\nwith torch.no_grad():\n    test_preds = model(X_test.to(device)).cpu().numpy()\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds).to_csv(\"./working/submission.csv\", index=False, header=False)\nprint(f\"Saved test predictions to ./working/submission.csv\")", "import os\nimport copy\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Seed for reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Standardize inputs\nscaler = StandardScaler()\ng_train = scaler.fit_transform(g_train)\ng_val = scaler.transform(g_val)\ng_test = scaler.transform(g_test)\n\n# Convert to torch tensors\nX_train = torch.tensor(g_train, dtype=torch.float32)\ny_train = torch.tensor(s_train, dtype=torch.float32)\nX_val = torch.tensor(g_val, dtype=torch.float32)\ny_val = torch.tensor(s_val, dtype=torch.float32)\nX_test = torch.tensor(g_test, dtype=torch.float32)\n\n# DataLoader\ntrain_ds = TensorDataset(X_train, y_train)\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n\n# Residual MLP definition\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.BatchNorm1d(256), nn.ReLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# Setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = ResMLP(14, 2001).to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.MSELoss()\n\n# Training with validation-based checkpointing\nepochs = 50\nbest_val_mse = float(\"inf\")\nbest_model_wts = copy.deepcopy(model.state_dict())\n\nfor epoch in range(1, epochs + 1):\n    model.train()\n    total_loss = 0.0\n    for xb, yb in train_loader:\n        xb, yb = xb.to(device), yb.to(device)\n        optimizer.zero_grad()\n        preds = model(xb)\n        loss = criterion(preds, yb)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * xb.size(0)\n    avg_train_loss = total_loss / len(train_ds)\n\n    # Validation\n    model.eval()\n    with torch.no_grad():\n        val_preds = model(X_val.to(device)).cpu().numpy()\n    val_mse = np.mean((val_preds - s_val) ** 2)\n\n    # Checkpoint\n    if val_mse < best_val_mse:\n        best_val_mse = val_mse\n        best_model_wts = copy.deepcopy(model.state_dict())\n\n    if epoch % 10 == 0 or epoch == 1:\n        print(\n            f\"Epoch {epoch}/{epochs}, Train MSE: {avg_train_loss:.6f}, Val MSE: {val_mse:.6f}\"\n        )\n\n# Load best model weights\nmodel.load_state_dict(best_model_wts)\n\n# Final validation evaluation\nmodel.eval()\nwith torch.no_grad():\n    val_preds = model(X_val.to(device)).cpu().numpy()\nfinal_val_mse = np.mean((val_preds - s_val) ** 2)\nprint(f\"Best Validation MSE: {final_val_mse:.6f}\")\n\n# Test predictions and save submission\nwith torch.no_grad():\n    test_preds = model(X_test.to(device)).cpu().numpy()\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds).to_csv(\"./working/submission.csv\", index=False, header=False)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport copy\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Seed for reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Standardize inputs\nscaler = StandardScaler()\ng_train = scaler.fit_transform(g_train)\ng_val = scaler.transform(g_val)\ng_test = scaler.transform(g_test)\n\n# Convert to torch tensors\nX_train = torch.tensor(g_train, dtype=torch.float32)\ny_train = torch.tensor(s_train, dtype=torch.float32)\nX_val = torch.tensor(g_val, dtype=torch.float32)\ny_val = torch.tensor(s_val, dtype=torch.float32)\nX_test = torch.tensor(g_test, dtype=torch.float32)\n\n# DataLoader\ntrain_ds = TensorDataset(X_train, y_train)\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n\n# Residual MLP with SiLU activations\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        act = nn.SiLU\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.BatchNorm1d(256), act())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.BatchNorm1d(512), act(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.BatchNorm1d(256), act())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# Setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = ResMLP(14, 2001).to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.MSELoss()\n\n# Training with validation-based checkpointing\nepochs = 50\nbest_val_mse = float(\"inf\")\nbest_model_wts = copy.deepcopy(model.state_dict())\n\nfor epoch in range(1, epochs + 1):\n    model.train()\n    total_loss = 0.0\n    for xb, yb in train_loader:\n        xb, yb = xb.to(device), yb.to(device)\n        optimizer.zero_grad()\n        preds = model(xb)\n        loss = criterion(preds, yb)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * xb.size(0)\n    avg_train_loss = total_loss / len(train_ds)\n\n    # Validation\n    model.eval()\n    with torch.no_grad():\n        val_preds = model(X_val.to(device)).cpu().numpy()\n    val_mse = np.mean((val_preds - s_val) ** 2)\n\n    # Checkpoint\n    if val_mse < best_val_mse:\n        best_val_mse = val_mse\n        best_model_wts = copy.deepcopy(model.state_dict())\n\n    if epoch % 10 == 0 or epoch == 1:\n        print(\n            f\"Epoch {epoch}/{epochs}, Train MSE: {avg_train_loss:.6f}, Val MSE: {val_mse:.6f}\"\n        )\n\n# Load best model weights\nmodel.load_state_dict(best_model_wts)\n\n# Final validation evaluation\nmodel.eval()\nwith torch.no_grad():\n    val_preds = model(X_val.to(device)).cpu().numpy()\nfinal_val_mse = np.mean((val_preds - s_val) ** 2)\nprint(f\"Best Validation MSE: {final_val_mse:.6f}\")\n\n# Test predictions and save submission\nwith torch.no_grad():\n    test_preds = model(X_test.to(device)).cpu().numpy()\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds).to_csv(\"./working/submission.csv\", index=False, header=False)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport copy\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\nn_samples, input_dim = X_all.shape\noutput_dim = y_all.shape[1]\nn_splits = 5\n\n# Prepare storage for out-of-fold and test predictions\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], output_dim), dtype=np.float32)\n\n\n# Define model\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.BatchNorm1d(256), nn.ReLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\nfold_mses = []\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all)):\n    # Split data\n    X_tr, y_tr = X_all[train_idx], y_all[train_idx]\n    X_va, y_va = X_all[val_idx], y_all[val_idx]\n    # Scale per-fold\n    scaler = StandardScaler().fit(X_tr)\n    X_tr_s = scaler.transform(X_tr)\n    X_va_s = scaler.transform(X_va)\n    X_test_s = scaler.transform(g_test)\n    # To tensors\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    y_va_np = y_va  # keep numpy for MSE\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32).to(device)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model, optimizer, loss\n    model = ResMLP(input_dim, output_dim).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n    criterion = nn.MSELoss()\n\n    best_val_mse = float(\"inf\")\n    best_w = None\n\n    # Train\n    for epoch in range(1, 51):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            preds = model(xb)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n        # Validate\n        model.eval()\n        with torch.no_grad():\n            val_preds = model(X_va_t).cpu().numpy()\n        val_mse = np.mean((val_preds - y_va_np) ** 2)\n        if val_mse < best_val_mse:\n            best_val_mse = val_mse\n            best_w = copy.deepcopy(model.state_dict())\n    # Load best and predict\n    model.load_state_dict(best_w)\n    model.eval()\n    with torch.no_grad():\n        # OOF\n        oof_preds[val_idx] = model(X_va_t).cpu().numpy()\n        # Test\n        test_preds += model(X_test_t).cpu().numpy()\n    fold_mses.append(best_val_mse)\n    print(f\"Fold {fold+1} Best Val MSE: {best_val_mse:.6f}\")\n\n# Compute CV MSE and save submission\ncv_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"CV MSE: {cv_mse:.6f}\")\n\n# Average test predictions\ntest_preds /= n_splits\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds).to_csv(\"./working/submission.csv\", index=False, header=False)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport copy\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Seed for reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Standardize inputs\nscaler = StandardScaler()\ng_train = scaler.fit_transform(g_train)\ng_val = scaler.transform(g_val)\ng_test = scaler.transform(g_test)\n\n# Convert to torch tensors\nX_train = torch.tensor(g_train, dtype=torch.float32)\ny_train = torch.tensor(s_train, dtype=torch.float32)\nX_val = torch.tensor(g_val, dtype=torch.float32)\ny_val = torch.tensor(s_val, dtype=torch.float32)\nX_test = torch.tensor(g_test, dtype=torch.float32)\n\n# DataLoader\ntrain_ds = TensorDataset(X_train, y_train)\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n\n# Residual MLP definition\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.BatchNorm1d(256), nn.ReLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# Setup device, model, optimizer, criterion, scheduler\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = ResMLP(14, 2001).to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-5)\ncriterion = nn.MSELoss()\n\n# Training with validation-based checkpointing\nepochs = 50\nbest_val_mse = float(\"inf\")\nbest_model_wts = copy.deepcopy(model.state_dict())\n\nfor epoch in range(1, epochs + 1):\n    model.train()\n    running_loss = 0.0\n    for xb, yb in train_loader:\n        xb, yb = xb.to(device), yb.to(device)\n        optimizer.zero_grad()\n        preds = model(xb)\n        loss = criterion(preds, yb)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * xb.size(0)\n    avg_train_loss = running_loss / len(train_ds)\n\n    # Validation\n    model.eval()\n    with torch.no_grad():\n        val_preds = model(X_val.to(device)).cpu().numpy()\n    val_mse = np.mean((val_preds - s_val) ** 2)\n\n    # Checkpoint\n    if val_mse < best_val_mse:\n        best_val_mse = val_mse\n        best_model_wts = copy.deepcopy(model.state_dict())\n\n    # Step scheduler\n    scheduler.step()\n\n    if epoch == 1 or epoch % 10 == 0:\n        lr = scheduler.get_last_lr()[0]\n        print(\n            f\"Epoch {epoch}/{epochs}, LR: {lr:.6f}, Train MSE: {avg_train_loss:.6f}, Val MSE: {val_mse:.6f}\"\n        )\n\n# Load best model weights\nmodel.load_state_dict(best_model_wts)\n\n# Final validation evaluation\nmodel.eval()\nwith torch.no_grad():\n    val_preds = model(X_val.to(device)).cpu().numpy()\nfinal_val_mse = np.mean((val_preds - s_val) ** 2)\nprint(f\"Best Validation MSE: {final_val_mse:.6f}\")\n\n# Test predictions and save submission\nwith torch.no_grad():\n    test_preds = model(X_test.to(device)).cpu().numpy()\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds).to_csv(\"./working/submission.csv\", index=False, header=False)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport copy\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Seed for reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Standardize inputs\nscaler = StandardScaler()\ng_train = scaler.fit_transform(g_train)\ng_val = scaler.transform(g_val)\ng_test = scaler.transform(g_test)\n\n# Convert to torch tensors\nX_train = torch.tensor(g_train, dtype=torch.float32)\ny_train = torch.tensor(s_train, dtype=torch.float32)\nX_val = torch.tensor(g_val, dtype=torch.float32)\ny_val = torch.tensor(s_val, dtype=torch.float32)\nX_test = torch.tensor(g_test, dtype=torch.float32)\n\n# DataLoader\ntrain_ds = TensorDataset(X_train, y_train)\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n\n# Residual MLP with LayerNorm\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.ReLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.ReLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.ReLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# Setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = ResMLP(14, 2001).to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.MSELoss()\n\n# Training with validation-based checkpointing\nepochs = 50\nbest_val_mse = float(\"inf\")\nbest_model_wts = copy.deepcopy(model.state_dict())\n\nfor epoch in range(1, epochs + 1):\n    model.train()\n    total_loss = 0.0\n    for xb, yb in train_loader:\n        xb, yb = xb.to(device), yb.to(device)\n        optimizer.zero_grad()\n        preds = model(xb)\n        loss = criterion(preds, yb)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * xb.size(0)\n    avg_train_loss = total_loss / len(train_ds)\n\n    model.eval()\n    with torch.no_grad():\n        val_preds = model(X_val.to(device)).cpu().numpy()\n    val_mse = np.mean((val_preds - s_val) ** 2)\n\n    if val_mse < best_val_mse:\n        best_val_mse = val_mse\n        best_model_wts = copy.deepcopy(model.state_dict())\n\n    if epoch == 1 or epoch % 10 == 0:\n        print(\n            f\"Epoch {epoch}/{epochs}, Train MSE: {avg_train_loss:.6f}, Val MSE: {val_mse:.6f}\"\n        )\n\n# Load best model weights\nmodel.load_state_dict(best_model_wts)\n\n# Final validation evaluation\nmodel.eval()\nwith torch.no_grad():\n    val_preds = model(X_val.to(device)).cpu().numpy()\nfinal_val_mse = np.mean((val_preds - s_val) ** 2)\nprint(f\"Best Validation MSE: {final_val_mse:.6f}\")\n\n# Test predictions and save submission\nwith torch.no_grad():\n    test_preds = model(X_test.to(device)).cpu().numpy()\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds).to_csv(\"./working/submission.csv\", index=False, header=False)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport copy\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Seed for reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+validation for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Define model\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.ReLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.ReLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.ReLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# CV setup\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfold = 0\nfor train_idx, val_idx in kf.split(X_all):\n    fold += 1\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n    # scaler per fold\n    scaler = StandardScaler()\n    X_tr_s = scaler.fit_transform(X_tr)\n    X_va_s = scaler.transform(X_va)\n    X_test_s = scaler.transform(g_test)\n    # tensors\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    y_va_t = torch.tensor(y_va, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n    # DataLoader\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n    # model, optimizer, loss\n    model = ResMLP(14, y_all.shape[1]).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n    criterion = nn.MSELoss()\n    # Train with checkpointing\n    best_val_mse = float(\"inf\")\n    best_w = copy.deepcopy(model.state_dict())\n    epochs = 50\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            preds = model(xb)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n        # validation\n        model.eval()\n        with torch.no_grad():\n            val_pred = model(X_va_t.to(device)).cpu().numpy()\n        val_mse = np.mean((val_pred - y_va) ** 2)\n        if val_mse < best_val_mse:\n            best_val_mse = val_mse\n            best_w = copy.deepcopy(model.state_dict())\n    model.load_state_dict(best_w)\n    # store OOF preds\n    model.eval()\n    with torch.no_grad():\n        oof_fold = model(X_va_t.to(device)).cpu().numpy()\n        test_fold = model(X_test_t.to(device)).cpu().numpy()\n    oof_preds[val_idx] = oof_fold\n    test_preds_sum += test_fold\n    print(f\"Fold {fold} Val MSE: {best_val_mse:.6f}\")\n\n# Compute overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Average test predictions and save\ntest_preds_avg = test_preds_sum / n_splits\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport copy\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Seed for reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+validation for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Define model\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.ReLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.ReLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.ReLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# CV setup\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfold = 0\nfor train_idx, val_idx in kf.split(X_all):\n    fold += 1\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n    # scaler per fold\n    scaler = StandardScaler()\n    X_tr_s = scaler.fit_transform(X_tr)\n    X_va_s = scaler.transform(X_va)\n    X_test_s = scaler.transform(g_test)\n    # tensors\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    y_va_t = torch.tensor(y_va, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n    # DataLoader\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n    # model, optimizer, loss\n    model = ResMLP(14, y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    # Train with checkpointing\n    best_val_mse = float(\"inf\")\n    best_w = copy.deepcopy(model.state_dict())\n    epochs = 50\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            preds = model(xb)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n        # validation\n        model.eval()\n        with torch.no_grad():\n            val_pred = model(X_va_t.to(device)).cpu().numpy()\n        val_mse = np.mean((val_pred - y_va) ** 2)\n        if val_mse < best_val_mse:\n            best_val_mse = val_mse\n            best_w = copy.deepcopy(model.state_dict())\n    model.load_state_dict(best_w)\n    # store OOF preds and test preds\n    model.eval()\n    with torch.no_grad():\n        oof_fold = model(X_va_t.to(device)).cpu().numpy()\n        test_fold = model(X_test_t.to(device)).cpu().numpy()\n    oof_preds[val_idx] = oof_fold\n    test_preds_sum += test_fold\n    print(f\"Fold {fold} Val MSE: {best_val_mse:.6f}\")\n\n# Compute overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Average test predictions and save\ntest_preds_avg = test_preds_sum / n_splits\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport copy\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Seed for reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+validation for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Define model\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.ReLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.ReLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.ReLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = x2\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# CV setup\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfold = 0\nfor train_idx, val_idx in kf.split(X_all):\n    fold += 1\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n    # Standardize inputs\n    scaler_x = StandardScaler()\n    X_tr_s = scaler_x.fit_transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_test_s = scaler_x.transform(g_test)\n    # Standardize targets\n    scaler_y = StandardScaler()\n    y_tr_s = scaler_y.fit_transform(y_tr)\n    y_va_s = scaler_y.transform(y_va)\n    # Tensors\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    y_va_t = torch.tensor(y_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n    # DataLoader\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n    # Model, optimizer, loss\n    model = ResMLP(14, y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    # Train with checkpointing\n    best_val_mse = float(\"inf\")\n    best_w = copy.deepcopy(model.state_dict())\n    epochs = 50\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            preds = model(xb)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n        # validation\n        model.eval()\n        with torch.no_grad():\n            val_pred_s = model(X_va_t.to(device)).cpu().numpy()\n        # inverse transform to original scale\n        val_pred = scaler_y.inverse_transform(val_pred_s)\n        val_mse = np.mean((val_pred - y_va) ** 2)\n        if val_mse < best_val_mse:\n            best_val_mse = val_mse\n            best_w = copy.deepcopy(model.state_dict())\n    model.load_state_dict(best_w)\n    # store OOF preds and test preds\n    model.eval()\n    with torch.no_grad():\n        oof_s = model(X_va_t.to(device)).cpu().numpy()\n        test_s = model(X_test_t.to(device)).cpu().numpy()\n    oof_fold = scaler_y.inverse_transform(oof_s)\n    test_fold = scaler_y.inverse_transform(test_s)\n    oof_preds[val_idx] = oof_fold\n    test_preds_sum += test_fold\n    print(f\"Fold {fold} Val MSE: {best_val_mse:.6f}\")\n\n# Compute overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Average test predictions and save\ntest_preds_avg = test_preds_sum / n_splits\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport copy\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.ReLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.ReLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.ReLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n    # Standardize X\n    scaler_x = StandardScaler()\n    X_tr_s = scaler_x.fit_transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_test_s = scaler_x.transform(g_test)\n    # Standardize y\n    scaler_y = StandardScaler()\n    y_tr_s = scaler_y.fit_transform(y_tr)\n    y_va_s = scaler_y.transform(y_va)\n    # Tensors and loaders\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n    # Model, optimizer, scheduler, loss\n    model = ResMLP(14, y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    scheduler = OneCycleLR(\n        optimizer, max_lr=1e-3, epochs=50, steps_per_epoch=len(train_loader)\n    )\n    criterion = nn.MSELoss()\n    best_val_mse = float(\"inf\")\n    best_w = copy.deepcopy(model.state_dict())\n    # Training loop\n    for epoch in range(50):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            preds = model(xb)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n        # Validation\n        model.eval()\n        with torch.no_grad():\n            val_s = model(X_va_t.to(device)).cpu().numpy()\n        val = scaler_y.inverse_transform(val_s)\n        val_mse = np.mean((val - y_va) ** 2)\n        if val_mse < best_val_mse:\n            best_val_mse = val_mse\n            best_w = copy.deepcopy(model.state_dict())\n    model.load_state_dict(best_w)\n    # OOF and test preds\n    model.eval()\n    with torch.no_grad():\n        oof_s = model(X_va_t.to(device)).cpu().numpy()\n        test_s = model(X_test_t.to(device)).cpu().numpy()\n    oof_preds[val_idx] = scaler_y.inverse_transform(oof_s)\n    test_preds_sum += scaler_y.inverse_transform(test_s)\n    print(f\"Fold {fold} Val MSE: {best_val_mse:.6f}\")\n\n# Overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\ntest_avg = test_preds_sum / n_splits\npd.DataFrame(test_avg).to_csv(\"./working/submission.csv\", index=False, header=False)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport copy\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Seed for reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+validation for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Define model\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.ReLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.ReLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.ReLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# CV setup\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfold = 0\nfor train_idx, val_idx in kf.split(X_all):\n    fold += 1\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n    # Standardize inputs\n    scaler_x = StandardScaler()\n    X_tr_s = scaler_x.fit_transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_test_s = scaler_x.transform(g_test)\n    # Standardize targets\n    scaler_y = StandardScaler()\n    y_tr_s = scaler_y.fit_transform(y_tr)\n    # Tensors\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n    # DataLoader\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n    # Model, optimizer, loss\n    model = ResMLP(14, y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    # SWA setup\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    epochs = 50\n    # Training loop with SWA\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            preds = model(xb)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n        # SWA update\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n    # Update BN for SWA model\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    # Validation predictions\n    swa_model.eval()\n    with torch.no_grad():\n        val_pred_s = swa_model(X_va_t.to(device)).cpu().numpy()\n    val_pred = scaler_y.inverse_transform(val_pred_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (SWA): {val_mse:.6f}\")\n    # OOF and test preds\n    oof_preds[val_idx] = val_pred\n    with torch.no_grad():\n        test_s = swa_model(X_test_t.to(device)).cpu().numpy()\n    test_fold = scaler_y.inverse_transform(test_s)\n    test_preds_sum += test_fold\n\n# Compute overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (SWA): {oof_mse:.6f}\")\n\n# Average test predictions and save\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Seed for reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+validation for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Define model\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.ReLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.ReLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.ReLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# CV setup\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfold = 0\nfor train_idx, val_idx in kf.split(X_all):\n    fold += 1\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n    # Standardize inputs\n    scaler_x = StandardScaler()\n    X_tr_s = scaler_x.fit_transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_test_s = scaler_x.transform(g_test)\n    # Standardize targets\n    scaler_y = StandardScaler()\n    y_tr_s = scaler_y.fit_transform(y_tr)\n    # Tensors\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n    # DataLoader\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n    # Model, optimizer, loss\n    model = ResMLP(14, y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    # SWA setup\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    epochs = 50\n    # Training loop with gradient clipping and SWA\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            preds = model(xb)\n            loss = criterion(preds, yb)\n            loss.backward()\n            # Gradient clipping to stabilize training\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        # SWA update and scheduler step\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n    # Finalize SWA model\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    # Validation\n    swa_model.eval()\n    with torch.no_grad():\n        val_pred_s = swa_model(X_va_t.to(device)).cpu().numpy()\n    val_pred = scaler_y.inverse_transform(val_pred_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (SWA+Clipping): {val_mse:.6f}\")\n    oof_preds[val_idx] = val_pred\n    # Test predictions\n    with torch.no_grad():\n        test_s = swa_model(X_test_t.to(device)).cpu().numpy()\n    test_fold = scaler_y.inverse_transform(test_s)\n    test_preds_sum += test_fold\n\n# Compute overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (SWA+Clipping): {oof_mse:.6f}\")\n\n# Save test predictions\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport copy\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Seed\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Mish activation\nclass Mish(nn.Module):\n    def forward(self, x):\n        return x * torch.tanh(F.softplus(x))\n\n\n# Model with Mish\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), Mish())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), Mish(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), Mish())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# Cross-validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n    # Scale inputs\n    scaler_x = StandardScaler()\n    X_tr_s = scaler_x.fit_transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_test_s = scaler_x.transform(g_test)\n    # Scale targets\n    scaler_y = StandardScaler()\n    y_tr_s = scaler_y.fit_transform(y_tr)\n    # Tensors\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n    # DataLoader\n    train_loader = DataLoader(\n        TensorDataset(X_tr_t, y_tr_t), batch_size=128, shuffle=True, num_workers=2\n    )\n    # Model & optimizer\n    model = ResMLP(14, y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    # SWA\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    # Train\n    for epoch in range(50):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            pred = model(xb)\n            loss = criterion(pred, yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n    # Update BN\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    # Validation\n    swa_model.eval()\n    with torch.no_grad():\n        val_pred_s = swa_model(X_va_t.to(device)).cpu().numpy()\n    val_pred = scaler_y.inverse_transform(val_pred_s)\n    mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[val_idx] = val_pred\n    # Test\n    with torch.no_grad():\n        test_s = swa_model(X_test_t.to(device)).cpu().numpy()\n    test_fold = scaler_y.inverse_transform(test_s)\n    test_preds_sum += test_fold\n\n# Overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\n\n# Seed for reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+validation for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Define model\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.ReLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.ReLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.ReLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# Cross-validation setup\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), start=1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n    # Standardize inputs\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_test_s = scaler_x.transform(g_test)\n    # Standardize targets\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n    # Tensors\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n    # DataLoader\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n    # Model, optimizer, loss\n    model = ResMLP(14, y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    # SWA setup\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    epochs = 50\n    # Training loop with gradient clipping and SWA\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            preds = model(xb)\n            loss = criterion(preds, yb)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n    # Finalize SWA model\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    # Validation predictions\n    swa_model.eval()\n    with torch.no_grad():\n        val_pred_s = swa_model(X_va_t.to(device)).cpu().numpy()\n    val_pred = scaler_y.inverse_transform(val_pred_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (SWA + clipping): {val_mse:.6f}\")\n    # Store OOF and test predictions\n    oof_preds[val_idx] = val_pred\n    with torch.no_grad():\n        test_s = swa_model(X_test_t.to(device)).cpu().numpy()\n    test_fold = scaler_y.inverse_transform(test_s)\n    test_preds_sum += test_fold\n\n# Overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (SWA + clipping): {oof_mse:.6f}\")\n\n# Save test predictions\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Seed for reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+validation for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Define model\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.ReLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.ReLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.ReLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# CV setup\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfold = 0\nfor train_idx, val_idx in kf.split(X_all):\n    fold += 1\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n    # Standardize inputs\n    scaler_x = StandardScaler()\n    X_tr_s = scaler_x.fit_transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_test_s = scaler_x.transform(g_test)\n    # Standardize targets\n    scaler_y = StandardScaler()\n    y_tr_s = scaler_y.fit_transform(y_tr)\n    # Tensors\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n    # DataLoader\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n    # Model, optimizer, loss\n    model = ResMLP(14, y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    # SWA setup\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    epochs = 50\n    # Training loop with gradient clipping and SWA\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            preds = model(xb)\n            loss = criterion(preds, yb)\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n    # Update BN for SWA model\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    # Validation predictions\n    swa_model.eval()\n    with torch.no_grad():\n        val_pred_s = swa_model(X_va_t.to(device)).cpu().numpy()\n    val_pred = scaler_y.inverse_transform(val_pred_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (SWA + Clipping): {val_mse:.6f}\")\n    oof_preds[val_idx] = val_pred\n    # Test predictions\n    with torch.no_grad():\n        test_s = swa_model(X_test_t.to(device)).cpu().numpy()\n    test_fold = scaler_y.inverse_transform(test_s)\n    test_preds_sum += test_fold\n\n# Compute overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (SWA + Clipping): {oof_mse:.6f}\")\n\n# Average test predictions and save\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport copy\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Seed for reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+validation for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Define model\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.ReLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.ReLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.ReLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# CV setup\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfold = 0\nfor train_idx, val_idx in kf.split(X_all):\n    fold += 1\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Polynomial feature expansion (degree 2)\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    X_tr_p = poly.fit_transform(X_tr)\n    X_va_p = poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n\n    # Standardize inputs\n    scaler_x = StandardScaler()\n    X_tr_s = scaler_x.fit_transform(X_tr_p)\n    X_va_s = scaler_x.transform(X_va_p)\n    X_test_s = scaler_x.transform(X_test_p)\n\n    # Standardize targets\n    scaler_y = StandardScaler()\n    y_tr_s = scaler_y.fit_transform(y_tr)\n\n    # Tensors\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n\n    # DataLoader\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model, optimizer, loss\n    model = ResMLP(X_tr_s.shape[1], y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n\n    # SWA setup\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    epochs = 50\n\n    # Training loop with SWA\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            preds = model(xb)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Update BN for SWA model\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n\n    # Validation predictions\n    swa_model.eval()\n    with torch.no_grad():\n        val_pred_s = swa_model(X_va_t.to(device)).cpu().numpy()\n    val_pred = scaler_y.inverse_transform(val_pred_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (SWA+Poly): {val_mse:.6f}\")\n\n    # OOF and test preds\n    oof_preds[val_idx] = val_pred\n    with torch.no_grad():\n        test_s = swa_model(X_test_t.to(device)).cpu().numpy()\n    test_fold = scaler_y.inverse_transform(test_s)\n    test_preds_sum += test_fold\n\n# Compute overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (SWA+Poly): {oof_mse:.6f}\")\n\n# Average test predictions and save\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Seed for reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+validation for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Define model\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.ReLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.ReLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.ReLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# CV setup\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\n# Noise std for augmentation\nnoise_std = 0.01\n\nfold = 0\nfor train_idx, val_idx in kf.split(X_all):\n    fold += 1\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Polynomial feature expansion (degree 2)\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    X_tr_p = poly.fit_transform(X_tr)\n    X_va_p = poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n\n    # Standardize inputs\n    scaler_x = StandardScaler()\n    X_tr_s = scaler_x.fit_transform(X_tr_p)\n    X_va_s = scaler_x.transform(X_va_p)\n    X_test_s = scaler_x.transform(X_test_p)\n\n    # Standardize targets\n    scaler_y = StandardScaler()\n    y_tr_s = scaler_y.fit_transform(y_tr)\n\n    # Tensors\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n\n    # DataLoader\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model, optimizer, loss\n    model = ResMLP(X_tr_s.shape[1], y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n\n    # SWA setup\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    epochs = 50\n\n    # Training loop with input noise augmentation and SWA\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb = xb + torch.randn_like(xb) * noise_std\n            optimizer.zero_grad()\n            preds = model(xb)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Update BN for SWA model\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n\n    # Validation predictions\n    swa_model.eval()\n    with torch.no_grad():\n        val_pred_s = swa_model(X_va_t.to(device)).cpu().numpy()\n    val_pred = scaler_y.inverse_transform(val_pred_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (with noise): {val_mse:.6f}\")\n\n    # OOF and test preds\n    oof_preds[val_idx] = val_pred\n    with torch.no_grad():\n        test_s = swa_model(X_test_t.to(device)).cpu().numpy()\n    test_fold = scaler_y.inverse_transform(test_s)\n    test_preds_sum += test_fold\n\n# Compute overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (with noise): {oof_mse:.6f}\")\n\n# Average test predictions and save\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport copy\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Seed for reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+validation for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Define model\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.ReLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.ReLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.ReLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# CV setup\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfold = 0\nfor train_idx, val_idx in kf.split(X_all):\n    fold += 1\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Polynomial feature expansion (degree 2)\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    X_tr_p = poly.fit_transform(X_tr)\n    X_va_p = poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n\n    # Standardize inputs\n    scaler_x = StandardScaler()\n    X_tr_s = scaler_x.fit_transform(X_tr_p)\n    X_va_s = scaler_x.transform(X_va_p)\n    X_test_s = scaler_x.transform(X_test_p)\n\n    # Standardize targets\n    scaler_y = StandardScaler()\n    y_tr_s = scaler_y.fit_transform(y_tr)\n\n    # Tensors\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n\n    # DataLoader\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model, optimizer, loss\n    model = ResMLP(X_tr_s.shape[1], y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n\n    # SWA setup\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    epochs = 50\n    noise_std = 0.01\n\n    # Training loop with SWA and input noise augmentation\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb = xb + torch.randn_like(xb) * noise_std\n            optimizer.zero_grad()\n            preds = model(xb)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Update BN statistics for SWA model\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n\n    # Validation predictions\n    swa_model.eval()\n    with torch.no_grad():\n        val_pred_s = swa_model(X_va_t.to(device)).cpu().numpy()\n    val_pred = scaler_y.inverse_transform(val_pred_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (SWA+Poly+Noise): {val_mse:.6f}\")\n\n    # OOF and test preds\n    oof_preds[val_idx] = val_pred\n    with torch.no_grad():\n        test_s = swa_model(X_test_t.to(device)).cpu().numpy()\n    test_fold = scaler_y.inverse_transform(test_s)\n    test_preds_sum += test_fold\n\n# Compute overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (SWA+Poly+Noise): {oof_mse:.6f}\")\n\n# Average test predictions and save\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Seed\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Model\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.ReLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.ReLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.ReLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# CV\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Poly features\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    X_tr_p = poly.fit_transform(X_tr)\n    X_va_p = poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n\n    # Scale X\n    scaler_x = StandardScaler().fit(X_tr_p)\n    X_tr_s = scaler_x.transform(X_tr_p)\n    X_va_s = scaler_x.transform(X_va_p)\n    X_test_s = scaler_x.transform(X_test_p)\n\n    # Scale y\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    # Tensors\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model, opt, loss\n    model = ResMLP(X_tr_s.shape[1], y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n\n    # SWA\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    epochs = 50\n    noise_std = 0.01\n\n    # Train\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * noise_std\n            optimizer.zero_grad()\n            preds = model(xb_noisy)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Update BN for SWA\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n\n    # Validation\n    swa_model.eval()\n    with torch.no_grad():\n        val_s = swa_model(X_va_t.to(device)).cpu().numpy()\n    val_pred = scaler_y.inverse_transform(val_s)\n    mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n    with torch.no_grad():\n        test_s = swa_model(X_test_t.to(device)).cpu().numpy()\n    test_fold = scaler_y.inverse_transform(test_s)\n    test_preds_sum += test_fold\n\n# OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Save test preds\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n\n# Mixup function\ndef mixup_data(x, y, alpha=0.2):\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1.0\n    batch_size = x.size(0)\n    index = torch.randperm(batch_size).to(x.device)\n    mixed_x = lam * x + (1 - lam) * x[index]\n    mixed_y = lam * y + (1 - lam) * y[index]\n    return mixed_x, mixed_y\n\n\n# Seed for reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+validation for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Define model\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.ReLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.ReLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.ReLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# CV setup\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), start=1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Polynomial feature expansion (degree 2)\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    X_tr_p = poly.fit_transform(X_tr)\n    X_va_p = poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n\n    # Standardize inputs\n    scaler_x = StandardScaler().fit(X_tr_p)\n    X_tr_s = scaler_x.transform(X_tr_p)\n    X_va_s = scaler_x.transform(X_va_p)\n    X_test_s = scaler_x.transform(X_test_p)\n\n    # Standardize targets\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    # Tensors and DataLoader\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model, optimizer, loss\n    model = ResMLP(X_tr_s.shape[1], y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n\n    # SWA setup\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    epochs = 50\n\n    # Training loop with mixup + SWA\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_mix, yb_mix = mixup_data(xb, yb, alpha=0.2)\n            optimizer.zero_grad()\n            preds = model(xb_mix)\n            loss = criterion(preds, yb_mix)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Update BN for SWA model\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n\n    # Validation predictions\n    swa_model.eval()\n    with torch.no_grad():\n        val_pred_s = swa_model(X_va_t.to(device)).cpu().numpy()\n    val_pred = scaler_y.inverse_transform(val_pred_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (mixup+SWA+Poly): {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n    with torch.no_grad():\n        test_s = swa_model(X_test_t.to(device)).cpu().numpy()\n    test_preds_sum += scaler_y.inverse_transform(test_s)\n\n# Compute overall OOF MSE and save\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (mixup+SWA+Poly): {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Model\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.ReLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.ReLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.ReLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# CV setup\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfold = 0\nfor train_idx, val_idx in kf.split(X_all):\n    fold += 1\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n    # Poly features\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    X_tr_p = poly.fit_transform(X_tr)\n    X_va_p = poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n    # Scale inputs\n    scaler_x = StandardScaler()\n    X_tr_s = scaler_x.fit_transform(X_tr_p)\n    X_va_s = scaler_x.transform(X_va_p)\n    X_test_s = scaler_x.transform(X_test_p)\n    # Scale targets\n    scaler_y = StandardScaler()\n    y_tr_s = scaler_y.fit_transform(y_tr)\n    # Tensors & loaders\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n    # Model, optimizer, schedulers\n    model = ResMLP(X_tr_s.shape[1], y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start = 25\n    epochs = 50\n    base_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=1e-5)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n    criterion = nn.MSELoss()\n    # Training\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            preds = model(xb)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n        if epoch < swa_start:\n            base_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n    # Finalize BN stats\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    # Validation\n    swa_model.eval()\n    with torch.no_grad():\n        pred_va_s = swa_model(X_va_t.to(device)).cpu().numpy()\n    pred_va = scaler_y.inverse_transform(pred_va_s)\n    mse = np.mean((pred_va - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[val_idx] = pred_va\n    # Test\n    with torch.no_grad():\n        pred_te_s = swa_model(X_test_t.to(device)).cpu().numpy()\n    test_preds += scaler_y.inverse_transform(pred_te_s)\n\n# OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Seed for reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+validation for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Define model\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.ReLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.ReLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.ReLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# CV setup\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfold = 0\nfor train_idx, val_idx in kf.split(X_all):\n    fold += 1\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Polynomial feature expansion (degree 3)\n    poly = PolynomialFeatures(degree=3, include_bias=False)\n    X_tr_p = poly.fit_transform(X_tr)\n    X_va_p = poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n\n    # Standardize inputs\n    scaler_x = StandardScaler()\n    X_tr_s = scaler_x.fit_transform(X_tr_p)\n    X_va_s = scaler_x.transform(X_va_p)\n    X_test_s = scaler_x.transform(X_test_p)\n\n    # Standardize targets\n    scaler_y = StandardScaler()\n    y_tr_s = scaler_y.fit_transform(y_tr)\n\n    # Tensors\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n\n    # DataLoader\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model, optimizer, loss\n    model = ResMLP(X_tr_s.shape[1], y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n\n    # SWA setup\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    epochs = 50\n\n    # Training loop with SWA\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            preds = model(xb)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Update BN for SWA model\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n\n    # Validation predictions\n    swa_model.eval()\n    with torch.no_grad():\n        val_pred_s = swa_model(X_va_t.to(device)).cpu().numpy()\n    val_pred = scaler_y.inverse_transform(val_pred_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (SWA+Poly3): {val_mse:.6f}\")\n\n    # OOF and test preds\n    oof_preds[val_idx] = val_pred\n    with torch.no_grad():\n        test_s = swa_model(X_test_t.to(device)).cpu().numpy()\n    test_fold = scaler_y.inverse_transform(test_s)\n    test_preds_sum += test_fold\n\n# Compute overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (SWA+Poly3): {oof_mse:.6f}\")\n\n# Average test predictions and save\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+validation for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Model definition\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.ReLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.ReLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.ReLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# 5-fold CV setup\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Polynomial feature expansion degree=3\n    poly = PolynomialFeatures(degree=3, include_bias=False)\n    X_tr_p = poly.fit_transform(X_tr)\n    X_va_p = poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n\n    # Standardize inputs\n    scaler_x = StandardScaler()\n    X_tr_s = scaler_x.fit_transform(X_tr_p)\n    X_va_s = scaler_x.transform(X_va_p)\n    X_test_s = scaler_x.transform(X_test_p)\n\n    # Standardize targets\n    scaler_y = StandardScaler()\n    y_tr_s = scaler_y.fit_transform(y_tr)\n\n    # Tensors and loader\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32, device=device)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32, device=device)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32, device=device)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32, device=device)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model, optimizer, loss\n    model = ResMLP(X_tr_s.shape[1], y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n\n    # SWA setup\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    epochs = 50\n\n    # Training loop\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            optimizer.zero_grad()\n            preds = model(xb)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Update BN stats for SWA\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n\n    # Validation\n    swa_model.eval()\n    with torch.no_grad():\n        val_pred_s = swa_model(X_va_t).cpu().numpy()\n    val_pred = scaler_y.inverse_transform(val_pred_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (Poly deg=3): {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n    with torch.no_grad():\n        test_s = swa_model(X_test_t).cpu().numpy()\n    test_fold = scaler_y.inverse_transform(test_s)\n    test_preds_sum += test_fold\n\n# Overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (Poly deg=3): {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\", header=None).values\ns_train = pd.read_csv(\"./input/s_training.csv\", header=None).values\ng_val = pd.read_csv(\"./input/g_validation.csv\", header=None).values\ns_val = pd.read_csv(\"./input/s_validation.csv\", header=None).values\ng_test = pd.read_csv(\"./input/test_g.csv\", header=None).values\n\n# Combine train+validation for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Model definition\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.ReLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.ReLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.ReLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# 5-fold CV setup\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Polynomial feature expansion degree=3\n    poly = PolynomialFeatures(degree=3, include_bias=False)\n    X_tr_p = poly.fit_transform(X_tr)\n    X_va_p = poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n\n    # Standardize inputs\n    scaler_x = StandardScaler()\n    X_tr_s = scaler_x.fit_transform(X_tr_p)\n    X_va_s = scaler_x.transform(X_va_p)\n    X_test_s = scaler_x.transform(X_test_p)\n\n    # Standardize targets\n    scaler_y = StandardScaler()\n    y_tr_s = scaler_y.fit_transform(y_tr)\n\n    # Create CPU tensors for DataLoader\n    X_tr_tensor = torch.from_numpy(X_tr_s).float()\n    y_tr_tensor = torch.from_numpy(y_tr_s).float()\n    train_ds = TensorDataset(X_tr_tensor, y_tr_tensor)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=0)\n\n    # Prepare validation and test tensors (CPU)\n    X_va_tensor = torch.from_numpy(X_va_s).float()\n    X_test_tensor = torch.from_numpy(X_test_s).float()\n\n    # Model, optimizer, loss\n    model = ResMLP(X_tr_s.shape[1], y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n\n    # SWA setup\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=5e-4,\n        anneal_strategy=\"cos\",\n        anneal_epochs=max(1, 50 - swa_start),\n    )\n    epochs = 50\n\n    # Training loop\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb = xb.to(device)\n            yb = yb.to(device)\n            optimizer.zero_grad()\n            preds = model(xb)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Update BN stats for SWA\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n\n    # Validation\n    swa_model.eval()\n    with torch.no_grad():\n        val_preds_s = swa_model(X_va_tensor.to(device)).cpu().numpy()\n    val_preds = scaler_y.inverse_transform(val_preds_s)\n    val_mse = np.mean((val_preds - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_preds\n    with torch.no_grad():\n        test_s = swa_model(X_test_tensor.to(device)).cpu().numpy()\n    test_fold = scaler_y.inverse_transform(test_s)\n    test_preds_sum += test_fold\n\n# Overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.nn.utils import weight_norm\n\n# Seed\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Model with weight normalization\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(\n            weight_norm(nn.Linear(in_dim, 256)), nn.LayerNorm(256), nn.ReLU()\n        )\n        self.fc2 = nn.Sequential(\n            weight_norm(nn.Linear(256, 512)),\n            nn.LayerNorm(512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n        )\n        self.fc3 = nn.Sequential(\n            weight_norm(nn.Linear(512, 256)), nn.LayerNorm(256), nn.ReLU()\n        )\n        self.shortcut = weight_norm(nn.Linear(in_dim, 256))\n        self.out = weight_norm(nn.Linear(256, out_dim))\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# CV setup\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n    # Poly features\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    X_tr_p = poly.fit_transform(X_tr)\n    X_va_p = poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n    # Standardize\n    scaler_x = StandardScaler().fit(X_tr_p)\n    X_tr_s = scaler_x.transform(X_tr_p)\n    X_va_s = scaler_x.transform(X_va_p)\n    X_test_s = scaler_x.transform(X_test_p)\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n    # Tensors & loader\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=0)\n    # Model, optimizer, SWA\n    model = ResMLP(X_tr_s.shape[1], y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    # Train\n    for epoch in range(50):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            loss = criterion(model(xb), yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n    # Update BN and eval\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n    with torch.no_grad():\n        val_pred_s = swa_model(X_va_t.to(device)).cpu().numpy()\n    val_pred = scaler_y.inverse_transform(val_pred_s)\n    mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[val_idx] = val_pred\n    with torch.no_grad():\n        test_s = swa_model(X_test_t.to(device)).cpu().numpy()\n    test_preds_sum += scaler_y.inverse_transform(test_s)\n\n# Overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Save test\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds_sum / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Seed for reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for cross\u2010validation\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP WITHOUT weight_norm to allow SWA copying\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.ReLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.ReLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.ReLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# 5\u2010fold cross\u2010validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n    # Polynomial features\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    X_tr_p = poly.fit_transform(X_tr)\n    X_va_p = poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n    # Scaling\n    scaler_x = StandardScaler().fit(X_tr_p)\n    X_tr_s = scaler_x.transform(X_tr_p)\n    X_va_s = scaler_x.transform(X_va_p)\n    X_test_s = scaler_x.transform(X_test_p)\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n    # DataLoaders\n    tr_ds = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_s, dtype=torch.float32),\n    )\n    tr_loader = DataLoader(tr_ds, batch_size=128, shuffle=True, num_workers=0)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32).to(device)\n    # Model, optimizer, SWA\n    model = ResMLP(X_tr_s.shape[1], y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    # Training\n    for epoch in range(50):\n        model.train()\n        for xb, yb in tr_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            loss = criterion(model(xb), yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n    # Update BN for SWA and evaluate\n    update_bn(tr_loader, swa_model, device=device)\n    swa_model.eval()\n    with torch.no_grad():\n        val_pred_s = swa_model(X_va_t).cpu().numpy()\n        val_pred = scaler_y.inverse_transform(val_pred_s)\n    mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Validation MSE: {mse:.6f}\")\n    oof_preds[va_idx] = val_pred\n    # Test predictions\n    with torch.no_grad():\n        test_pred_s = swa_model(X_test_t).cpu().numpy()\n        test_preds_sum += scaler_y.inverse_transform(test_pred_s)\n\n# Overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Save final test predictions\nos.makedirs(\"./working\", exist_ok=True)\nsubmission = pd.DataFrame(test_preds_sum / n_splits)\nsubmission.to_csv(\"./working/submission.csv\", index=False, header=False)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+validation for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Define model with GELU activations\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# Cross-validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Polynomial features\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    X_tr_p = poly.fit_transform(X_tr)\n    X_va_p = poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n\n    # Scaling\n    scaler_x = StandardScaler()\n    X_tr_s = scaler_x.fit_transform(X_tr_p)\n    X_va_s = scaler_x.transform(X_va_p)\n    X_test_s = scaler_x.transform(X_test_p)\n\n    scaler_y = StandardScaler()\n    y_tr_s = scaler_y.fit_transform(y_tr)\n\n    # Tensors and loaders\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model, optimizer, SWA\n    model = ResMLP(X_tr_s.shape[1], y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    epochs = 50\n\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            preds = model(xb)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Update BN and evaluate\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n\n    with torch.no_grad():\n        val_pred_s = swa_model(X_va_t.to(device)).cpu().numpy()\n    val_pred = scaler_y.inverse_transform(val_pred_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (SWA+GELU): {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n    with torch.no_grad():\n        test_s = swa_model(X_test_t.to(device)).cpu().numpy()\n    test_fold = scaler_y.inverse_transform(test_s)\n    test_preds_sum += test_fold\n\n# Overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (SWA+GELU): {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+validation for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Define model with dropout added after fc3\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        # add dropout after third block\n        self.fc3 = nn.Sequential(\n            nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# Cross-validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Polynomial features\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    X_tr_p = poly.fit_transform(X_tr)\n    X_va_p = poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n\n    # Scaling\n    scaler_x = StandardScaler()\n    X_tr_s = scaler_x.fit_transform(X_tr_p)\n    X_va_s = scaler_x.transform(X_va_p)\n    X_test_s = scaler_x.transform(X_test_p)\n\n    scaler_y = StandardScaler()\n    y_tr_s = scaler_y.fit_transform(y_tr)\n\n    # Tensors and loaders\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=0)\n\n    # Model, optimizer, SWA\n    model = ResMLP(X_tr_s.shape[1], y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    epochs = 50\n\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            preds = model(xb)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Update BN and evaluate\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n\n    with torch.no_grad():\n        val_pred_s = swa_model(X_va_t.to(device)).cpu().numpy()\n    val_pred = scaler_y.inverse_transform(val_pred_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n    with torch.no_grad():\n        test_s = swa_model(X_test_t.to(device)).cpu().numpy()\n    test_fold = scaler_y.inverse_transform(test_s)\n    test_preds_sum += test_fold\n\n# Overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+validation for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Define model with GELU activations\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# Cross-validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Polynomial features\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    X_tr_p = poly.fit_transform(X_tr)\n    X_va_p = poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n\n    # Scaling\n    scaler_x = StandardScaler()\n    X_tr_s = scaler_x.fit_transform(X_tr_p)\n    X_va_s = scaler_x.transform(X_va_p)\n    X_test_s = scaler_x.transform(X_test_p)\n\n    scaler_y = StandardScaler()\n    y_tr_s = scaler_y.fit_transform(y_tr)\n\n    # Tensors and loaders\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model, optimizer, SWA\n    model = ResMLP(X_tr_s.shape[1], y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    epochs = 50\n\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            # Gaussian noise augmentation\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            optimizer.zero_grad()\n            preds = model(xb_noisy)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Update BN and evaluate\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n\n    with torch.no_grad():\n        val_pred_s = swa_model(X_va_t.to(device)).cpu().numpy()\n    val_pred = scaler_y.inverse_transform(val_pred_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (noise aug): {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n    with torch.no_grad():\n        test_s = swa_model(X_test_t.to(device)).cpu().numpy()\n    test_fold = scaler_y.inverse_transform(test_s)\n    test_preds_sum += test_fold\n\n# Overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (noise aug): {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# ResMLP with extra dropout after fc1 and fc3\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(\n            nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU(), nn.Dropout(0.1)\n        )\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(\n            nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU(), nn.Dropout(0.1)\n        )\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# 5-fold CV\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Polynomial features degree=2\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    X_tr_p = poly.fit_transform(X_tr)\n    X_va_p = poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n\n    # Scale inputs\n    scaler_x = StandardScaler().fit(X_tr_p)\n    X_tr_s = scaler_x.transform(X_tr_p)\n    X_va_s = scaler_x.transform(X_va_p)\n    X_test_s = scaler_x.transform(X_test_p)\n\n    # Scale targets\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    # DataLoaders\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model, optimizer, SWA\n    model = ResMLP(X_tr_s.shape[1], y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    epochs = 50\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    # Training loop\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            # Gaussian noise augmentation\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            optimizer.zero_grad()\n            preds = model(xb_noisy)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Final BN update & eval\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n    with torch.no_grad():\n        val_pred_s = swa_model(X_va_t.to(device)).cpu().numpy()\n    val_pred = scaler_y.inverse_transform(val_pred_s)\n    mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n    with torch.no_grad():\n        test_fold_s = swa_model(X_test_t.to(device)).cpu().numpy()\n    test_fold = scaler_y.inverse_transform(test_fold_s)\n    test_preds_sum += test_fold\n\n# Overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Save test predictions\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    X_tr_p = poly.fit_transform(X_tr)\n    X_va_p = poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n\n    scaler_x = StandardScaler()\n    X_tr_s = scaler_x.fit_transform(X_tr_p)\n    X_va_s = scaler_x.transform(X_va_p)\n    X_test_s = scaler_x.transform(X_test_p)\n\n    scaler_y = StandardScaler()\n    y_tr_s = scaler_y.fit_transform(y_tr)\n\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    model = ResMLP(X_tr_s.shape[1], y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n\n    swa_start = 25\n    epochs = 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n    scheduler_pre = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=1e-5)\n\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            optimizer.zero_grad()\n            preds = model(xb_noisy)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n        # Pre-SWA LR decay\n        if epoch < swa_start:\n            scheduler_pre.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n    with torch.no_grad():\n        val_pred_s = swa_model(X_va_t.to(device)).cpu().numpy()\n    val_pred = scaler_y.inverse_transform(val_pred_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (with pre-SWA Cosine): {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n    with torch.no_grad():\n        test_s = swa_model(X_test_t.to(device)).cpu().numpy()\n    test_fold = scaler_y.inverse_transform(test_s)\n    test_preds_sum += test_fold\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (with pre-SWA Cosine): {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+validation for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Define model with GELU activations\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# Cross-validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\ntta_steps = 5  # number of noisy copies for TTA\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Polynomial features\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    X_tr_p = poly.fit_transform(X_tr)\n    X_va_p = poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n\n    # Scaling\n    scaler_x = StandardScaler()\n    X_tr_s = scaler_x.fit_transform(X_tr_p)\n    X_va_s = scaler_x.transform(X_va_p)\n    X_test_s = scaler_x.transform(X_test_p)\n\n    scaler_y = StandardScaler()\n    y_tr_s = scaler_y.fit_transform(y_tr)\n\n    # Tensors and loaders\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model, optimizer, SWA\n    model = ResMLP(X_tr_s.shape[1], y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    epochs = 50\n\n    # Training loop\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            optimizer.zero_grad()\n            preds = model(xb_noisy)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Finalize SWA and update BN\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n\n    # Validation with TTA\n    val_preds_accum = np.zeros((X_va_s.shape[0], y_all.shape[1]))\n    with torch.no_grad():\n        for _ in range(tta_steps):\n            inp = X_va_t.to(device) + torch.randn_like(X_va_t.to(device)) * 0.01\n            pred_s = swa_model(inp).cpu().numpy()\n            val_preds_accum += scaler_y.inverse_transform(pred_s)\n    val_pred = val_preds_accum / tta_steps\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE with TTA: {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n\n    # Test with TTA\n    test_accum = np.zeros((g_test.shape[0], y_all.shape[1]))\n    with torch.no_grad():\n        for _ in range(tta_steps):\n            inp_t = X_test_t.to(device) + torch.randn_like(X_test_t.to(device)) * 0.01\n            test_s = swa_model(inp_t).cpu().numpy()\n            test_accum += scaler_y.inverse_transform(test_s)\n    test_fold = test_accum / tta_steps\n    test_preds_sum += test_fold\n\n# Overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"Overall OOF MSE with TTA: {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Model definition\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.short = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.short(x)\n        return self.out(x3)\n\n\n# CV and storage\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n    # Polynomial features\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    X_tr_p = poly.fit_transform(X_tr)\n    X_va_p = poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n    # Scaling\n    sx = StandardScaler().fit(X_tr_p)\n    X_tr_s, X_va_s, X_test_s = (\n        sx.transform(X_tr_p),\n        sx.transform(X_va_p),\n        sx.transform(X_test_p),\n    )\n    sy = StandardScaler().fit(y_tr)\n    y_tr_s = sy.transform(y_tr)\n    # Tensors & loaders\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_te_t = torch.tensor(X_test_s, dtype=torch.float32)\n    train_loader = DataLoader(\n        TensorDataset(X_tr_t, y_tr_t), batch_size=128, shuffle=True, num_workers=2\n    )\n    # Model, optimizer, SWA\n    model = ResMLP(X_tr_s.shape[1], y_all.shape[1]).to(device)\n    opt = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    crit = nn.MSELoss()\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_sched = SWALR(\n        opt, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    # Training\n    for ep in range(50):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            opt.zero_grad()\n            loss = crit(model(xb_noisy), yb)\n            loss.backward()\n            opt.step()\n        if ep >= swa_start:\n            swa_model.update_parameters(model)\n            swa_sched.step()\n    # Update BN\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n    # Prepare for TTA\n    X_va_d = X_va_t.to(device)\n    X_te_d = X_te_t.to(device)\n    tta = 5\n    # Validation TTA\n    val_preds_s = []\n    with torch.no_grad():\n        for _ in range(tta):\n            noise = torch.randn_like(X_va_d) * 0.01\n            val_preds_s.append(swa_model(X_va_d + noise).cpu().numpy())\n    val_mean_s = np.mean(val_preds_s, axis=0)\n    val_pred = sy.inverse_transform(val_mean_s)\n    mse_val = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (TTA): {mse_val:.6f}\")\n    oof_preds[va_idx] = val_pred\n    # Test TTA\n    test_preds_s = []\n    with torch.no_grad():\n        for _ in range(tta):\n            noise = torch.randn_like(X_te_d) * 0.01\n            test_preds_s.append(swa_model(X_te_d + noise).cpu().numpy())\n    test_mean_s = np.mean(test_preds_s, axis=0)\n    test_fold = sy.inverse_transform(test_mean_s)\n    test_preds_sum += test_fold\n\n# OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (TTA): {oof_mse:.6f}\")\n\n# Save test submission\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds_sum / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+validation for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Define model with GELU activations\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# Cross-validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\nn_tta = 5  # number of noisy copies for TTA\nnoise_scale = 0.01\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Polynomial features\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    X_tr_p = poly.fit_transform(X_tr)\n    X_va_p = poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n\n    # Scaling\n    scaler_x = StandardScaler()\n    X_tr_s = scaler_x.fit_transform(X_tr_p)\n    X_va_s = scaler_x.transform(X_va_p)\n    X_test_s = scaler_x.transform(X_test_p)\n\n    scaler_y = StandardScaler()\n    y_tr_s = scaler_y.fit_transform(y_tr)\n\n    # Tensors and loaders\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model, optimizer, SWA\n    model = ResMLP(X_tr_s.shape[1], y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    epochs = 50\n\n    # Training\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * noise_scale\n            optimizer.zero_grad()\n            preds = model(xb_noisy)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Update BN statistics for SWA model\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n\n    # Validation with TTA\n    val_preds_accum = np.zeros((X_va_t.size(0), y_all.shape[1]))\n    with torch.no_grad():\n        for _ in range(n_tta):\n            xb = X_va_t.to(device) + torch.randn_like(X_va_t.to(device)) * noise_scale\n            pred_s = swa_model(xb).cpu().numpy()\n            val_preds_accum += scaler_y.inverse_transform(pred_s)\n    val_pred = val_preds_accum / n_tta\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (TTA): {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n\n    # Test with TTA\n    test_accum = np.zeros((X_test_t.size(0), y_all.shape[1]))\n    with torch.no_grad():\n        for _ in range(n_tta):\n            xb = (\n                X_test_t.to(device)\n                + torch.randn_like(X_test_t.to(device)) * noise_scale\n            )\n            test_s = swa_model(xb).cpu().numpy()\n            test_accum += scaler_y.inverse_transform(test_s)\n    test_fold = test_accum / n_tta\n    test_preds_sum += test_fold\n\n# Overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (with TTA): {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+validation for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Define model with GELU activations\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# Cross-validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\ntta = 5  # number of test-time augmentations\nnoise_scale = 0.01\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Polynomial features\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    X_tr_p = poly.fit_transform(X_tr)\n    X_va_p = poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n\n    # Scaling\n    scaler_x = StandardScaler()\n    X_tr_s = scaler_x.fit_transform(X_tr_p)\n    X_va_s = scaler_x.transform(X_va_p)\n    X_test_s = scaler_x.transform(X_test_p)\n\n    scaler_y = StandardScaler()\n    y_tr_s = scaler_y.fit_transform(y_tr)\n\n    # Tensors and loaders\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model, optimizer, SWA\n    model = ResMLP(X_tr_s.shape[1], y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    epochs = 50\n\n    # Training loop\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * noise_scale\n            optimizer.zero_grad()\n            preds = model(xb_noisy)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Update BN stats\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n\n    # Validation with TTA\n    X_va_dev = X_va_t.to(device)\n    pred_sum_va = np.zeros((X_va_dev.size(0), y_all.shape[1]))\n    with torch.no_grad():\n        for _ in range(tta):\n            noise = torch.randn_like(X_va_dev) * noise_scale\n            out_s = swa_model(X_va_dev + noise).cpu().numpy()\n            pred_sum_va += out_s\n    pred_s_avg = pred_sum_va / tta\n    val_pred = scaler_y.inverse_transform(pred_s_avg)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (TTA): {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n\n    # Test with TTA\n    X_test_dev = X_test_t.to(device)\n    test_sum = np.zeros((X_test_dev.size(0), y_all.shape[1]))\n    with torch.no_grad():\n        for _ in range(tta):\n            noise = torch.randn_like(X_test_dev) * noise_scale\n            out_s = swa_model(X_test_dev + noise).cpu().numpy()\n            test_sum += out_s\n    test_avg = scaler_y.inverse_transform(test_sum / tta)\n    test_preds_sum += test_avg\n\n# Overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (TTA): {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Model with GELU\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\nTTA = 5  # number of noisy runs\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Polynomial features\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    X_tr_p = poly.fit_transform(X_tr)\n    X_va_p = poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n\n    # Scaling inputs\n    scaler_x = StandardScaler()\n    X_tr_s = scaler_x.fit_transform(X_tr_p)\n    X_va_s = scaler_x.transform(X_va_p)\n    X_test_s = scaler_x.transform(X_test_p)\n\n    # Scaling targets\n    scaler_y = StandardScaler()\n    y_tr_s = scaler_y.fit_transform(y_tr)\n\n    # Convert to tensors\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model, optimizer, SWA\n    model = ResMLP(X_tr_s.shape[1], y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    epochs = 50\n\n    # Training with noise augmentation\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            optimizer.zero_grad()\n            preds = model(xb_noisy)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Finalize SWA model\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n\n    # Validation with TTA\n    val_preds_s_sum = np.zeros((len(val_idx), y_all.shape[1]))\n    with torch.no_grad():\n        for _ in range(TTA):\n            xb = X_va_t.to(device) + torch.randn_like(X_va_t.to(device)) * 0.01\n            out = swa_model(xb).cpu().numpy()\n            val_preds_s_sum += out\n    val_pred_s = val_preds_s_sum / TTA\n    val_pred = scaler_y.inverse_transform(val_pred_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (TTA): {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n\n    # Test with TTA\n    test_preds_s_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n    with torch.no_grad():\n        for _ in range(TTA):\n            xb_test = X_test_t.to(device) + torch.randn_like(X_test_t.to(device)) * 0.01\n            out_test = swa_model(xb_test).cpu().numpy()\n            test_preds_s_sum += out_test\n    test_pred_s = test_preds_s_sum / TTA\n    test_fold = scaler_y.inverse_transform(test_pred_s)\n    test_preds_sum += test_fold\n\n# Overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (TTA): {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+validation for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Define model with GELU activations\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# Cross-validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\nn_tta = 5\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Polynomial features\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    X_tr_p = poly.fit_transform(X_tr)\n    X_va_p = poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n\n    # Scaling\n    scaler_x = StandardScaler()\n    X_tr_s = scaler_x.fit_transform(X_tr_p)\n    X_va_s = scaler_x.transform(X_va_p)\n    X_test_s = scaler_x.transform(X_test_p)\n\n    scaler_y = StandardScaler()\n    y_tr_s = scaler_y.fit_transform(y_tr)\n\n    # Tensors and loaders\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model, optimizer, SWA\n    model = ResMLP(X_tr_s.shape[1], y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    epochs = 50\n\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            optimizer.zero_grad()\n            preds = model(xb_noisy)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Update BN on SWA model\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n\n    # Validation with TTA\n    swa_model.eval()\n    X_va_gpu = X_va_t.to(device)\n    pred_va_s_sum = np.zeros((X_va.shape[0], y_all.shape[1]), dtype=np.float32)\n    with torch.no_grad():\n        for _ in range(n_tta):\n            noise = torch.randn_like(X_va_gpu) * 0.01\n            out_s = swa_model(X_va_gpu + noise).cpu().numpy()\n            pred_va_s_sum += out_s\n    pred_va_s = pred_va_s_sum / n_tta\n    val_pred = scaler_y.inverse_transform(pred_va_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (TTA): {val_mse:.6f}\")\n    oof_preds[val_idx] = val_pred\n\n    # Test with TTA\n    X_test_gpu = X_test_t.to(device)\n    pred_test_s_sum = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n    with torch.no_grad():\n        for _ in range(n_tta):\n            noise = torch.randn_like(X_test_gpu) * 0.01\n            out_s = swa_model(X_test_gpu + noise).cpu().numpy()\n            pred_test_s_sum += out_s\n    pred_test_s = pred_test_s_sum / n_tta\n    test_fold = scaler_y.inverse_transform(pred_test_s)\n    test_preds_sum += test_fold\n\n# Overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (TTA): {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nn_splits, batch_size = 5, 128\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    X_tr_p, X_va_p = poly.fit_transform(X_tr), poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n\n    scaler_x = StandardScaler()\n    X_tr_s, X_va_s = scaler_x.fit_transform(X_tr_p), scaler_x.transform(X_va_p)\n    X_test_s = scaler_x.transform(X_test_p)\n\n    scaler_y = StandardScaler()\n    y_tr_s = scaler_y.fit_transform(y_tr)\n\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(\n        train_ds, batch_size=batch_size, shuffle=True, num_workers=2\n    )\n\n    model = ResMLP(X_tr_s.shape[1], y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n\n    swa_start = 25\n    epochs = 50\n    # Pre-SWA Cosine LR scheduler\n    pre_swa_scheduler = optim.lr_scheduler.CosineAnnealingLR(\n        optimizer, T_max=swa_start, eta_min=1e-5\n    )\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            optimizer.zero_grad()\n            preds = model(xb_noisy)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n        if epoch < swa_start:\n            pre_swa_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n\n    with torch.no_grad():\n        val_pred_s = swa_model(X_va_t.to(device)).cpu().numpy()\n    val_pred = scaler_y.inverse_transform(val_pred_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n    with torch.no_grad():\n        test_s = swa_model(X_test_t.to(device)).cpu().numpy()\n    test_fold = scaler_y.inverse_transform(test_s)\n    test_preds_sum += test_fold\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+validation for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\ntta_times = 5\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    X_tr_p = poly.fit_transform(X_tr)\n    X_va_p = poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n\n    scaler_x = StandardScaler()\n    X_tr_s = scaler_x.fit_transform(X_tr_p)\n    X_va_s = scaler_x.transform(X_va_p)\n    X_test_s = scaler_x.transform(X_test_p)\n\n    scaler_y = StandardScaler()\n    y_tr_s = scaler_y.fit_transform(y_tr)\n\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    model = ResMLP(X_tr_s.shape[1], y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    epochs = 50\n\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            optimizer.zero_grad()\n            loss = criterion(model(xb_noisy), yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Update BN\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n\n    # Validation with TTA\n    with torch.no_grad():\n        preds_sum = np.zeros((X_va_t.size(0), y_all.shape[1]), dtype=np.float32)\n        for _ in range(tta_times):\n            xb = X_va_t.to(device) + torch.randn_like(X_va_t.to(device)) * 0.01\n            preds_sum += swa_model(xb).cpu().numpy()\n        val_pred_s = preds_sum / tta_times\n    val_pred = scaler_y.inverse_transform(val_pred_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE with TTA: {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n    # Test with TTA\n    with torch.no_grad():\n        preds_sum = np.zeros((X_test_t.size(0), y_all.shape[1]), dtype=np.float32)\n        for _ in range(tta_times):\n            xb = X_test_t.to(device) + torch.randn_like(X_test_t.to(device)) * 0.01\n            preds_sum += swa_model(xb).cpu().numpy()\n        test_fold_s = preds_sum / tta_times\n    test_fold = scaler_y.inverse_transform(test_fold_s)\n    test_preds_sum += test_fold\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE with TTA: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+validation for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Define model with GELU activations\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\ntta_runs = 5\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Polynomial features\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    X_tr_p = poly.fit_transform(X_tr)\n    X_va_p = poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n\n    # Scaling\n    scaler_x = StandardScaler()\n    X_tr_s = scaler_x.fit_transform(X_tr_p)\n    X_va_s = scaler_x.transform(X_va_p)\n    X_test_s = scaler_x.transform(X_test_p)\n\n    scaler_y = StandardScaler()\n    y_tr_s = scaler_y.fit_transform(y_tr)\n\n    # Tensors and loaders\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model, optimizer, SWA\n    model = ResMLP(X_tr_s.shape[1], y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    epochs = 50\n\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            optimizer.zero_grad()\n            loss = criterion(model(xb_noisy), yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Update BN for SWA model\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n\n    # Validation with TTA\n    val_preds_s = np.zeros((X_va_s.shape[0], y_all.shape[1]), dtype=np.float32)\n    with torch.no_grad():\n        for _ in range(tta_runs):\n            noise = torch.randn_like(X_va_t) * 0.01\n            pred_s = swa_model((X_va_t + noise).to(device)).cpu().numpy()\n            val_preds_s += pred_s\n    val_preds_s /= tta_runs\n    val_pred = scaler_y.inverse_transform(val_preds_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (TTA): {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n\n    # Test with TTA\n    test_fold_s = np.zeros((X_test_s.shape[0], y_all.shape[1]), dtype=np.float32)\n    with torch.no_grad():\n        for _ in range(tta_runs):\n            noise_t = torch.randn_like(X_test_t) * 0.01\n            pred_ts = swa_model((X_test_t + noise_t).to(device)).cpu().numpy()\n            test_fold_s += pred_ts\n    test_fold_s /= tta_runs\n    test_fold = scaler_y.inverse_transform(test_fold_s)\n    test_preds_sum += test_fold\n\n# Overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (TTA): {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\nn_pca = 100  # number of PCA components\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    X_tr_p = poly.fit_transform(X_tr)\n    X_va_p = poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n\n    scaler_x = StandardScaler().fit(X_tr_p)\n    X_tr_s = scaler_x.transform(X_tr_p)\n    X_va_s = scaler_x.transform(X_va_p)\n    X_test_s = scaler_x.transform(X_test_p)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=n_pca, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_pca, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    model = ResMLP(X_tr_s.shape[1], n_pca).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    epochs = 50\n\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            optimizer.zero_grad()\n            preds = model(xb_noisy)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n\n    with torch.no_grad():\n        val_pca = swa_model(X_va_t.to(device)).cpu().numpy()\n    val_s = pca.inverse_transform(val_pca)\n    val_pred = scaler_y.inverse_transform(val_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (PCA targets): {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n    with torch.no_grad():\n        test_pca = swa_model(X_test_t.to(device)).cpu().numpy()\n    test_s = pca.inverse_transform(test_pca)\n    test_fold = scaler_y.inverse_transform(test_s)\n    test_preds_sum += test_fold\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (PCA targets): {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+validation for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Model with BatchNorm1d instead of LayerNorm\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.BatchNorm1d(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.BatchNorm1d(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.BatchNorm1d(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# 5-fold CV\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Polynomial features degree 2\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    X_tr_p = poly.fit_transform(X_tr)\n    X_va_p = poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n\n    # Scale inputs\n    scaler_x = StandardScaler()\n    X_tr_s = scaler_x.fit_transform(X_tr_p)\n    X_va_s = scaler_x.transform(X_va_p)\n    X_test_s = scaler_x.transform(X_test_p)\n\n    # Scale targets\n    scaler_y = StandardScaler()\n    y_tr_s = scaler_y.fit_transform(y_tr)\n\n    # DataLoaders\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model, optimizer, SWA\n    model = ResMLP(X_tr_s.shape[1], y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    epochs = 50\n\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            # Gaussian noise augmentation\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            optimizer.zero_grad()\n            preds = model(xb_noisy)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Update BN statistics\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n\n    # Validation\n    with torch.no_grad():\n        val_pred_s = swa_model(X_va_t.to(device)).cpu().numpy()\n    val_pred = scaler_y.inverse_transform(val_pred_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (BatchNorm): {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n    # Test predictions\n    with torch.no_grad():\n        test_s = swa_model(X_test_t.to(device)).cpu().numpy()\n    test_fold = scaler_y.inverse_transform(test_s)\n    test_preds_sum += test_fold\n\n# Overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (BatchNorm): {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+validation for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Define model with GELU and dropout\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Polynomial features degree=2\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    X_tr_p = poly.fit_transform(X_tr)\n    X_va_p = poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n\n    # Scaling\n    scaler_x = StandardScaler()\n    X_tr_s = scaler_x.fit_transform(X_tr_p)\n    X_va_s = scaler_x.transform(X_va_p)\n    X_test_s = scaler_x.transform(X_test_p)\n\n    scaler_y = StandardScaler()\n    y_tr_s = scaler_y.fit_transform(y_tr)\n\n    # Tensors and loaders\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model, optimizer, SWA\n    model = ResMLP(X_tr_s.shape[1], y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    epochs = 50\n\n    # Training loop\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            optimizer.zero_grad()\n            preds = model(xb_noisy)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # BN update (no BatchNorm but kept for consistency)\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n\n    # Monte Carlo Dropout + noise TTA at inference\n    n_mc = 10\n    swa_model.train()  # keep dropout active\n\n    # Validation predictions\n    val_acc = np.zeros((X_va_s.shape[0], y_all.shape[1]), dtype=np.float32)\n    with torch.no_grad():\n        for _ in range(n_mc):\n            xb = X_va_t.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            preds_s = swa_model(xb_noisy).cpu().numpy()\n            val_acc += preds_s\n    val_pred_s = val_acc / n_mc\n    val_pred = scaler_y.inverse_transform(val_pred_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (MC Dropout TTA): {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n\n    # Test predictions\n    test_acc = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n    with torch.no_grad():\n        for _ in range(n_mc):\n            xb_t = X_test_t.to(device)\n            xb_noisy_t = xb_t + torch.randn_like(xb_t) * 0.01\n            preds_ts = swa_model(xb_noisy_t).cpu().numpy()\n            test_acc += preds_ts\n    test_fold_s = test_acc / n_mc\n    test_fold = scaler_y.inverse_transform(test_fold_s)\n    test_preds_sum += test_fold\n\n# Overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (MC Dropout TTA): {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+validation for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Define model with GELU activations\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# Cross-validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Polynomial features\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    X_tr_p = poly.fit_transform(X_tr)\n    X_va_p = poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n\n    # PCA reduction\n    pca = PCA(n_components=64, random_state=seed)\n    X_tr_pca = pca.fit_transform(X_tr_p)\n    X_va_pca = pca.transform(X_va_p)\n    X_test_pca = pca.transform(X_test_p)\n\n    # Scaling\n    scaler_x = StandardScaler()\n    X_tr_s = scaler_x.fit_transform(X_tr_pca)\n    X_va_s = scaler_x.transform(X_va_pca)\n    X_test_s = scaler_x.transform(X_test_pca)\n\n    scaler_y = StandardScaler()\n    y_tr_s = scaler_y.fit_transform(y_tr)\n\n    # Tensors and loaders\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model, optimizer, SWA\n    model = ResMLP(X_tr_s.shape[1], y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    epochs = 50\n\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            optimizer.zero_grad()\n            loss = criterion(model(xb_noisy), yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Update BN and evaluate\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n\n    with torch.no_grad():\n        val_pred_s = swa_model(X_va_t.to(device)).cpu().numpy()\n    val_pred = scaler_y.inverse_transform(val_pred_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (PCA): {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n    with torch.no_grad():\n        test_s = swa_model(X_test_t.to(device)).cpu().numpy()\n    test_fold = scaler_y.inverse_transform(test_s)\n    test_preds_sum += test_fold\n\n# Overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (PCA): {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Model definition\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# CV setup\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\n# TTA params\nn_tta = 5\nnoise_std = 0.01\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n    # Polynomial features\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    X_tr_p = poly.fit_transform(X_tr)\n    X_va_p = poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n    # Scaling\n    scaler_x = StandardScaler()\n    X_tr_s = scaler_x.fit_transform(X_tr_p)\n    X_va_s = scaler_x.transform(X_va_p)\n    X_test_s = scaler_x.transform(X_test_p)\n    scaler_y = StandardScaler()\n    y_tr_s = scaler_y.fit_transform(y_tr)\n    # Tensors and loaders\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n    # Model, optimizer, SWA\n    model = ResMLP(X_tr_s.shape[1], y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    epochs = 50\n    # Training\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * noise_std\n            optimizer.zero_grad()\n            loss = criterion(model(xb_noisy), yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n    # BN update\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n    # TTA inference on validation\n    X_va_d = X_va_t.to(device)\n    preds_s_sum = np.zeros((X_va_s.shape[0], y_all.shape[1]))\n    with torch.no_grad():\n        for _ in range(n_tta):\n            inp = X_va_d + torch.randn_like(X_va_d) * noise_std\n            out_s = swa_model(inp).cpu().numpy()\n            preds_s_sum += out_s\n    preds_s_avg = preds_s_sum / n_tta\n    val_pred = scaler_y.inverse_transform(preds_s_avg)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (TTA): {val_mse:.6f}\")\n    oof_preds[val_idx] = val_pred\n    # TTA inference on test\n    X_test_d = X_test_t.to(device)\n    test_s_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n    with torch.no_grad():\n        for _ in range(n_tta):\n            inp = X_test_d + torch.randn_like(X_test_d) * noise_std\n            test_s_sum += swa_model(inp).cpu().numpy()\n    test_s_avg = test_s_sum / n_tta\n    test_fold = scaler_y.inverse_transform(test_s_avg)\n    test_preds_sum += test_fold\n\n# Overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (TTA): {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+validation for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\ny_dim = y_all.shape[1]\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Define model with GELU activations and residual connection\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ResMLP, self).__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# Cross-validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_dim))\n\n# TTA parameters\nTTA = 5\nnoise_std = 0.01\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Polynomial feature expansion\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    X_tr_p = poly.fit_transform(X_tr)\n    X_va_p = poly.transform(X_va)\n    X_test_p = poly.transform(g_test)\n\n    # Scaling inputs\n    scaler_x = StandardScaler()\n    X_tr_s = scaler_x.fit_transform(X_tr_p)\n    X_va_s = scaler_x.transform(X_va_p)\n    X_test_s = scaler_x.transform(X_test_p)\n    # Scaling targets\n    scaler_y = StandardScaler()\n    y_tr_s = scaler_y.fit_transform(y_tr)\n\n    # Convert to tensors\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n\n    # DataLoader\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model, optimizer, SWA\n    model = ResMLP(X_tr_s.shape[1], y_dim).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start = 25\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=50 - swa_start\n    )\n    epochs = 50\n\n    # Training loop\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * noise_std\n            optimizer.zero_grad()\n            preds = model(xb_noisy)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Update BN stats for SWA model\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n\n    # Validation with TTA\n    X_va_gpu = X_va_t.to(device)\n    val_preds_s = np.zeros((X_va_t.shape[0], y_dim), dtype=np.float32)\n    with torch.no_grad():\n        for _ in range(TTA):\n            noisy = X_va_gpu + torch.randn_like(X_va_gpu) * noise_std\n            val_preds_s += swa_model(noisy).cpu().numpy()\n    val_preds_s /= TTA\n    val_pred = scaler_y.inverse_transform(val_preds_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (TTA): {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n\n    # Test prediction with TTA\n    X_test_gpu = X_test_t.to(device)\n    test_preds_s = np.zeros((X_test_t.shape[0], y_dim), dtype=np.float32)\n    with torch.no_grad():\n        for _ in range(TTA):\n            noisy = X_test_gpu + torch.randn_like(X_test_gpu) * noise_std\n            test_preds_s += swa_model(noisy).cpu().numpy()\n    test_preds_s /= TTA\n    test_fold = scaler_y.inverse_transform(test_preds_s)\n    test_preds_sum += test_fold\n\n# Overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (TTA): {oof_mse:.6f}\")\n\n# Save averaged test predictions\nos.makedirs(\"./working\", exist_ok=True)\ntest_preds_avg = test_preds_sum / n_splits\npd.DataFrame(test_preds_avg).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer-style model\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        # x: [B, F]\n        B, F = x.size()\n        x_ = x.unsqueeze(-1)  # [B, F, 1]\n        e = self.emb(x_) + self.pos_emb.unsqueeze(0)  # [B, F, D]\n        t = e.permute(1, 0, 2)  # [F, B, D]\n        out = self.trans(t)  # [F, B, D]\n        out = out.permute(1, 0, 2).reshape(B, -1)  # [B, F*D]\n        return self.head(out)\n\n\n# Cross-validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Scale inputs\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    # Scale targets\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    # Tensors & loader\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model & optimizer\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    # Training\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            optimizer.zero_grad()\n            loss = criterion(model(xb_noisy), yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # SWA finalize & eval\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n    with torch.no_grad():\n        val_s = swa_model(X_va_t.to(device)).cpu().numpy()\n    val_pred = scaler_y.inverse_transform(val_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n    with torch.no_grad():\n        test_s = swa_model(X_te_t.to(device)).cpu().numpy()\n    test_preds_sum += scaler_y.inverse_transform(test_s)\n\n# Overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds_sum / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer-style model\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        x_ = x.unsqueeze(-1)\n        e = self.emb(x_) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t)\n        out = out.permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Cross-validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\nn_tta = 5\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s, X_va_s, X_te_s = (\n        scaler_x.transform(X_tr),\n        scaler_x.transform(X_va),\n        scaler_x.transform(g_test),\n    )\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    # Training\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            optimizer.zero_grad()\n            loss = criterion(model(xb_noisy), yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # SWA finalize & BN update\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n\n    # Validation with TTA\n    with torch.no_grad():\n        xt = X_va_t.to(device)\n        acc = torch.zeros((xt.size(0), y_all.shape[1]), device=device)\n        for _ in range(n_tta):\n            acc += swa_model(xt + torch.randn_like(xt) * 0.01)\n        val_s = (acc / n_tta).cpu().numpy()\n    val_pred = scaler_y.inverse_transform(val_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (TTA): {val_mse:.6f}\")\n    oof_preds[val_idx] = val_pred\n\n    # Test with TTA\n    with torch.no_grad():\n        xt = X_te_t.to(device)\n        acc_te = torch.zeros((xt.size(0), y_all.shape[1]), device=device)\n        for _ in range(n_tta):\n            acc_te += swa_model(xt + torch.randn_like(xt) * 0.01)\n        test_s = (acc_te / n_tta).cpu().numpy()\n    test_preds_sum += scaler_y.inverse_transform(test_s)\n\n# Overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (TTA): {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds_sum / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer-style model\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        x_ = x.unsqueeze(-1)\n        e = self.emb(x_) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t)\n        out = out.permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Cross-validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\ntta_runs = 5  # number of noisy repeats at inference\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Scale inputs\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    # Scale targets\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    # Tensors & loader\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model & optimizer\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    # Training\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            optimizer.zero_grad()\n            loss = criterion(model(xb_noisy), yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # SWA finalize & BN update\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n\n    # Validation with TTA\n    X_va_dev = X_va_t.to(device)\n    preds_va = []\n    with torch.no_grad():\n        for _ in range(tta_runs):\n            noise = torch.randn_like(X_va_dev) * 0.01\n            out = swa_model(X_va_dev + noise).cpu().numpy()\n            preds_va.append(out)\n    mean_va = np.mean(preds_va, axis=0)\n    val_pred = scaler_y.inverse_transform(mean_va)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (TTA): {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n\n    # Test with TTA\n    X_te_dev = X_te_t.to(device)\n    preds_te = []\n    with torch.no_grad():\n        for _ in range(tta_runs):\n            noise = torch.randn_like(X_te_dev) * 0.01\n            out = swa_model(X_te_dev + noise).cpu().numpy()\n            preds_te.append(out)\n    mean_te = np.mean(preds_te, axis=0)\n    test_preds_sum += scaler_y.inverse_transform(mean_te)\n\n# Overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (TTA): {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds_sum / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer-style model\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        x_ = x.unsqueeze(-1)  # [B, F, 1]\n        e = self.emb(x_) + self.pos_emb.unsqueeze(0)  # [B, F, D]\n        t = e.permute(1, 0, 2)  # [F, B, D]\n        out = self.trans(t)  # [F, B, D]\n        out = out.permute(1, 0, 2).reshape(B, -1)  # [B, F*D]\n        return self.head(out)\n\n\n# CV and training\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\ntta_n = 5\nnoise_std = 0.01\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n    # scale\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s, X_va_s, X_te_s = (\n        scaler_x.transform(X_tr),\n        scaler_x.transform(X_va),\n        scaler_x.transform(g_test),\n    )\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n    # tensors\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32)\n\n    train_loader = DataLoader(\n        TensorDataset(X_tr_t, y_tr_t), batch_size=128, shuffle=True, num_workers=2\n    )\n\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    # training\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * noise_std\n            optimizer.zero_grad()\n            loss = criterion(model(xb_noisy), yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # finalize SWA & update BN\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n\n    # inference with TTA\n    X_va_dev = X_va_t.to(device)\n    tta_preds = []\n    with torch.no_grad():\n        for _ in range(tta_n):\n            out = swa_model(X_va_dev + torch.randn_like(X_va_dev) * noise_std)\n            tta_preds.append(out.cpu().numpy())\n    val_s = np.mean(tta_preds, axis=0)\n    val_pred = scaler_y.inverse_transform(val_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {val_mse:.6f}\")\n    oof_preds[val_idx] = val_pred\n\n    # test inference with TTA\n    X_te_dev = X_te_t.to(device)\n    tta_test = []\n    with torch.no_grad():\n        for _ in range(tta_n):\n            out = swa_model(X_te_dev + torch.randn_like(X_te_dev) * noise_std)\n            tta_test.append(out.cpu().numpy())\n    test_s = np.mean(tta_test, axis=0)\n    test_preds_sum += scaler_y.inverse_transform(test_s)\n\n# overall\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# save submission\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds_sum / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer-style model\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        x_ = x.unsqueeze(-1)\n        e = self.emb(x_) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t)\n        out = out.permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Cross-validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\ntta_rounds = 10  # Number of stochastic passes\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Scale inputs\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    # Scale targets\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    # Tensors & loader\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model & optimizer\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    # Training loop\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            optimizer.zero_grad()\n            loss = criterion(model(xb_noisy), yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # SWA finalize\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n\n    # Evaluation with MC Dropout + Gaussian noise TTA\n    preds_list = []\n    for inp in [X_va_t, X_te_t]:\n        agg = np.zeros((inp.size(0), y_all.shape[1]), dtype=np.float32)\n        for _ in range(tta_rounds):\n            swa_model.train()  # enable dropout\n            with torch.no_grad():\n                xb = inp.to(device)\n                xb_noisy = xb + torch.randn_like(xb) * 0.01\n                out = swa_model(xb_noisy).cpu().numpy()\n            agg += out\n        agg /= tta_rounds\n        # inverse scale\n        agg_orig = scaler_y.inverse_transform(agg)\n        preds_list.append(agg_orig)\n\n    val_pred, test_pred = preds_list\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n    test_preds_sum += test_pred\n\n# Overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds_sum / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        x_ = x.unsqueeze(-1)\n        e = self.emb(x_) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t)\n        out = out.permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\ntta_runs = 5  # number of noisy inference passes\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            optimizer.zero_grad()\n            loss = criterion(model(xb_noisy), yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # finalize SWA\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n\n    # Validation TTA\n    X_va_dev = X_va_t.to(device)\n    pred_sum = torch.zeros_like(swa_model(X_va_dev))\n    with torch.no_grad():\n        for _ in range(tta_runs):\n            noise = torch.randn_like(X_va_dev) * 0.01\n            pred_sum += swa_model(X_va_dev + noise)\n    val_s = (pred_sum / tta_runs).cpu().numpy()\n    val_pred = scaler_y.inverse_transform(val_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE with TTA: {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n\n    # Test TTA\n    X_te_dev = X_te_t.to(device)\n    pred_sum = torch.zeros_like(swa_model(X_te_dev))\n    with torch.no_grad():\n        for _ in range(tta_runs):\n            noise = torch.randn_like(X_te_dev) * 0.01\n            pred_sum += swa_model(X_te_dev + noise)\n    test_s = (pred_sum / tta_runs).cpu().numpy()\n    test_preds_sum += scaler_y.inverse_transform(test_s)\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"Overall OOF MSE with TTA: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds_sum / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer-style model\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        x_ = x.unsqueeze(-1)  # [B, F, 1]\n        e = self.emb(x_) + self.pos_emb.unsqueeze(0)  # [B, F, D]\n        t = e.permute(1, 0, 2)  # [F, B, D]\n        out = self.trans(t)\n        out = out.permute(1, 0, 2).reshape(B, -1)  # [B, F*D]\n        return self.head(out)\n\n\n# Cross-validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nn_tta = 5  # number of noisy copies for TTA\nnoise_std = 0.01\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Scale inputs\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    # Scale targets\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    # Tensors & loader\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model & optimizer\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    # Training\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * noise_std\n            optimizer.zero_grad()\n            loss = criterion(model(xb_noisy), yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # SWA finalize & bn update\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n\n    # Validation with TTA\n    X_va_dev = X_va_t.to(device)\n    preds_val = np.zeros((X_va_dev.size(0), y_all.shape[1]), dtype=np.float32)\n    with torch.no_grad():\n        for _ in range(n_tta):\n            x_noisy = X_va_dev + torch.randn_like(X_va_dev) * noise_std\n            out = swa_model(x_noisy).cpu().numpy()\n            preds_val += out\n    preds_val /= n_tta\n    val_pred = scaler_y.inverse_transform(preds_val)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (TTA): {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n\n    # Test with TTA\n    X_te_dev = X_te_t.to(device)\n    preds_test = np.zeros((X_te_dev.size(0), y_all.shape[1]), dtype=np.float32)\n    with torch.no_grad():\n        for _ in range(n_tta):\n            x_noisy = X_te_dev + torch.randn_like(X_te_dev) * noise_std\n            out = swa_model(x_noisy).cpu().numpy()\n            preds_test += out\n    preds_test /= n_tta\n    test_preds_sum += scaler_y.inverse_transform(preds_test)\n\n# Overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (TTA): {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds_sum / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        x_ = x.unsqueeze(-1)\n        e = self.emb(x_) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t)\n        out = out.permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\nalpha = 0.2  # mixup alpha\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            # Mixup\n            l = np.random.beta(alpha, alpha)\n            idx = torch.randperm(xb.size(0)).to(device)\n            xb_sh, yb_sh = xb[idx], yb[idx]\n            xb_mix = l * xb + (1 - l) * xb_sh\n            yb_mix = l * yb + (1 - l) * yb_sh\n            # Noise\n            xb_noisy = xb_mix + torch.randn_like(xb_mix) * 0.01\n            optimizer.zero_grad()\n            loss = criterion(model(xb_noisy), yb_mix)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n    with torch.no_grad():\n        val_s = swa_model(X_va_t.to(device)).cpu().numpy()\n    val_pred = scaler_y.inverse_transform(val_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n    with torch.no_grad():\n        test_s = swa_model(X_te_t.to(device)).cpu().numpy()\n    test_preds_sum += scaler_y.inverse_transform(test_s)\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds_sum / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer-style model\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        x_ = x.unsqueeze(-1)\n        e = self.emb(x_) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t)\n        out = out.permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# CV and training parameters\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\ntta_n = 5\nnoise_sigma = 0.01\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32)\n    train_loader = DataLoader(\n        TensorDataset(X_tr_t, y_tr_t), batch_size=128, shuffle=True, num_workers=2\n    )\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n    # Training\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * noise_sigma\n            optimizer.zero_grad()\n            loss = criterion(model(xb_noisy), yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n    # SWA finalize & eval with TTA\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n    # Validation TTA\n    val_sum = np.zeros((X_va_t.shape[0], y_all.shape[1]))\n    with torch.no_grad():\n        for _ in range(tta_n):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * noise_sigma\n            preds = swa_model(xb_noisy.to(device)).cpu().numpy()\n            val_sum += preds\n    val_avg = val_sum / tta_n\n    val_pred = scaler_y.inverse_transform(val_avg)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {val_mse:.6f}\")\n    oof_preds[val_idx] = val_pred\n    # Test TTA\n    test_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n    with torch.no_grad():\n        for _ in range(tta_n):\n            xt_noisy = X_te_t + torch.randn_like(X_te_t) * noise_sigma\n            preds_t = swa_model(xt_noisy.to(device)).cpu().numpy()\n            test_sum += preds_t\n    test_avg = test_sum / tta_n\n    test_preds_sum += scaler_y.inverse_transform(test_avg)\n\n# Overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds_sum / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\nTTA_ROUNDS = 10\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32)\n\n    train_loader = DataLoader(\n        TensorDataset(X_tr_t, y_tr_t), batch_size=128, shuffle=True, num_workers=2\n    )\n\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            optimizer.zero_grad()\n            loss = criterion(model(xb_noisy), yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n\n    # MC Dropout + TTA Inference for validation\n    val_accum = np.zeros((X_va_t.size(0), y_all.shape[1]), dtype=np.float32)\n    for _ in range(TTA_ROUNDS):\n        swa_model.train()  # enable dropout\n        with torch.no_grad():\n            pred_norm = swa_model(X_va_t.to(device)).cpu().numpy()\n        val_accum += pred_norm\n    val_mean_norm = val_accum / TTA_ROUNDS\n    val_pred = scaler_y.inverse_transform(val_mean_norm)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n\n    # MC Dropout + TTA for test\n    test_accum = np.zeros((X_te_t.size(0), y_all.shape[1]), dtype=np.float32)\n    for _ in range(TTA_ROUNDS):\n        swa_model.train()\n        with torch.no_grad():\n            tnorm = swa_model(X_te_t.to(device)).cpu().numpy()\n        test_accum += tnorm\n    test_mean_norm = test_accum / TTA_ROUNDS\n    test_preds_sum += scaler_y.inverse_transform(test_mean_norm)\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds_sum / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer-style model\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Cross-validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\ntta = 5  # number of noisy copies for TTA\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n    # Scale inputs\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n    # Scale targets\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n    # Tensors & loader\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32)\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n    # Model & optimizer\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n    # Training loop\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            optimizer.zero_grad()\n            loss = criterion(model(xb_noisy), yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n    # SWA finalize\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n    # Validation with TTA\n    val_preds = np.zeros((X_va_t.size(0), y_all.shape[1]), dtype=np.float32)\n    with torch.no_grad():\n        for t in range(tta):\n            inp = X_va_t.to(device) + torch.randn_like(X_va_t).to(device) * 0.01\n            out = swa_model(inp).cpu().numpy()\n            val_preds += out\n    val_preds /= tta\n    val_pred = scaler_y.inverse_transform(val_preds)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE with TTA: {val_mse:.6f}\")\n    oof_preds[val_idx] = val_pred\n    # Test with TTA\n    test_preds = np.zeros((X_te_t.size(0), y_all.shape[1]), dtype=np.float32)\n    with torch.no_grad():\n        for t in range(tta):\n            inp = X_te_t.to(device) + torch.randn_like(X_te_t).to(device) * 0.01\n            out = swa_model(inp).cpu().numpy()\n            test_preds += out\n    test_preds /= tta\n    test_preds_sum += scaler_y.inverse_transform(test_preds)\n\n# Overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE with TTA: {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds_sum / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions with TTA to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        x_ = x.unsqueeze(-1)\n        e = self.emb(x_) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t)\n        out = out.permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    # Training\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            optimizer.zero_grad()\n            loss = criterion(model(xb_noisy), yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # SWA finalize & eval with TTA\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n\n    # prepare devices\n    X_va_dev = X_va_t.to(device)\n    X_te_dev = X_te_t.to(device)\n    n_tta = 5\n\n    # validation TTA\n    with torch.no_grad():\n        accum_va = torch.zeros((X_va_dev.size(0), y_all.shape[1]), device=device)\n        for _ in range(n_tta):\n            noisy = X_va_dev + torch.randn_like(X_va_dev) * 0.01\n            accum_va += swa_model(noisy)\n        val_s = (accum_va / n_tta).cpu().numpy()\n    val_pred = scaler_y.inverse_transform(val_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {val_mse:.6f}\")\n    oof_preds[val_idx] = val_pred\n\n    # test TTA\n    with torch.no_grad():\n        accum_te = torch.zeros((X_te_dev.size(0), y_all.shape[1]), device=device)\n        for _ in range(n_tta):\n            noisy_te = X_te_dev + torch.randn_like(X_te_dev) * 0.01\n            accum_te += swa_model(noisy_te)\n        test_s = (accum_te / n_tta).cpu().numpy()\n    test_preds_sum += scaler_y.inverse_transform(test_s)\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds_sum / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer-style model\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        x_ = x.unsqueeze(-1)\n        e = self.emb(x_) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t)\n        out = out.permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Cross-validation with TTA\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\ntta_rounds = 5\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    # Training loop\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            optimizer.zero_grad()\n            loss = criterion(model(xb_noisy), yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Finalize SWA\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n\n    # Validation TTA\n    swa_model.eval()\n    val_preds_accum = np.zeros((X_va.shape[0], y_va.shape[1]))\n    with torch.no_grad():\n        for _ in range(tta_rounds):\n            xb = X_va_t.to(device) + torch.randn_like(X_va_t).to(device) * 0.01\n            ps = swa_model(xb).cpu().numpy()\n            val_preds_accum += ps\n    val_s_mean = val_preds_accum / tta_rounds\n    val_pred = scaler_y.inverse_transform(val_s_mean)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} TTA Val MSE: {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n\n    # Test TTA\n    test_accum = np.zeros((g_test.shape[0], y_all.shape[1]))\n    with torch.no_grad():\n        for _ in range(tta_rounds):\n            xb = X_te_t.to(device) + torch.randn_like(X_te_t).to(device) * 0.01\n            ts = swa_model(xb).cpu().numpy()\n            test_accum += ts\n    test_mean = scaler_y.inverse_transform(test_accum / tta_rounds)\n    test_preds_sum += test_mean\n\n# Overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE with TTA: {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds_sum / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer-style model\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        x_ = x.unsqueeze(-1)\n        e = self.emb(x_) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t)\n        out = out.permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Cross-validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\nTTA = 5  # number of noisy copies at inference\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Scale inputs\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    # Scale targets\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    # Tensors & loader\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model & optimizer\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    # Training\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            optimizer.zero_grad()\n            loss = criterion(model(xb_noisy), yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # SWA finalize & eval with TTA\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n    with torch.no_grad():\n        # Validation TTA\n        xv = X_va_t.to(device)\n        pred_sum = torch.zeros_like(xv).unsqueeze(-1)  # placeholder for shape\n        # Actually get correct shape for preds\n        pred_acc = torch.zeros((xv.size(0), y_va.shape[1]), device=device)\n        for _ in range(TTA):\n            noisy = xv + torch.randn_like(xv) * 0.01\n            pred_acc += swa_model(noisy)\n        pred_acc /= TTA\n        val_s = pred_acc.cpu().numpy()\n    val_pred = scaler_y.inverse_transform(val_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n    with torch.no_grad():\n        xt = X_te_t.to(device)\n        test_acc = torch.zeros((xt.size(0), y_all.shape[1]), device=device)\n        for _ in range(TTA):\n            noisy = xt + torch.randn_like(xt) * 0.01\n            test_acc += swa_model(noisy)\n        test_acc /= TTA\n        test_s = test_acc.cpu().numpy()\n    test_preds_sum += scaler_y.inverse_transform(test_s)\n\n# Overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds_sum / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer-style model\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        x_ = x.unsqueeze(-1)\n        e = self.emb(x_) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t)\n        out = out.permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Cross-validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nTTA_N = 5\nnoise_std = 0.01\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Scale inputs\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    # Scale targets\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    # Tensors & loader\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model & optimizer\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    # Training\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * noise_std\n            optimizer.zero_grad()\n            loss = criterion(model(xb_noisy), yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # SWA finalize & BN update\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n\n    # Monte Carlo Dropout TTA for validation\n    swa_model.train()  # enable dropout\n    val_accum = np.zeros((X_va_t.size(0), y_all.shape[1]))\n    with torch.no_grad():\n        Xva_dev = X_va_t.to(device)\n        for _ in range(TTA_N):\n            xb_noisy = Xva_dev + torch.randn_like(Xva_dev) * noise_std\n            out_s = swa_model(xb_noisy).cpu().numpy()\n            val_accum += out_s\n    val_avg_s = val_accum / TTA_N\n    val_pred = scaler_y.inverse_transform(val_avg_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {val_mse:.6f}\")\n    oof_preds[val_idx] = val_pred\n\n    # Monte Carlo Dropout TTA for test\n    test_accum = np.zeros((X_te_t.size(0), y_all.shape[1]))\n    with torch.no_grad():\n        Xte_dev = X_te_t.to(device)\n        for _ in range(TTA_N):\n            xb_noisy = Xte_dev + torch.randn_like(Xte_dev) * noise_std\n            out_s = swa_model(xb_noisy).cpu().numpy()\n            test_accum += out_s\n    test_preds_sum += scaler_y.inverse_transform(test_accum / TTA_N)\n\n# Overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds_sum / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer-style model\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t)\n        return self.head(out.permute(1, 0, 2).reshape(B, -1))\n\n\n# CV and training\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\nTTA_K = 5  # number of noisy passes at inference\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n    # scale inputs\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s, X_va_s, X_te_s = (\n        scaler_x.transform(X_tr),\n        scaler_x.transform(X_va),\n        scaler_x.transform(g_test),\n    )\n    # scale targets\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n    # tensors\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    # loader\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n    # model & optimizer\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n    # train loop\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            optimizer.zero_grad()\n            loss = criterion(model(xb_noisy), yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n    # finalize SWA\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n    # validation TTA\n    val_s_sum = np.zeros((X_va.shape[0], y_all.shape[1]), dtype=np.float32)\n    with torch.no_grad():\n        for _ in range(TTA_K):\n            vb = X_va_t + torch.randn_like(X_va_t) * 0.01\n            val_s_sum += swa_model(vb).cpu().numpy()\n    val_s_avg = val_s_sum / TTA_K\n    val_pred = scaler_y.inverse_transform(val_s_avg)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (TTA): {val_mse:.6f}\")\n    oof_preds[val_idx] = val_pred\n    # test TTA\n    test_s_sum = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n    with torch.no_grad():\n        for _ in range(TTA_K):\n            tb = X_te_t + torch.randn_like(X_te_t) * 0.01\n            test_s_sum += swa_model(tb).cpu().numpy()\n    test_s_avg = test_s_sum / TTA_K\n    test_pred = scaler_y.inverse_transform(test_s_avg)\n    test_preds_sum += test_pred\n\n# overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (TTA): {oof_mse:.6f}\")\n\n# save submission\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds_sum / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Cross-validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\ntta_runs = 5  # number of test-time augmentations\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Scale inputs\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    # Scale targets\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    # Tensors & loader\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model & optimizer\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    # Training loop\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            optimizer.zero_grad()\n            loss = criterion(model(xb_noisy), yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Finalize SWA\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n\n    # Validation inference with TTA\n    val_sum = np.zeros((X_va_t.size(0), y_all.shape[1]), dtype=np.float32)\n    with torch.no_grad():\n        for _ in range(tta_runs):\n            xb = X_va_t + torch.randn_like(X_va_t) * 0.01\n            preds_s = swa_model(xb.to(device)).cpu().numpy()\n            val_sum += scaler_y.inverse_transform(preds_s)\n    val_pred = val_sum / tta_runs\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (TTA): {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n\n    # Test inference with TTA\n    test_sum = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n    with torch.no_grad():\n        for _ in range(tta_runs):\n            xt = X_te_t + torch.randn_like(X_te_t) * 0.01\n            preds_s = swa_model(xt.to(device)).cpu().numpy()\n            test_sum += scaler_y.inverse_transform(preds_s)\n    test_fold_pred = test_sum / tta_runs\n    test_preds_sum += test_fold_pred\n\n# Overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (with TTA): {oof_mse:.6f}\")\n\n# Save test predictions\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds_sum / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Cross-validation with TTA\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\nn_tta = 5\ntta_noise = 0.01\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32)\n\n    train_loader = DataLoader(\n        TensorDataset(X_tr_t, y_tr_t), batch_size=128, shuffle=True, num_workers=2\n    )\n\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    # Train\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            optimizer.zero_grad()\n            loss = criterion(model(xb_noisy), yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # SWA finalize\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n\n    # Validation with TTA\n    X_va_dev = X_va_t.to(device)\n    val_s_sum = np.zeros((X_va_dev.size(0), y_all.shape[1]))\n    with torch.no_grad():\n        for _ in range(n_tta):\n            noisy = X_va_dev + torch.randn_like(X_va_dev) * tta_noise\n            val_s = swa_model(noisy).cpu().numpy()\n            val_s_sum += val_s\n    val_s_avg = val_s_sum / n_tta\n    val_pred = scaler_y.inverse_transform(val_s_avg)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n\n    # Test with TTA\n    X_te_dev = X_te_t.to(device)\n    test_fold_sum = np.zeros((X_te_dev.size(0), y_all.shape[1]))\n    with torch.no_grad():\n        for _ in range(n_tta):\n            noisy = X_te_dev + torch.randn_like(X_te_dev) * tta_noise\n            ts = swa_model(noisy).cpu().numpy()\n            test_fold_sum += scaler_y.inverse_transform(ts)\n    test_fold_avg = test_fold_sum / n_tta\n    test_preds_sum += test_fold_avg\n\n# Overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds_sum / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer-style model\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Cross-validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\nalpha = 0.2  # mixup alpha\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Scale inputs\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    # Scale targets\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    # Tensors & loader\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=0)\n\n    # Model & optimizer\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    # Training with mixup and noise\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            # mixup\n            lam = np.random.beta(alpha, alpha)\n            idx = torch.randperm(xb.size(0)).to(device)\n            xb_sh, yb_sh = xb[idx], yb[idx]\n            xb_mix = lam * xb + (1 - lam) * xb_sh\n            yb_mix = lam * yb + (1 - lam) * yb_sh\n            # noise\n            xb_noisy = xb_mix + torch.randn_like(xb_mix) * 0.01\n            optimizer.zero_grad()\n            loss = criterion(model(xb_noisy), yb_mix)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # SWA finalize & eval\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n    with torch.no_grad():\n        val_s = swa_model(X_va_t.to(device)).cpu().numpy()\n    val_pred = scaler_y.inverse_transform(val_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n    with torch.no_grad():\n        test_s = swa_model(X_te_t.to(device)).cpu().numpy()\n    test_preds_sum += scaler_y.inverse_transform(test_s)\n\n# Overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds_sum / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer-style model\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        x_ = x.unsqueeze(-1)\n        e = self.emb(x_) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t)\n        out = out.permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Cross-validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\ntta_times = 5\nnoise_std = 0.01\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Scale inputs\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    # Scale targets\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    # Tensors & loader\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model & optimizer\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    # Training loop\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * noise_std\n            optimizer.zero_grad()\n            loss = criterion(model(xb_noisy), yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # SWA finalize & BN update\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n\n    # Validation with TTA\n    swa_model.eval()\n    X_va_d = X_va_t.to(device)\n    preds_tta = []\n    with torch.no_grad():\n        for _ in range(tta_times):\n            noise = torch.randn_like(X_va_d) * noise_std\n            preds_tta.append(swa_model(X_va_d + noise).cpu().numpy())\n    val_s = np.mean(preds_tta, axis=0)\n    val_pred = scaler_y.inverse_transform(val_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE with TTA: {val_mse:.6f}\")\n    oof_preds[val_idx] = val_pred\n\n    # Test with TTA\n    X_te_d = X_te_t.to(device)\n    test_fold_preds = []\n    with torch.no_grad():\n        for _ in range(tta_times):\n            noise = torch.randn_like(X_te_d) * noise_std\n            test_fold_preds.append(swa_model(X_te_d + noise).cpu().numpy())\n    test_s_mean = np.mean(test_fold_preds, axis=0)\n    test_preds_sum += scaler_y.inverse_transform(test_s_mean)\n\n# Overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"Overall OOF MSE with TTA: {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds_sum / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer-style model\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        x_ = x.unsqueeze(-1)\n        e = self.emb(x_) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t)\n        out = out.permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Cross-validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\nn_tta = 5  # number of TTA samples\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Scale inputs and targets\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    # Tensors & loader\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32)\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model & optimizer & SWA\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    # Training loop\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            # Gaussian noise augmentation\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            optimizer.zero_grad()\n            loss = criterion(model(xb_noisy), yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # SWA finalize & batch-norm update\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n\n    # Evaluation with TTA\n    swa_model.eval()\n    # Validation\n    val_accum = np.zeros((X_va_t.size(0), y_va.shape[1]), dtype=np.float32)\n    with torch.no_grad():\n        for _ in range(n_tta):\n            xb = X_va_t.to(device) + torch.randn_like(X_va_t.to(device)) * 0.01\n            out = swa_model(xb).cpu().numpy()\n            val_accum += out\n    val_pred_s = val_accum / n_tta\n    val_pred = scaler_y.inverse_transform(val_pred_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n\n    # Test predictions TTA\n    test_accum = np.zeros((X_te_t.size(0), y_va.shape[1]), dtype=np.float32)\n    with torch.no_grad():\n        for _ in range(n_tta):\n            xb = X_te_t.to(device) + torch.randn_like(X_te_t.to(device)) * 0.01\n            out = swa_model(xb).cpu().numpy()\n            test_accum += out\n    test_s_avg = test_accum / n_tta\n    test_preds_sum += scaler_y.inverse_transform(test_s_avg)\n\n# Overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Save test predictions\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds_sum / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer-style model\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        x_ = x.unsqueeze(-1)\n        e = self.emb(x_) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t)\n        out = out.permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Cross-validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\ntta = 5  # number of TTA forward passes\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Scale inputs\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    # Scale targets\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    # Tensors & loader\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model & optimizer\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    # Training\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            optimizer.zero_grad()\n            loss = criterion(model(xb_noisy), yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # SWA finalize & update BN\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n\n    # Validation with TTA\n    with torch.no_grad():\n        preds_val = []\n        for _ in range(tta):\n            xv = X_va_t.to(device) + torch.randn_like(X_va_t.to(device)) * 0.01\n            preds_val.append(swa_model(xv).cpu().numpy())\n        val_s = np.mean(preds_val, axis=0)\n    val_pred = scaler_y.inverse_transform(val_s)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE with TTA: {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n\n    # Test with TTA\n    with torch.no_grad():\n        preds_test = []\n        for _ in range(tta):\n            xt = X_te_t.to(device) + torch.randn_like(X_te_t.to(device)) * 0.01\n            preds_test.append(swa_model(xt).cpu().numpy())\n        test_s = np.mean(preds_test, axis=0)\n    test_preds_sum += scaler_y.inverse_transform(test_s)\n\n# Overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE with TTA: {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds_sum / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer-style model\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        x_ = x.unsqueeze(-1)\n        e = self.emb(x_) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t)\n        out = out.permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Cross-validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\n\ntta_steps = 5\nnoise_std = 0.01\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Scale inputs\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    # Scale targets\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    # Tensors & loader\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model & optimizer\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    # Training\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * noise_std\n            optimizer.zero_grad()\n            loss = criterion(model(xb_noisy), yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # SWA finalize\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n\n    # Validation with TTA and MC Dropout\n    swa_model.train()  # enable dropout and transformer dropout\n    val_preds_norm = np.zeros((len(X_va_t), y_all.shape[1]), dtype=np.float32)\n    X_va_dev = X_va_t.to(device)\n    for _ in range(tta_steps):\n        with torch.no_grad():\n            noisy = X_va_dev + torch.randn_like(X_va_dev) * noise_std\n            out_norm = swa_model(noisy).cpu().numpy()\n            val_preds_norm += out_norm\n    val_preds_norm /= tta_steps\n    val_pred = scaler_y.inverse_transform(val_preds_norm)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n\n    # Test with TTA\n    test_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n    X_te_dev = X_te_t.to(device)\n    for _ in range(tta_steps):\n        with torch.no_grad():\n            noisy = X_te_dev + torch.randn_like(X_te_dev) * noise_std\n            out_norm = swa_model(noisy).cpu().numpy()\n            test_preds += scaler_y.inverse_transform(out_norm)\n    test_preds /= tta_steps\n    test_preds_sum += test_preds\n\n# Overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds_sum / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer-style model\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        x_ = x.unsqueeze(-1)\n        e = self.emb(x_) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t)\n        out = out.permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Cross-validation with TTA\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds_sum = np.zeros((g_test.shape[0], y_all.shape[1]))\nn_tta = 5\ntta_noise_std = 0.01\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[train_idx], X_all[val_idx]\n    y_tr, y_va = y_all[train_idx], y_all[val_idx]\n\n    # Scale inputs\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    # Scale targets\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    # Tensors & loader\n    X_tr_t = torch.tensor(X_tr_s, dtype=torch.float32)\n    y_tr_t = torch.tensor(y_tr_s, dtype=torch.float32)\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32)\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_tr_t, y_tr_t)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n\n    # Model & optimizer\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    # Training loop\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * tta_noise_std\n            optimizer.zero_grad()\n            loss = criterion(model(xb_noisy), yb)\n            loss.backward()\n            optimizer.step()\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # SWA finalize & BN update\n    swa_model.to(device)\n    update_bn(train_loader, swa_model, device=device)\n    swa_model.eval()\n\n    # Validation with TTA\n    val_preds_scaled = np.zeros((n_tta, X_va.shape[0], y_all.shape[1]))\n    with torch.no_grad():\n        for i in range(n_tta):\n            inp = (\n                X_va_t.to(device) + torch.randn_like(X_va_t).to(device) * tta_noise_std\n            )\n            out_s = swa_model(inp).cpu().numpy()\n            val_preds_scaled[i] = out_s\n    val_pred_scaled_mean = val_preds_scaled.mean(axis=0)\n    val_pred = scaler_y.inverse_transform(val_pred_scaled_mean)\n    val_mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {val_mse:.6f}\")\n\n    oof_preds[val_idx] = val_pred\n\n    # Test with TTA\n    test_preds_scaled = np.zeros((n_tta, g_test.shape[0], y_all.shape[1]))\n    with torch.no_grad():\n        for i in range(n_tta):\n            inp = (\n                X_te_t.to(device) + torch.randn_like(X_te_t).to(device) * tta_noise_std\n            )\n            out_s = swa_model(inp).cpu().numpy()\n            test_preds_scaled[i] = out_s\n    test_scaled_mean = test_preds_scaled.mean(axis=0)\n    test_orig = scaler_y.inverse_transform(test_scaled_mean)\n    test_preds_sum += test_orig\n\n# Overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds_sum / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer-style model\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)  # [B,F,emb]\n        t = e.permute(1, 0, 2)  # [F,B,emb]\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Smoothness penalty: mean squared second-order differences\ndef smoothness_penalty(y):\n    # y: [B,2001]\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]))\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_s, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True)\n\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred = model(xb_noisy)\n            loss = criterion(pred, yb) + lambda_smooth * smoothness_penalty(pred)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        if ep >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # finalize SWA\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n    swa_model.eval()\n    # validation\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    with torch.no_grad():\n        pred_s = swa_model(X_va_t).cpu().numpy()\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n    # test\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    with torch.no_grad():\n        test_s = swa_model(X_te_t).cpu().numpy()\n    test_preds += scaler_y.inverse_transform(test_s)\n\n# overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer-style model\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)  # [B,F,emb]\n        t = e.permute(1, 0, 2)  # [F,B,emb]\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Smoothness penalty: mean squared second-order differences\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]))\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\ntta_runs = 5  # number of noisy forward passes at inference\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    # Split\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n    # Scalers\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n    # Dataset\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_s, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True)\n    # Model, optimizer, SWA\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n    # Train\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred = model(xb_noisy)\n            loss = criterion(pred, yb) + lambda_smooth * smoothness_penalty(pred)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        if ep >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n    # SWA finalize\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n    swa_model.eval()\n    # Validation with TTA\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    preds_sum = np.zeros((X_va_t.size(0), y_all.shape[1]), dtype=np.float32)\n    with torch.no_grad():\n        for _ in range(tta_runs):\n            noisy = X_va_t + torch.randn_like(X_va_t) * 0.01\n            out = swa_model(noisy).cpu().numpy()\n            preds_sum += out\n    pred_s = preds_sum / tta_runs\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n    # Test with TTA\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    tp_sum = np.zeros((X_te_t.size(0), y_all.shape[1]), dtype=np.float32)\n    with torch.no_grad():\n        for _ in range(tta_runs):\n            noisy = X_te_t + torch.randn_like(X_te_t) * 0.01\n            out = swa_model(noisy).cpu().numpy()\n            tp_sum += out\n    test_s = tp_sum / tta_runs\n    test_preds += scaler_y.inverse_transform(test_s)\n\n# Overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer-style model\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Smoothness penalty: mean squared second-order differences\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]))\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\ntta_runs = 5\nnoise_std = 0.01\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_s, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True)\n\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * noise_std\n            pred = model(xb_noisy)\n            loss = criterion(pred, yb) + lambda_smooth * smoothness_penalty(pred)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        if ep >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # finalize SWA\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n    swa_model.eval()\n\n    # validation with TTA\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    with torch.no_grad():\n        acc = np.zeros((X_va_t.size(0), y_all.shape[1]), dtype=np.float32)\n        for _ in range(tta_runs):\n            noise = torch.randn_like(X_va_t) * noise_std\n            pred_s = swa_model(X_va_t + noise).cpu().numpy()\n            acc += pred_s\n        pred_s_avg = acc / tta_runs\n    pred = scaler_y.inverse_transform(pred_s_avg)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE with TTA: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # test with TTA\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    with torch.no_grad():\n        acc_t = np.zeros((X_te_t.size(0), y_all.shape[1]), dtype=np.float32)\n        for _ in range(tta_runs):\n            noise = torch.randn_like(X_te_t) * noise_std\n            test_s = swa_model(X_te_t + noise).cpu().numpy()\n            acc_t += test_s\n        test_preds += scaler_y.inverse_transform(acc_t / tta_runs)\n\n# overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE with TTA: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer-style model\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Smoothness penalty: mean squared second-order differences\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# Mixup augmentation\ndef mixup_batch(x, y, alpha=0.2):\n    if alpha <= 0:\n        return x, y\n    lam = np.random.beta(alpha, alpha)\n    batch_size = x.size(0)\n    perm = torch.randperm(batch_size).to(x.device)\n    x2, y2 = x[perm], y[perm]\n    x_mix = lam * x + (1 - lam) * x2\n    y_mix = lam * y + (1 - lam) * y2\n    return x_mix, y_mix\n\n\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]))\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_s, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True)\n\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb, yb = mixup_batch(xb, yb, alpha=0.2)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred = model(xb_noisy)\n            loss = criterion(pred, yb) + lambda_smooth * smoothness_penalty(pred)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        if ep >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n    swa_model.eval()\n    # validation\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    with torch.no_grad():\n        pred_s = swa_model(X_va_t).cpu().numpy()\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n    # test\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    with torch.no_grad():\n        test_s = swa_model(X_te_t).cpu().numpy()\n    test_preds += scaler_y.inverse_transform(test_s)\n\n# overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]))\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nTTA = 5  # number of noisy copies at inference\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_s, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True)\n\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred = model(xb_noisy)\n            loss = criterion(pred, yb) + lambda_smooth * smoothness_penalty(pred)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        if ep >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # finalize SWA\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n    swa_model.eval()\n\n    # validation with TTA\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    preds_va = 0\n    with torch.no_grad():\n        for _ in range(TTA):\n            noise = torch.randn_like(X_va_t) * 0.01\n            preds_va += swa_model(X_va_t + noise)\n    preds_va /= TTA\n    pred_s = preds_va.cpu().numpy()\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # test with TTA\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    preds_te = 0\n    with torch.no_grad():\n        for _ in range(TTA):\n            noise = torch.randn_like(X_te_t) * 0.01\n            preds_te += swa_model(X_te_t + noise)\n    preds_te /= TTA\n    test_preds += scaler_y.inverse_transform(preds_te.cpu().numpy())\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer-style model\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]))\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\ntta = 5  # number of noisy forward passes at test time\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_s, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True)\n\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    # Training loop with SWA\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred = model(xb_noisy)\n            loss = criterion(pred, yb) + lambda_smooth * smoothness_penalty(pred)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        if ep >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Finalize SWA and update BN\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # Validation with TTA\n    swa_model.eval()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    with torch.no_grad():\n        pred_acc = torch.zeros_like(swa_model(X_va_t))\n        for _ in range(tta):\n            noise = torch.randn_like(X_va_t) * 0.01\n            pred_acc += swa_model(X_va_t + noise)\n        pred_s = (pred_acc / tta).cpu().numpy()\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE with TTA: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # Test with TTA\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    with torch.no_grad():\n        test_acc = torch.zeros((g_test.shape[0], y_all.shape[1]), device=device)\n        for _ in range(tta):\n            noise = torch.randn_like(X_te_t) * 0.01\n            test_acc += swa_model(X_te_t + noise)\n        test_s = (test_acc / tta).cpu().numpy()\n        test_preds += scaler_y.inverse_transform(test_s)\n\n# Overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE with TTA: {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer-style model\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Smoothness penalty: mean squared second-order differences\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]))\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nn_tta = 5  # number of noisy inference passes\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_s, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True)\n\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    # Training loop with SWA\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred = model(xb_noisy)\n            loss = criterion(pred, yb) + lambda_smooth * smoothness_penalty(pred)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        if ep >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Finalize SWA model\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n    swa_model.eval()\n\n    # Validation with TTA\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    preds_s = np.zeros((X_va_t.size(0), y_all.shape[1]))\n    with torch.no_grad():\n        for _ in range(n_tta):\n            noise = torch.randn_like(X_va_t) * 0.01\n            out = swa_model(X_va_t + noise).cpu().numpy()\n            preds_s += out\n    preds_s /= n_tta\n    preds = scaler_y.inverse_transform(preds_s)\n    mse = np.mean((preds - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = preds\n\n    # Test with TTA\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    preds_test_s = np.zeros((X_te_t.size(0), y_all.shape[1]))\n    with torch.no_grad():\n        for _ in range(n_tta):\n            noise = torch.randn_like(X_te_t) * 0.01\n            out = swa_model(X_te_t + noise).cpu().numpy()\n            preds_test_s += out\n    preds_test_s /= n_tta\n    test_preds += scaler_y.inverse_transform(preds_test_s)\n\n# Overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Save test predictions\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer-style model\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)  # [B,F,emb]\n        t = e.permute(1, 0, 2)  # [F,B,emb]\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Smoothness penalty: mean squared second-order differences\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# Cross-validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nn_tta = 5\nnoise_std = 0.01\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_s, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True)\n\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    # Training\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * noise_std\n            pred = model(xb_noisy)\n            loss = criterion(pred, yb) + lambda_smooth * smoothness_penalty(pred)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        if ep >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Finalize SWA\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n    swa_model.eval()\n\n    # Prepare tensors\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n\n    # Validation with TTA\n    val_s_acc = np.zeros((X_va_s.shape[0], y_all.shape[1]), dtype=np.float32)\n    with torch.no_grad():\n        for _ in range(n_tta):\n            noise = torch.randn_like(X_va_t) * noise_std\n            out_s = swa_model(X_va_t + noise).cpu().numpy()\n            val_s_acc += out_s\n    val_s_acc /= n_tta\n    val_pred = scaler_y.inverse_transform(val_s_acc)\n    mse = np.mean((val_pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (TTA): {mse:.6f}\")\n    oof_preds[va_idx] = val_pred\n\n    # Test with TTA\n    test_s_acc = np.zeros((X_te_s.shape[0], y_all.shape[1]), dtype=np.float32)\n    with torch.no_grad():\n        for _ in range(n_tta):\n            noise = torch.randn_like(X_te_t) * noise_std\n            out_s = swa_model(X_te_t + noise).cpu().numpy()\n            test_s_acc += out_s\n    test_s_acc /= n_tta\n    test_pred_fold = scaler_y.inverse_transform(test_s_acc)\n    test_preds += test_pred_fold\n\n# Overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (TTA): {oof_mse:.6f}\")\n\n# Save test predictions\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head with GELU, LayerNorm, and Dropout\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer-style feature interaction\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Smoothness penalty: mean squared second-order differences\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]))\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\ntta_steps = 5  # Monte Carlo passes at inference\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    # Split data\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n    # Scale inputs and targets\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    # DataLoader\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_s, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True)\n\n    # Model, optimizer, SWA\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    # Training loop with input noise and smoothness penalty\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred = model(xb_noisy)\n            loss = criterion(pred, yb) + lambda_smooth * smoothness_penalty(pred)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        if ep >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Finalize SWA\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # Validation inference with MC Dropout TTA\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    swa_model.train()  # enable dropout\n    with torch.no_grad():\n        pred_sum = swa_model(X_va_t + torch.randn_like(X_va_t) * 0.01)\n        for _ in range(tta_steps - 1):\n            pred_sum += swa_model(X_va_t + torch.randn_like(X_va_t) * 0.01)\n    pred_avg = pred_sum / tta_steps\n    pred_s = pred_avg.cpu().numpy()\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # Test inference with MC Dropout TTA\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    swa_model.train()\n    with torch.no_grad():\n        test_sum = swa_model(X_te_t + torch.randn_like(X_te_t) * 0.01)\n        for _ in range(tta_steps - 1):\n            test_sum += swa_model(X_te_t + torch.randn_like(X_te_t) * 0.01)\n    test_avg = test_sum / tta_steps\n    test_s = test_avg.cpu().numpy()\n    test_preds += scaler_y.inverse_transform(test_s)\n\n# Overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer-style model\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)  # [B,F,emb]\n        t = e.permute(1, 0, 2)  # [F,B,emb]\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Smoothness penalty: mean squared second-order differences\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]))\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nn_tta = 5\ntta_noise = 0.01\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_s, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * tta_noise\n            pred = model(xb_noisy)\n            loss = criterion(pred, yb) + lambda_smooth * smoothness_penalty(pred)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        if ep >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # finalize SWA\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n    swa_model.eval()\n\n    # validation with TTA\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    preds_s = np.zeros((X_va_s.shape[0], y_all.shape[1]))\n    with torch.no_grad():\n        for _ in range(n_tta):\n            X_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            preds_s += swa_model(X_noisy).cpu().numpy()\n    pred_s = preds_s / n_tta\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (TTA): {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # test with TTA\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    preds_te = np.zeros((X_te_s.shape[0], y_all.shape[1]))\n    with torch.no_grad():\n        for _ in range(n_tta):\n            X_noisy_te = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            preds_te += swa_model(X_noisy_te).cpu().numpy()\n    test_preds += scaler_y.inverse_transform(preds_te / n_tta)\n\n# overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (TTA): {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head with GELU and LayerNorm\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer-style feature encoder + ResMLP head\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)  # [B,F,emb_dim]\n        t = e.permute(1, 0, 2)  # [F,B,emb_dim]\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Smoothness penalty\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# CV and training\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]))\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_s, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True)\n\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    # Training loop\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred = model(xb_noisy)\n            loss = criterion(pred, yb) + lambda_smooth * smoothness_penalty(pred)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        if ep >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # SWA finalize and MC-Dropout TTA inference\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n    N = 5  # number of MC samples\n    # Validation\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    preds = []\n    swa_model.train()\n    with torch.no_grad():\n        for _ in range(N):\n            noise = torch.randn_like(X_va_t) * 0.01\n            preds.append(swa_model(X_va_t + noise))\n    pred_s = torch.stack(preds).mean(0).cpu().numpy()\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # Test\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    preds_t = []\n    swa_model.train()\n    with torch.no_grad():\n        for _ in range(N):\n            noise = torch.randn_like(X_te_t) * 0.01\n            preds_t.append(swa_model(X_te_t + noise))\n    pred_s_t = torch.stack(preds_t).mean(0).cpu().numpy()\n    test_preds += scaler_y.inverse_transform(pred_s_t)\n\n# Overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer-style model\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=32, num_heads=4, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)  # [B,F,emb]\n        t = e.permute(1, 0, 2)  # [F,B,emb]\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Smoothness penalty: mean squared second-order differences\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# Mixup function\ndef mixup(x, y, alpha=0.2):\n    lam = np.random.beta(alpha, alpha)\n    batch_size = x.size(0)\n    perm = torch.randperm(batch_size).to(x.device)\n    x2, y2 = x[perm], y[perm]\n    x_mix = lam * x + (1 - lam) * x2\n    y_mix = lam * y + (1 - lam) * y2\n    return x_mix, y_mix\n\n\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]))\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nalpha_mix = 0.2\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_s, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True)\n\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_mix, yb_mix = mixup(xb, yb, alpha=alpha_mix)\n            xb_noisy = xb_mix + torch.randn_like(xb_mix) * 0.01\n            pred = model(xb_noisy)\n            loss = criterion(pred, yb_mix) + lambda_smooth * smoothness_penalty(pred)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        if ep >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # finalize SWA\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n    swa_model.eval()\n    # validation\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    with torch.no_grad():\n        pred_s = swa_model(X_va_t).cpu().numpy()\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n    # test\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    with torch.no_grad():\n        test_s = swa_model(X_te_t).cpu().numpy()\n    test_preds += scaler_y.inverse_transform(test_s)\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head unchanged\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer with larger embedding\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=64, num_heads=8, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)  # [B,F,emb]\n        t = e.permute(1, 0, 2)  # [F,B,emb]\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Smoothness penalty unchanged\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# 5-fold CV + SWA\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]))\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_s, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True)\n\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred = model(xb_noisy)\n            loss = criterion(pred, yb) + lambda_smooth * smoothness_penalty(pred)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        if ep >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n    swa_model.eval()\n\n    # validation\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    with torch.no_grad():\n        pred_s = swa_model(X_va_t).cpu().numpy()\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # test\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    with torch.no_grad():\n        test_s = swa_model(X_te_t).cpu().numpy()\n    test_preds += scaler_y.inverse_transform(test_s)\n\n# overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=64, num_heads=8, num_layers=2):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Smoothness penalty\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# 5-fold CV + SWA + TTA\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]))\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nnum_tta = 5\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_s, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True)\n\n    model = TabTransformerReg(num_features=14, out_dim=y_all.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred = model(xb_noisy)\n            loss = criterion(pred, yb) + lambda_smooth * smoothness_penalty(pred)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        if ep >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n    swa_model.eval()\n\n    # Validation with TTA\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], y_all.shape[1]), dtype=np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            x_noisy = X_va_t + torch.randn_like(X_va_t) * 0.01\n            pred_s = swa_model(x_noisy).cpu().numpy()\n            sum_pred += pred_s\n    avg_pred_s = sum_pred / num_tta\n    pred = scaler_y.inverse_transform(avg_pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # Test with TTA\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            x_noisy = X_te_t + torch.randn_like(X_te_t) * 0.01\n            test_s = swa_model(x_noisy).cpu().numpy()\n            sum_test += test_s\n    avg_test_s = sum_test / num_tta\n    test_preds += scaler_y.inverse_transform(avg_test_s)\n\n# overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer with increased capacity\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=128, num_heads=8, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Smoothness penalty\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# 5-fold CV + SWA + TTA\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]))\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nnum_tta = 5\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_s, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True)\n\n    model = TabTransformerReg(\n        num_features=14, out_dim=y_all.shape[1], emb_dim=128, num_heads=8, num_layers=3\n    ).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred = model(xb_noisy)\n            loss = criterion(pred, yb) + lambda_smooth * smoothness_penalty(pred)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        if ep >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n    swa_model.eval()\n\n    # Validation with TTA\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], y_all.shape[1]), dtype=np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            x_noisy = X_va_t + torch.randn_like(X_va_t) * 0.01\n            pred_s = swa_model(x_noisy).cpu().numpy()\n            sum_pred += pred_s\n    avg_pred_s = sum_pred / num_tta\n    pred = scaler_y.inverse_transform(avg_pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # Test with TTA\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            x_noisy = X_te_t + torch.randn_like(X_te_t) * 0.01\n            test_s = swa_model(x_noisy).cpu().numpy()\n            sum_test += test_s\n    avg_test_s = sum_test / num_tta\n    test_preds += scaler_y.inverse_transform(avg_test_s)\n\n# overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer with increased capacity\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=128, num_heads=8, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Smoothness penalty\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# 5-fold CV + SWA + MC Dropout TTA\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]))\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nnum_tta = 5\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_s, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True)\n\n    model = TabTransformerReg(\n        num_features=14, out_dim=y_all.shape[1], emb_dim=128, num_heads=8, num_layers=3\n    ).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=5e-4, anneal_strategy=\"cos\", anneal_epochs=epochs - swa_start\n    )\n\n    # Training\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred = model(xb_noisy)\n            loss = criterion(pred, yb) + lambda_smooth * smoothness_penalty(pred)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        if ep >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Update BN stats (no BN here but for completeness)\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # Monte Carlo Dropout TTA\n    swa_model.train()  # keep dropout active\n    # Validation with TTA\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], y_all.shape[1]), dtype=np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * 0.01\n            pred_s = swa_model(xb_noisy).cpu().numpy()\n            sum_pred += pred_s\n    avg_pred_s = sum_pred / num_tta\n    pred = scaler_y.inverse_transform(avg_pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # Test with TTA\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * 0.01\n            test_s = swa_model(xb_noisy).cpu().numpy()\n            sum_test += test_s\n    avg_test_s = sum_test / num_tta\n    test_preds += scaler_y.inverse_transform(avg_test_s)\n\n# overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer with increased capacity\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=128, num_heads=8, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Smoothness penalty\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# 5-fold CV + SWA + MC Dropout TTA + pre-SWA CosineAnnealingLR\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]))\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nnum_tta = 5\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_s, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True)\n\n    model = TabTransformerReg(\n        num_features=14, out_dim=y_all.shape[1], emb_dim=128, num_heads=8, num_layers=3\n    ).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    # Pre-SWA Cosine Annealing scheduler\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    # SWA\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    # Training\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred = model(xb_noisy)\n            loss = criterion(pred, yb) + lambda_smooth * smoothness_penalty(pred)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        # Scheduler step\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Update BN stats\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # Monte Carlo Dropout TTA\n    swa_model.train()\n    # Validation with TTA\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], y_all.shape[1]), dtype=np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * 0.01\n            pred_s = swa_model(xb_noisy).cpu().numpy()\n            sum_pred += pred_s\n    avg_pred_s = sum_pred / num_tta\n    pred = scaler_y.inverse_transform(avg_pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # Test with TTA\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * 0.01\n            test_s = swa_model(xb_noisy).cpu().numpy()\n            sum_test += test_s\n    avg_test_s = sum_test / num_tta\n    test_preds += scaler_y.inverse_transform(avg_test_s)\n\n# overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer regressor\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=128, num_heads=8, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# smoothness penalty\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# 5-fold CV + SWA + pre-SWA CosineAnnealing + TTA\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]))\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nnum_tta = 10\ntta_noise = 0.005\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n    # scale inputs\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n    # scale targets\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_s, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(\n        num_features=14, out_dim=y_all.shape[1], emb_dim=128, num_heads=8, num_layers=3\n    ).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred = model(xb_noisy)\n            loss = criterion(pred, yb) + lambda_smooth * smoothness_penalty(pred)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # validation TTA\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], y_all.shape[1]), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pred_s = sum_pred / num_tta\n    pred = scaler_y.inverse_transform(avg_pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # test TTA\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], y_all.shape[1]), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    test_preds += scaler_y.inverse_transform(sum_test / num_tta)\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer regressor\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=128, num_heads=8, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# 5-fold CV + SWA + CosineAnnealing + TTA (improved)\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]))\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nnum_tta = 20  # Increased from 10 to 20\ntta_noise = 0.003  # Reduced from 0.005 to 0.003\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_s, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(\n        num_features=14, out_dim=y_all.shape[1], emb_dim=128, num_heads=8, num_layers=3\n    ).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred = model(xb_noisy)\n            loss = criterion(pred, yb) + lambda_smooth * smoothness_penalty(pred)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # validation TTA\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], y_all.shape[1]), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pred_s = sum_pred / num_tta\n    pred = scaler_y.inverse_transform(avg_pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # test TTA\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], y_all.shape[1]), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    test_preds += scaler_y.inverse_transform(sum_test / num_tta)\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer regressor\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=128, num_heads=8, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# smoothness penalty\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# CV + SWA + Cosine + TTA + PCA-on-targets\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nnum_tta = 20\ntta_noise = 0.003\npca_dims = 100\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    # scale inputs\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    # scale targets\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    # PCA on targets\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    # Dataloader\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    # model and optim\n    model = TabTransformerReg(14, pca_dims, emb_dim=128, num_heads=8, num_layers=3).to(\n        device\n    )\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    # train\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # finalize SWA\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # validation TTA\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred_pca = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred_pca += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred_pca / num_tta\n    # reconstruct and inverse-scale\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # test TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test_pca = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test_pca += swa_model(xb_noisy).cpu().numpy()\n    avg_test_pca = sum_test_pca / num_tta\n    pred_te_s = pca.inverse_transform(avg_test_pca)\n    test_preds += scaler_y.inverse_transform(pred_te_s)\n\n# overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# save submission\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Residual MLP head\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\n# TabTransformer regressor\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=128, num_heads=8, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# smoothness penalty\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# CV + SWA + Cosine + TTA + PCA-on-targets (with 200 components)\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nnum_tta = 20\ntta_noise = 0.003\npca_dims = 200  # increased from 100 to 200\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(14, pca_dims, emb_dim=128, num_heads=8, num_layers=3).to(\n        device\n    )\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # validation TTA\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred_pca = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred_pca += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred_pca / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # test TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test_pca = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test_pca += swa_model(xb_noisy).cpu().numpy()\n    avg_test_pca = sum_test_pca / num_tta\n    pred_te_s = pca.inverse_transform(avg_test_pca)\n    test_preds += scaler_y.inverse_transform(pred_te_s)\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=128, num_heads=8, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# CV + SWA + Cosine + TTA + PCA-on-targets\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nnum_tta = 50  # increased TTA passes\ntta_noise = 0.002  # reduced noise level\npca_dims = 100\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(14, pca_dims, emb_dim=128, num_heads=8, num_layers=3).to(\n        device\n    )\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # validation TTA\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred_pca = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred_pca += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred_pca / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # test TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test_pca = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test_pca += swa_model(xb_noisy).cpu().numpy()\n    avg_test_pca = sum_test_pca / num_tta\n    pred_te_s = pca.inverse_transform(avg_test_pca)\n    test_preds += scaler_y.inverse_transform(pred_te_s)\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=128, num_heads=8, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# Cross-validation with SWA, cosine annealing, PCA-targets, and TTA\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nnum_tta = 50\ntta_noise = 0.002\npca_dims = 150  # increased PCA components\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(14, pca_dims, emb_dim=128, num_heads=8, num_layers=3).to(\n        device\n    )\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # Validation with TTA\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred_pca = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred_pca += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred_pca / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # Test with TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test_pca = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test_pca += swa_model(xb_noisy).cpu().numpy()\n    avg_test_pca = sum_test_pca / num_tta\n    pred_te_s = pca.inverse_transform(avg_test_pca)\n    test_preds += scaler_y.inverse_transform(pred_te_s)\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=128, num_heads=8, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# 5-fold CV with SWA, PCA targets, Cosine LR, and TTA\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nnum_tta = 50\ntta_noise = 0.002\npca_dims = 100\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(14, pca_dims, emb_dim=128, num_heads=8, num_layers=3).to(\n        device\n    )\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            # gradient clipping\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # validation TTA\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred_pca = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred_pca += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred_pca / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # test TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test_pca = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test_pca += swa_model(xb_noisy).cpu().numpy()\n    avg_test_pca = sum_test_pca / num_tta\n    pred_te_s = pca.inverse_transform(avg_test_pca)\n    test_preds += scaler_y.inverse_transform(pred_te_s)\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.GELU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# CV+SWA+PCA+Cosine+TTA settings\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nnum_tta = 50\ntta_noise = 0.002\npca_dims = 100\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(14, pca_dims, emb_dim=256, num_heads=16, num_layers=3).to(\n        device\n    )\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # validation TTA\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred_pca = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred_pca += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred_pca / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # test TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test_pca = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test_pca += swa_model(xb_noisy).cpu().numpy()\n    avg_test_pca = sum_test_pca / num_tta\n    pred_te_s = pca.inverse_transform(avg_test_pca)\n    test_preds += scaler_y.inverse_transform(pred_te_s)\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# Settings\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nnum_tta = 50\ntta_noise = 0.002\npca_dims = 100\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(14, pca_dims, emb_dim=256, num_heads=16, num_layers=3).to(\n        device\n    )\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # validation TTA\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # test TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    test_preds += scaler_y.inverse_transform(pred_te_s)\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# Settings\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nnum_tta = 100  # Increased from 50 to 100\ntta_noise = 0.002\npca_dims = 100\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(14, pca_dims, emb_dim=256, num_heads=16, num_layers=3).to(\n        device\n    )\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # validation TTA\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # test TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    test_preds += scaler_y.inverse_transform(pred_te_s)\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# Settings\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\ncriterion = nn.MSELoss()\nlambda_smooth = 5e-4  # increased from 1e-4 to 5e-4\nnum_tta = 100\ntta_noise = 0.002\npca_dims = 100\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(14, pca_dims, emb_dim=256, num_heads=16, num_layers=3).to(\n        device\n    )\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # validation TTA\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # test TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    test_preds += scaler_y.inverse_transform(pred_te_s)\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val for OOF\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# Settings\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-3  # increased from 1e-4 to 1e-3\npca_dims = 100\nnum_tta = 100\ntta_noise = 0.002\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n    model = TabTransformerReg(num_features=14, out_dim=pca_dims).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n    # validation TTA\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            sum_pred += (\n                swa_model(X_va_t + torch.randn_like(X_va_t) * tta_noise).cpu().numpy()\n            )\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n    # test TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            sum_test += (\n                swa_model(X_te_t + torch.randn_like(X_te_t) * tta_noise).cpu().numpy()\n            )\n    avg_test = sum_test / num_tta\n    test_preds += scaler_y.inverse_transform(pca.inverse_transform(avg_test))\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# Settings\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nnum_tta = 200  # increased from 100 to 200\ntta_noise = 0.001  # reduced from 0.002 to 0.001\npca_dims = 100\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(14, pca_dims, emb_dim=256, num_heads=16, num_layers=3).to(\n        device\n    )\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # validation TTA\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # test TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    test_preds += scaler_y.inverse_transform(pred_te_s)\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.2)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# CV settings\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nnum_tta = 100\ntta_noise = 0.002\npca_dims = 100\nmixup_alpha = 0.2\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(14, pca_dims, emb_dim=256, num_heads=16, num_layers=3).to(\n        device\n    )\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            # mixup\n            lam = np.random.beta(mixup_alpha, mixup_alpha)\n            idx = torch.randperm(xb.size(0))\n            xb = lam * xb + (1 - lam) * xb[idx]\n            yb = lam * yb + (1 - lam) * yb[idx]\n            # noise aug\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred = model(xb_noisy)\n            loss = criterion(pred, yb) + lambda_smooth * smoothness_penalty(pred)\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # batchnorm update\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # validation TTA\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # test TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    test_preds += scaler_y.inverse_transform(pred_te_s)\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.3)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# Settings\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nnum_tta = 100\ntta_noise = 0.002\npca_dims = 100\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(14, pca_dims, emb_dim=256, num_heads=16, num_layers=3).to(\n        device\n    )\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # validation TTA\n    swa_model.train()  # enable dropout for MC-TTA\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # test TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    test_preds += scaler_y.inverse_transform(pred_te_s)\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.3)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(\n        self,\n        num_features,\n        out_dim,\n        emb_dim=256,\n        num_heads=16,\n        num_layers=3,\n        trans_dropout=0.3,\n    ):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            dropout=trans_dropout,  # increased dropout\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# CV and training settings\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nnum_tta = 100\ntta_noise = 0.002\npca_dims = 100\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    # Scale inputs\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    # Scale and PCA targets\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    # Dataloader\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    # Model, optimizer, schedulers\n    model = TabTransformerReg(\n        14, pca_dims, emb_dim=256, num_heads=16, num_layers=3, trans_dropout=0.3\n    ).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    # Training loop\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # SWA finalize\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # Validation TTA (MC-Dropout + noise)\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # Test TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    test_preds += scaler_y.inverse_transform(pred_te_s)\n\n# OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.3)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# CV settings\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\ncriterion = nn.MSELoss()\nlambda_smooth = 5e-4  # increased smoothness penalty\nnum_tta = 100\ntta_noise = 0.002\npca_dims = 100\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(14, pca_dims, emb_dim=256, num_heads=16, num_layers=3).to(\n        device\n    )\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # validation TTA\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # test TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    test_preds += scaler_y.inverse_transform(pred_te_s)\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.3)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# Settings\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nnum_tta = 100\ntta_noise = 0.002\npca_dims = 150  # increased from 100 to 150\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(14, pca_dims, emb_dim=256, num_heads=16, num_layers=3).to(\n        device\n    )\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # validation TTA\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # test TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    test_preds += scaler_y.inverse_transform(pred_te_s)\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.3)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# Settings\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nnum_tta = 100\ntta_noise = 0.002\npca_dims = 150\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(14, pca_dims, emb_dim=256, num_heads=16, num_layers=3).to(\n        device\n    )\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n\n    # START SWA EARLIER AT EPOCH 15\n    swa_start, epochs = 15, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # validation TTA\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # test TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    test_preds += scaler_y.inverse_transform(pred_te_s)\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.3)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# CV and storage\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nnum_tta = 100\ntta_noise = 0.002\npca_dims = 150\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(\n        num_features=14, out_dim=pca_dims, emb_dim=256, num_heads=16, num_layers=3\n    ).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n\n    # Start SWA earlier at epoch 15\n    swa_start = 15\n    epochs = 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # Validation TTA\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # Test TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    test_preds += scaler_y.inverse_transform(pred_te_s)\n\n# Overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.3)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# Settings\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nnum_tta = 200  # increased from 100 to 200\ntta_noise = 0.001  # reduced noise from 0.002 to 0.001\npca_dims = 150\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(14, pca_dims).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # validation TTA\n    swa_model.train()  # keep dropout active\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # test TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    test_preds += scaler_y.inverse_transform(pred_te_s)\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom scipy.ndimage import gaussian_filter1d\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.3)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# Settings\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nnum_tta = 100\ntta_noise = 0.002\npca_dims = 150\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(14, pca_dims, emb_dim=256, num_heads=16, num_layers=3).to(\n        device\n    )\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # validation TTA\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    # apply gaussian smoothing across the spectrum axis\n    pred_sm = gaussian_filter1d(pred, sigma=1, axis=1)\n    mse = np.mean((pred_sm - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred_sm\n\n    # test TTA (accumulate)\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    pred_te = scaler_y.inverse_transform(pred_te_s)\n    test_preds += pred_te\n\n# compute OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# save smoothed test predictions\nos.makedirs(\"./working\", exist_ok=True)\ntest_avg = test_preds / n_splits\ntest_sm = gaussian_filter1d(test_avg, sigma=1, axis=1)\npd.DataFrame(test_sm).to_csv(\"./working/submission.csv\", index=False, header=False)\nprint(\"Saved smoothed test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.3)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# Settings\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nnum_tta = 100\ntta_noise = 0.002\npca_dims = 150\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s, X_va_s = scaler_x.transform(X_tr), scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(14, pca_dims, emb_dim=256, num_heads=16, num_layers=3).to(\n        device\n    )\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n\n    swa_start = 15  # start SWA earlier\n    epochs = 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # validation TTA\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # test TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    test_preds += scaler_y.inverse_transform(pred_te_s)\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom scipy.ndimage import gaussian_filter1d\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.3)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# Settings\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nnum_tta = 100\ntta_noise = 0.002\npca_dims = 200  # increased from 150 to 200\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(14, pca_dims, emb_dim=256, num_heads=16, num_layers=3).to(\n        device\n    )\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    # training\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # update BN for SWA\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # validation with TTA + gaussian smoothing\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    # gaussian smoothing on full spectrum\n    pred = gaussian_filter1d(pred, sigma=1, axis=1)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # test with TTA + smoothing\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    pred_te = scaler_y.inverse_transform(pred_te_s)\n    pred_te = gaussian_filter1d(pred_te, sigma=1, axis=1)\n    test_preds += pred_te\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Model definitions\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.3)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\ndef smooth_spectra(arr):\n    # simple triangular smoothing [1,2,1]/4 along axis=1\n    out = arr.copy()\n    out[:, 1:-1] = (arr[:, :-2] + 2 * arr[:, 1:-1] + arr[:, 2:]) / 4.0\n    return out\n\n\n# CV and training settings\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nnum_tta = 100\ntta_noise = 0.002\npca_dims = 150\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(14, pca_dims, emb_dim=256, num_heads=16, num_layers=3).to(\n        device\n    )\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # finalize SWA\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # Validation TTA with smoothing\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    pred_sm = smooth_spectra(pred)\n    mse = np.mean((pred_sm - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (smoothed): {mse:.6f}\")\n    oof_preds[va_idx] = pred_sm\n\n    # Test TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    test_preds += scaler_y.inverse_transform(pred_te_s)\n\n# Compute overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (smoothed): {oof_mse:.6f}\")\n\n# Save smoothed test predictions\nfinal_test = test_preds / n_splits\nfinal_test_sm = smooth_spectra(final_test)\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(final_test_sm).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved smoothed test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.3)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# CV and training settings\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\ncriterion = nn.MSELoss()\nlambda_smooth = 1e-4\nnum_tta = 100\ntta_noise = 0.002\npca_dims = 200\n\n# Triangular smoothing kernel\ntri_kernel = np.array([1.0, 2.0, 1.0], dtype=np.float32) / 4.0\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(14, pca_dims, emb_dim=256, num_heads=16, num_layers=3).to(\n        device\n    )\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # finalize SWA\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # validation TTA + smoothing\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    # apply triangular smoothing filter\n    pred_sm = np.apply_along_axis(\n        lambda r: np.convolve(r, tri_kernel, mode=\"same\"), 1, pred\n    )\n    mse = np.mean((pred_sm - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred_sm\n\n    # test TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test_pca = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test_pca)\n    pred_te = scaler_y.inverse_transform(pred_te_s)\n    test_preds += pred_te\n\n# overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# finalize test predictions with smoothing\ntest_final = test_preds / n_splits\ntest_smooth = np.apply_along_axis(\n    lambda r: np.convolve(r, tri_kernel, mode=\"same\"), 1, test_final\n)\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_smooth).to_csv(\"./working/submission.csv\", index=False, header=False)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.3)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# CV and placeholders\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\n# Use Huber loss instead of MSE\ncriterion = nn.SmoothL1Loss()\nlambda_smooth = 1e-4\nnum_tta = 100\ntta_noise = 0.002\npca_dims = 150\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(14, pca_dims, emb_dim=256, num_heads=16, num_layers=3).to(\n        device\n    )\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # validation TTA\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # test TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    test_preds += scaler_y.inverse_transform(pred_te_s)\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Define models\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.3)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# Triangular smoothing filter\nkernel = np.array([1, 2, 1], dtype=np.float32) / 4\n\n\ndef smooth_spectra(arr):\n    out = np.empty_like(arr)\n    for i in range(arr.shape[0]):\n        out[i] = np.convolve(arr[i], kernel, mode=\"same\")\n    return out\n\n\n# CV and placeholders\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\n# Loss and settings\ncriterion = nn.SmoothL1Loss()\nlambda_smooth = 1e-4\nnum_tta = 100\ntta_noise = 0.002\npca_dims = 150\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(14, pca_dims, emb_dim=256, num_heads=16, num_layers=3).to(\n        device\n    )\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    # Training\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # Update BN for SWA model\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # Validation with TTA and smoothing\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    pred = smooth_spectra(pred)  # apply smoothing\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # Test with TTA and smoothing\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    pred_te = scaler_y.inverse_transform(pred_te_s)\n    pred_te = smooth_spectra(pred_te)  # apply smoothing\n    test_preds += pred_te\n\n# Overall OOF metric\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.3)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# Cross-validation setup\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\n# Loss and hyperparameters\ncriterion = nn.SmoothL1Loss()\nlambda_smooth = 1e-4\nnum_tta = 100\ntta_noise = 0.002\npca_dims = 200  # increased from 150 to 200\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(\n        num_features=14, out_dim=pca_dims, emb_dim=256, num_heads=16, num_layers=3\n    ).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # update BN on SWA model\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # Validation TTA\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # Test TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    test_preds += scaler_y.inverse_transform(pred_te_s)\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.3)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# CV and containers\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\n# Loss and hyperparams\ncriterion = nn.MSELoss()  # Changed from SmoothL1Loss to MSELoss\nlambda_smooth = 1e-4\nnum_tta = 100\ntta_noise = 0.002\npca_dims = 200\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(num_features=14, out_dim=pca_dims).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # Validation TTA\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            sum_pred += (\n                swa_model(X_va_t + torch.randn_like(X_va_t) * tta_noise).cpu().numpy()\n            )\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # Test TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            sum_test += (\n                swa_model(X_te_t + torch.randn_like(X_te_t) * tta_noise).cpu().numpy()\n            )\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    test_preds += scaler_y.inverse_transform(pred_te_s)\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.3)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\ndef triangular_smooth(arr):\n    # arr: (n_samples, n_features)\n    kernel = np.array([1, 2, 1], dtype=np.float32) / 4.0\n    pad = np.pad(arr, ((0, 0), (1, 1)), mode=\"edge\")\n    sm = np.apply_along_axis(\n        lambda x: np.convolve(x, kernel, mode=\"valid\"), axis=1, arr=pad\n    )\n    return sm\n\n\n# Cross-validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\ncriterion = nn.SmoothL1Loss()\nlambda_smooth = 1e-4\nnum_tta = 100\ntta_noise = 0.002\npca_dims = 200\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(14, pca_dims, emb_dim=256, num_heads=16, num_layers=3).to(\n        device\n    )\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # Validation TTA + smoothing\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), dtype=np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    # apply triangular smoothing\n    pred_sm = triangular_smooth(pred)\n    mse = np.mean((pred_sm - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (smoothed): {mse:.6f}\")\n    oof_preds[va_idx] = pred_sm\n\n    # Test TTA + smoothing\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), dtype=np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    pred_te = scaler_y.inverse_transform(pred_te_s)\n    test_preds += triangular_smooth(pred_te)\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (smoothed): {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved smoothed test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Smoothing function\nkernel = np.array([1, 2, 1], dtype=np.float32) / 4.0\n\n\ndef smooth_spectra(arr):\n    # arr: (N, D)\n    padded = np.pad(arr, ((0, 0), (1, 1)), mode=\"edge\")\n    sm = np.apply_along_axis(\n        lambda row: np.convolve(row, kernel, mode=\"valid\"), 1, padded\n    )\n    return sm\n\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.3)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# CV setup\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\n# Hyperparams\ncriterion = nn.SmoothL1Loss()\nlambda_smooth = 1e-4\nnum_tta = 100\ntta_noise = 0.002\npca_dims = 200\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(14, pca_dims, emb_dim=256, num_heads=16, num_layers=3).to(\n        device\n    )\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # Validation TTA + smoothing\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    pred_sm = smooth_spectra(pred)\n    mse = np.mean((pred_sm - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred_sm\n\n    # Test TTA + smoothing\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    pred_te = scaler_y.inverse_transform(pred_te_s)\n    pred_te_sm = smooth_spectra(pred_te)\n    test_preds += pred_te_sm\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# combine train+val\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# model definitions (unchanged)\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.3)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\ndef triangular_smooth(y):\n    # y: (n_samples, spec_len)\n    y_s = y.copy()\n    y_s[:, 0] = (2 * y[:, 0] + y[:, 1]) / 3.0\n    y_s[:, -1] = (y[:, -2] + 2 * y[:, -1]) / 3.0\n    y_s[:, 1:-1] = (y[:, :-2] + 2 * y[:, 1:-1] + y[:, 2:]) / 4.0\n    return y_s\n\n\n# cross-validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros_like(g_test, dtype=np.float32)  # placeholder for shape\n\n# hyperparams\ncriterion = nn.SmoothL1Loss()\nlambda_smooth = 1e-4\nnum_tta = 100\ntta_noise = 0.002\npca_dims = 200\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n    # scale inputs\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s, X_va_s, X_te_s = (\n        scaler_x.transform(X_tr),\n        scaler_x.transform(X_va),\n        scaler_x.transform(g_test),\n    )\n    # scale and PCA on targets\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n    # dataloader\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n    # model and optim\n    model = TabTransformerReg(14, pca_dims).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_sched = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_sched = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n    # train\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_sched.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_sched.step()\n    # update BN stats\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n    # validation TTA\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    # apply triangular smoothing\n    pred = triangular_smooth(pred)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n    # test TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb).cpu().numpy()\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    pred_te = scaler_y.inverse_transform(pred_te_s)\n    pred_te = triangular_smooth(pred_te)\n    if fold == 1:\n        test_preds = pred_te\n    else:\n        test_preds += pred_te\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# save test preds\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n\n# Simple triangular smoothing filter\ndef smooth_spectra(arr):\n    kernel = np.array([1, 2, 1], dtype=np.float32) / 4\n    smoothed = np.empty_like(arr)\n    for i in range(arr.shape[0]):\n        smoothed[i] = np.convolve(arr[i], kernel, mode=\"same\")\n    return smoothed\n\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Model definitions\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.3)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# CV and training settings\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\ncriterion = nn.SmoothL1Loss()\nlambda_smooth = 1e-4\npca_dims = 200\nnum_tta = 100\ntta_noise = 0.002\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(num_features=14, out_dim=pca_dims).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    # Training loop\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    # update BN for SWA model\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # Validation TTA + smoothing\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    # apply triangular smoothing\n    pred_sm = smooth_spectra(pred)\n    mse = np.mean((pred_sm - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred_sm\n\n    # Test TTA + smoothing\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test_pca = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test_pca)\n    pred_te = scaler_y.inverse_transform(pred_te_s)\n    pred_te_sm = smooth_spectra(pred_te)\n    test_preds += pred_te_sm\n\n# Compute overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Save final test predictions\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.3)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=3):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# 5-fold CV\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\n# Hyperparams\ncriterion = nn.SmoothL1Loss()\nlambda_smooth = 1e-4\nnum_tta = 100\ntta_noise = 0.002\npca_dims = 300  # increased PCA components\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(\n        num_features=14, out_dim=pca_dims, emb_dim=256, num_heads=16, num_layers=3\n    ).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred_pca = model(xb_noisy)\n            loss = criterion(pred_pca, yb) + lambda_smooth * smoothness_penalty(\n                pred_pca\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # Validation TTA\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # Test TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    test_preds += scaler_y.inverse_transform(pred_te_s)\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.3)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=4):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# Cross-validation setup\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\n# Loss and hyperparameters\ncriterion = nn.SmoothL1Loss()\nlambda_smooth = 1e-4\nnum_tta = 100\ntta_noise = 0.002\npca_dims = 200\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(\n        num_features=14, out_dim=pca_dims, emb_dim=256, num_heads=16, num_layers=4\n    ).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred = model(xb_noisy)\n            loss = criterion(pred, yb) + lambda_smooth * smoothness_penalty(pred)\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # Validation TTA\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # Test TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    test_preds += scaler_y.inverse_transform(pred_te_s)\n\n# Overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val for CV\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Model definitions\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.3)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=4):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# CV setup\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\n# Hyperparameters\ncriterion = nn.SmoothL1Loss()\nlambda_smooth = 1e-4\nnum_tta = 100\ntta_noise = 0.002\npca_dims = 200\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(\n        num_features=14, out_dim=pca_dims, emb_dim=256, num_heads=16, num_layers=4\n    ).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred = model(xb_noisy)\n            loss = criterion(pred, yb) + lambda_smooth * smoothness_penalty(pred)\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # Validation TTA + smoothing\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    # triangular smoothing filter\n    padded = np.pad(pred, ((0, 0), (1, 1)), mode=\"edge\")\n    pred_sm = (padded[:, :-2] + 2 * padded[:, 1:-1] + padded[:, 2:]) / 4.0\n    mse = np.mean((pred_sm - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred_sm\n\n    # Test TTA + smoothing\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    pred_te = scaler_y.inverse_transform(pred_te_s)\n    padded_te = np.pad(pred_te, ((0, 0), (1, 1)), mode=\"edge\")\n    pred_te_sm = (padded_te[:, :-2] + 2 * padded_te[:, 1:-1] + padded_te[:, 2:]) / 4.0\n    test_preds += pred_te_sm\n\n# Overall OOF MSE\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n\n# Smoothing post\u2010processing\ndef smooth_spectra(arr):\n    sm = np.empty_like(arr)\n    sm[:, 1:-1] = (arr[:, :-2] + 2 * arr[:, 1:-1] + arr[:, 2:]) / 4.0\n    sm[:, 0] = (arr[:, 0] + arr[:, 1]) / 2.0\n    sm[:, -1] = (arr[:, -2] + arr[:, -1]) / 2.0\n    return sm\n\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Model definitions\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.3)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=4):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\n# Smoothness penalty\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# CV and training settings\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\ncriterion = nn.SmoothL1Loss()\nlambda_smooth = 1e-4\nnum_tta = 100\ntta_noise = 0.002\npca_dims = 200\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(\n        num_features=14, out_dim=pca_dims, emb_dim=256, num_heads=16, num_layers=4\n    ).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred = model(xb_noisy)\n            loss = criterion(pred, yb) + lambda_smooth * smoothness_penalty(pred)\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # Validation with TTA + smoothing\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred_raw = scaler_y.inverse_transform(pred_s)\n    pred = smooth_spectra(pred_raw)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # Test with TTA (accumulate raw, smooth later)\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test_pca = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test_pca)\n    test_raw = scaler_y.inverse_transform(pred_te_s)\n    test_preds += test_raw\n\n# Overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Smooth the final test predictions and save\nfinal_test = test_preds / n_splits\nsmooth_test = smooth_spectra(final_test)\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(smooth_test).to_csv(\"./working/submission.csv\", index=False, header=False)\nprint(\"Saved smoothed test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.3)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=512, num_heads=16, num_layers=4):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# Cross-validation setup\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\n# Loss and hyperparameters\ncriterion = nn.SmoothL1Loss()\nlambda_smooth = 1e-4\nnum_tta = 100\ntta_noise = 0.002\npca_dims = 200\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(\n        num_features=14, out_dim=pca_dims, emb_dim=512, num_heads=16, num_layers=4\n    ).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred = model(xb_noisy)\n            loss = criterion(pred, yb) + lambda_smooth * smoothness_penalty(pred)\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # Validation TTA\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # Test TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    test_preds += scaler_y.inverse_transform(pred_te_s)\n\n# Overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.3)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=4):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# CV and inference parameters\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\n# Loss and hyperparams\ncriterion = nn.SmoothL1Loss()\nlambda_smooth = 1e-4\npca_dims = 200\n\n# Updated TTA settings\nnum_tta = 200\ntta_noise = 0.001\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(\n        num_features=14, out_dim=pca_dims, emb_dim=256, num_heads=16, num_layers=4\n    ).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred = model(xb_noisy)\n            loss = criterion(pred, yb) + lambda_smooth * smoothness_penalty(pred)\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # Validation TTA\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # Test TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    test_preds += scaler_y.inverse_transform(pred_te_s)\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.3)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=4):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\ndef triangular_smooth(arr):\n    # arr shape (N, D)\n    pad = np.pad(arr, ((0, 0), (1, 1)), mode=\"edge\")\n    return (pad[:, :-2] + 2 * pad[:, 1:-1] + pad[:, 2:]) / 4\n\n\n# CV and storage\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\ncriterion = nn.SmoothL1Loss()\nlambda_smooth = 1e-4\nnum_tta = 100\ntta_noise = 0.002\npca_dims = 200\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n    model = TabTransformerReg(14, pca_dims, emb_dim=256, num_heads=16, num_layers=4).to(\n        device\n    )\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred = model(xb_noisy)\n            loss = criterion(pred, yb) + lambda_smooth * smoothness_penalty(pred)\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n    # Validation TTA + smoothing\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    pred_sm = triangular_smooth(pred)\n    mse = np.mean((pred_sm - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred_sm\n    # Test TTA + smoothing\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    pred_te = scaler_y.inverse_transform(pred_te_s)\n    test_preds += triangular_smooth(pred_te)\n\n# Overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Models\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.3)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=4):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# 1D Gaussian smoothing kernel\ngauss_kernel = np.array([1, 4, 6, 4, 1], dtype=np.float32) / 16.0\n\n\ndef apply_smoothing(preds):\n    # preds: (N, D)\n    return np.apply_along_axis(\n        lambda x: np.convolve(x, gauss_kernel, mode=\"same\"), 1, preds\n    )\n\n\n# CV and training params\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = (\n    np.zeros_like(g_test, dtype=np.float32)\n    if False\n    else np.zeros((g_test.shape[0], y_all.shape[1]), np.float32)\n)\n\ncriterion = nn.SmoothL1Loss()\nlambda_smooth = 1e-4\nnum_tta = 100\ntta_noise = 0.002\npca_dims = 200\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(\n        num_features=14, out_dim=pca_dims, emb_dim=256, num_heads=16, num_layers=4\n    ).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred = model(xb_noisy)\n            loss = criterion(pred, yb) + lambda_smooth * smoothness_penalty(pred)\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # Validation with TTA + smoothing\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    pred_sm = apply_smoothing(pred)\n    mse = np.mean((pred_sm - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE (smoothed): {mse:.6f}\")\n    oof_preds[va_idx] = pred_sm\n\n    # Test with TTA + smoothing\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    pred_te = scaler_y.inverse_transform(pred_te_s)\n    test_preds += apply_smoothing(pred_te)\n\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE (smoothed): {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.3)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=4):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# CV setup\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\n# Hyperparams\ncriterion = nn.MSELoss()  # REPLACE Huber with MSE\nlambda_smooth = 1e-4\nnum_tta = 100\ntta_noise = 0.002\npca_dims = 200\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(14, pca_dims, emb_dim=256, num_heads=16, num_layers=4).to(\n        device\n    )\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred = model(xb_noisy)\n            loss = criterion(pred, yb) + lambda_smooth * smoothness_penalty(pred)\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # Validation TTA\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            sum_pred += (\n                swa_model(X_va_t + torch.randn_like(X_va_t) * tta_noise).cpu().numpy()\n            )\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    mse = np.mean((pred - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred\n\n    # Test TTA\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            sum_test += (\n                swa_model(X_te_t + torch.randn_like(X_te_t) * tta_noise).cpu().numpy()\n            )\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    test_preds += scaler_y.inverse_transform(pred_te_s)\n\n# Overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Load data\ng_train = pd.read_csv(\"./input/g_training.csv\").values\ns_train = pd.read_csv(\"./input/s_training.csv\").values\ng_val = pd.read_csv(\"./input/g_validation.csv\").values\ns_val = pd.read_csv(\"./input/s_validation.csv\").values\ng_test = pd.read_csv(\"./input/test_g.csv\").values\n\n# Combine train+val\nX_all = np.vstack([g_train, g_val])\ny_all = np.vstack([s_train, s_val])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ResMLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Sequential(nn.Linear(in_dim, 256), nn.LayerNorm(256), nn.SiLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 512), nn.LayerNorm(512), nn.SiLU(), nn.Dropout(0.3)\n        )\n        self.fc3 = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256), nn.SiLU())\n        self.shortcut = nn.Linear(in_dim, 256)\n        self.out = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2) + self.shortcut(x)\n        return self.out(x3)\n\n\nclass TabTransformerReg(nn.Module):\n    def __init__(self, num_features, out_dim, emb_dim=256, num_heads=16, num_layers=4):\n        super().__init__()\n        self.emb = nn.Linear(1, emb_dim)\n        self.pos_emb = nn.Parameter(torch.randn(num_features, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=emb_dim * 4,\n            activation=\"gelu\",\n            batch_first=False,\n        )\n        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n        self.head = ResMLP(num_features * emb_dim, out_dim)\n\n    def forward(self, x):\n        B, F = x.size()\n        e = self.emb(x.unsqueeze(-1)) + self.pos_emb.unsqueeze(0)\n        t = e.permute(1, 0, 2)\n        out = self.trans(t).permute(1, 0, 2).reshape(B, -1)\n        return self.head(out)\n\n\ndef smoothness_penalty(y):\n    diff2 = y[:, 2:] - 2 * y[:, 1:-1] + y[:, :-2]\n    return torch.mean(diff2**2)\n\n\n# Cross-validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\noof_preds = np.zeros_like(y_all, dtype=np.float32)\ntest_preds = np.zeros_like(g_test, dtype=np.float32) * np.nan\n# Actually test_preds should match y dims\ntest_preds = np.zeros((g_test.shape[0], y_all.shape[1]), dtype=np.float32)\n\n# Loss and settings\ncriterion = nn.SmoothL1Loss()\nlambda_smooth = 1e-4\nnum_tta = 100\ntta_noise = 0.002\npca_dims = 200\n\n# smoothing kernel\nkernel = np.array([1, 2, 1], dtype=np.float32) / 4\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n\n    scaler_x = StandardScaler().fit(X_tr)\n    X_tr_s = scaler_x.transform(X_tr)\n    X_va_s = scaler_x.transform(X_va)\n    X_te_s = scaler_x.transform(g_test)\n\n    scaler_y = StandardScaler().fit(y_tr)\n    y_tr_s = scaler_y.transform(y_tr)\n\n    pca = PCA(n_components=pca_dims, random_state=seed).fit(y_tr_s)\n    y_tr_pca = pca.transform(y_tr_s)\n\n    ds_tr = TensorDataset(\n        torch.tensor(X_tr_s, dtype=torch.float32),\n        torch.tensor(y_tr_pca, dtype=torch.float32),\n    )\n    loader = DataLoader(ds_tr, batch_size=128, shuffle=True, num_workers=0)\n\n    model = TabTransformerReg(\n        num_features=14, out_dim=pca_dims, emb_dim=256, num_heads=16, num_layers=4\n    ).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    swa_start, epochs = 25, 50\n    swa_lr = 5e-4\n    cos_scheduler = CosineAnnealingLR(optimizer, T_max=swa_start, eta_min=swa_lr)\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer,\n        swa_lr=swa_lr,\n        anneal_strategy=\"cos\",\n        anneal_epochs=epochs - swa_start,\n    )\n\n    for ep in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            xb_noisy = xb + torch.randn_like(xb) * 0.01\n            pred = model(xb_noisy)\n            loss = criterion(pred, yb) + lambda_smooth * smoothness_penalty(pred)\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        if ep < swa_start:\n            cos_scheduler.step()\n        else:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n\n    swa_model.to(device)\n    update_bn(loader, swa_model, device=device)\n\n    # Validation TTA + smoothing\n    swa_model.train()\n    X_va_t = torch.tensor(X_va_s, dtype=torch.float32).to(device)\n    sum_pred = np.zeros((X_va_s.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_va_t + torch.randn_like(X_va_t) * tta_noise\n            sum_pred += swa_model(xb_noisy).cpu().numpy()\n    avg_pca = sum_pred / num_tta\n    pred_s = pca.inverse_transform(avg_pca)\n    pred = scaler_y.inverse_transform(pred_s)\n    # apply triangular smoothing\n    pred_smooth = np.apply_along_axis(\n        lambda row: np.convolve(row, kernel, mode=\"same\"), axis=1, arr=pred\n    )\n    mse = np.mean((pred_smooth - y_va) ** 2)\n    print(f\"Fold {fold} Val MSE: {mse:.6f}\")\n    oof_preds[va_idx] = pred_smooth\n\n    # Test TTA + smoothing\n    swa_model.train()\n    X_te_t = torch.tensor(X_te_s, dtype=torch.float32).to(device)\n    sum_test = np.zeros((g_test.shape[0], pca_dims), np.float32)\n    with torch.no_grad():\n        for _ in range(num_tta):\n            xb_noisy = X_te_t + torch.randn_like(X_te_t) * tta_noise\n            sum_test += swa_model(xb_noisy).cpu().numpy()\n    avg_test = sum_test / num_tta\n    pred_te_s = pca.inverse_transform(avg_test)\n    pred_te = scaler_y.inverse_transform(pred_te_s)\n    pred_te_smooth = np.apply_along_axis(\n        lambda row: np.convolve(row, kernel, mode=\"same\"), axis=1, arr=pred_te\n    )\n    test_preds += pred_te_smooth\n\n# Overall OOF\noof_mse = np.mean((oof_preds - y_all) ** 2)\nprint(f\"OOF MSE: {oof_mse:.6f}\")\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\npd.DataFrame(test_preds / n_splits).to_csv(\n    \"./working/submission.csv\", index=False, header=False\n)\nprint(\"Saved test predictions to ./working/submission.csv\")"], "term_out": ["Validation MSE: 2.983489e-03\nExecution time: a minute seconds (time limit is 10 hours).", "Epoch 1/50, Train MSE: 0.027658\nEpoch 10/50, Train MSE: 0.004953\nEpoch 20/50, Train MSE: 0.004150\nEpoch 30/50, Train MSE: 0.003729\nEpoch 40/50, Train MSE: 0.003452\nEpoch 50/50, Train MSE: 0.003276\nValidation MSE: 0.002894\nSaved test predictions to ./working/submission.csv\nExecution time: a minute seconds (time limit is 10 hours).", "Epoch 01: val MSE = 0.007053\nEpoch 02: val MSE = 0.006552\nEpoch 03: val MSE = 0.006356\nEpoch 04: val MSE = 0.006251\nEpoch 05: val MSE = 0.006220\nEpoch 06: val MSE = 0.006170\nEpoch 07: val MSE = 0.006052\nEpoch 08: val MSE = 0.006099\nEpoch 09: val MSE = 0.006074\nEpoch 10: val MSE = 0.005988\nEpoch 11: val MSE = 0.005987\nEpoch 12: val MSE = 0.005956\nEpoch 13: val MSE = 0.006136\nEpoch 14: val MSE = 0.006005\nEpoch 15: val MSE = 0.005908\nEpoch 16: val MSE = 0.005932\nEpoch 17: val MSE = 0.005926\nEpoch 18: val MSE = 0.005871\nEpoch 19: val MSE = 0.005914\nEpoch 20: val MSE = 0.005976\nEpoch 21: val MSE = 0.005947\nEpoch 22: val MSE = 0.005884\nEpoch 23: val MSE = 0.005993\nEpoch 24: val MSE = 0.005850\nEpoch 25: val MSE = 0.005852\nEpoch 26: val MSE = 0.005838\nEpoch 27: val MSE = 0.005857\nEpoch 28: val MSE = 0.005833\nEpoch 29: val MSE = 0.005907\nEpoch 30: val MSE = 0.005824\nEpoch 31: val MSE = 0.005836\nEpoch 32: val MSE = 0.005839\nEpoch 33: val MSE = 0.005798\nEpoch 34: val MSE = 0.005846\nEpoch 35: val MSE = 0.005808\nEpoch 36: val MSE = 0.005886\nEpoch 37: val MSE = 0.005824\nEpoch 38: val MSE = 0.005836\nEpoch 39: val MSE = 0.005827\nEpoch 40: val MSE = 0.005815\nEpoch 41: val MSE = 0.005832\nEpoch 42: val MSE = 0.005801\nEpoch 43: val MSE = 0.005808\nEpoch 44: val MSE = 0.005803\nEpoch 45: val MSE = 0.005851\nEpoch 46: val MSE = 0.005863\nEpoch 47: val MSE = 0.005803\nEpoch 48: val MSE = 0.005785\nEpoch 49: val MSE = 0.005841\nEpoch 50: val MSE = 0.005834\nFinal validation MSE: 0.005785\nExecution time: 2 minutes seconds (time limit is 10 hours).", "Validation MSE: 0.004699\nExecution time: 2 minutes seconds (time limit is 10 hours).", "Validation MSE: 0.003407\nExecution time: 2 minutes seconds (time limit is 10 hours).", "Epoch 1/50, Train MSE: 0.025273\nEpoch 10/50, Train MSE: 0.005381\nEpoch 20/50, Train MSE: 0.005149\nEpoch 30/50, Train MSE: 0.005070\nEpoch 40/50, Train MSE: 0.005028\nEpoch 50/50, Train MSE: 0.004977\nValidation MSE: 0.004620\nSaved test predictions to ./working/submission.csv\nExecution time: a minute seconds (time limit is 10 hours).", "Epoch 1/50 - Train MSE: 0.027658, Val MSE: 0.008699\nEpoch 10/50 - Train MSE: 0.004953, Val MSE: 0.004684\nEpoch 20/50 - Train MSE: 0.004150, Val MSE: 0.003835\nEpoch 30/50 - Train MSE: 0.003729, Val MSE: 0.003338\nEpoch 40/50 - Train MSE: 0.003452, Val MSE: 0.003050\nEpoch 50/50 - Train MSE: 0.003276, Val MSE: 0.002894\nBest Validation MSE: 0.002894\nSaved test predictions to ./working/submission.csv\nExecution time: a minute seconds (time limit is 10 hours).", "Epoch 1/50, Train MSE: 0.027658, Val MSE: 0.008699\nEpoch 10/50, Train MSE: 0.004953, Val MSE: 0.004684\nEpoch 20/50, Train MSE: 0.004150, Val MSE: 0.003835\nEpoch 30/50, Train MSE: 0.003729, Val MSE: 0.003338\nEpoch 40/50, Train MSE: 0.003452, Val MSE: 0.003050\nEpoch 50/50, Train MSE: 0.003276, Val MSE: 0.002894\nFinal Validation MSE: 0.002894\nSaved test predictions to ./working/submission.csv\nExecution time: a minute seconds (time limit is 10 hours).", "Epoch 1/50, Train MSE: 0.025273\nEpoch 10/50, Train MSE: 0.005381\nEpoch 20/50, Train MSE: 0.005149\nEpoch 30/50, Train MSE: 0.005070\nEpoch 40/50, Train MSE: 0.005028\nEpoch 50/50, Train MSE: 0.004977\nValidation MSE: 0.004620\nSaved test predictions to ./working/submission.csv\nExecution time: a minute seconds (time limit is 10 hours).", "Epoch 1/50, Train MSE: 0.213317\nEpoch 10/50, Train MSE: 0.006392\nEpoch 20/50, Train MSE: 0.004637\nEpoch 30/50, Train MSE: 0.003933\nEpoch 40/50, Train MSE: 0.003492\nEpoch 50/50, Train MSE: 0.003326\nValidation MSE: 0.003017\nSaved test predictions to ./working/submission.csv\nExecution time: a minute seconds (time limit is 10 hours).", "Epoch 1/50, Train MSE: 0.027658, Val MSE: 0.008699\nEpoch 10/50, Train MSE: 0.004953, Val MSE: 0.004684\nEpoch 20/50, Train MSE: 0.004150, Val MSE: 0.003835\nEpoch 30/50, Train MSE: 0.003729, Val MSE: 0.003338\nEpoch 40/50, Train MSE: 0.003452, Val MSE: 0.003050\nEpoch 50/50, Train MSE: 0.003276, Val MSE: 0.002894\nBest Validation MSE: 0.002846\nSaved test predictions to ./working/submission.csv\nExecution time: a minute seconds (time limit is 10 hours).", "Epoch 1/50, Train MSE: 0.024707, Val MSE: 0.007300\nEpoch 10/50, Train MSE: 0.005138, Val MSE: 0.004800\nEpoch 20/50, Train MSE: 0.004390, Val MSE: 0.004133\nEpoch 30/50, Train MSE: 0.003947, Val MSE: 0.003537\nEpoch 40/50, Train MSE: 0.003691, Val MSE: 0.003195\nEpoch 50/50, Train MSE: 0.003504, Val MSE: 0.003053\nBest Validation MSE: 0.003053\nSaved test predictions to ./working/submission.csv\nExecution time: a minute seconds (time limit is 10 hours).", "Fold 1 Best Val MSE: 0.002863\nFold 2 Best Val MSE: 0.002899\nFold 3 Best Val MSE: 0.002912\nFold 4 Best Val MSE: 0.002943\nFold 5 Best Val MSE: 0.002881\nCV MSE: 0.002900\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Epoch 1/50, LR: 0.000999, Train MSE: 0.027658, Val MSE: 0.008699\nEpoch 10/50, LR: 0.000905, Train MSE: 0.004942, Val MSE: 0.004649\nEpoch 20/50, LR: 0.000658, Train MSE: 0.004119, Val MSE: 0.003796\nEpoch 30/50, LR: 0.000352, Train MSE: 0.003676, Val MSE: 0.003263\nEpoch 40/50, LR: 0.000105, Train MSE: 0.003405, Val MSE: 0.003020\nEpoch 50/50, LR: 0.000010, Train MSE: 0.003307, Val MSE: 0.002994\nBest Validation MSE: 0.002963\nSaved test predictions to ./working/submission.csv\nExecution time: a minute seconds (time limit is 10 hours).", "Epoch 1/50, Train MSE: 0.019393, Val MSE: 0.009114\nEpoch 10/50, Train MSE: 0.004521, Val MSE: 0.004430\nEpoch 20/50, Train MSE: 0.003493, Val MSE: 0.003352\nEpoch 30/50, Train MSE: 0.003080, Val MSE: 0.002981\nEpoch 40/50, Train MSE: 0.002836, Val MSE: 0.002747\nEpoch 50/50, Train MSE: 0.002692, Val MSE: 0.002646\nBest Validation MSE: 0.002646\nSaved test predictions to ./working/submission.csv\nExecution time: a minute seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.002640\nFold 2 Val MSE: 0.002625\nFold 3 Val MSE: 0.002626\nFold 4 Val MSE: 0.002654\nFold 5 Val MSE: 0.002664\nOOF MSE: 0.002642\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.002641\nFold 2 Val MSE: 0.002606\nFold 3 Val MSE: 0.002665\nFold 4 Val MSE: 0.002629\nFold 5 Val MSE: 0.002659\nOOF MSE: 0.002640\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.002371\nFold 2 Val MSE: 0.002422\nFold 3 Val MSE: 0.002424\nFold 4 Val MSE: 0.002384\nFold 5 Val MSE: 0.002414\nOOF MSE: 0.002403\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.002470\nFold 2 Val MSE: 0.002482\nFold 3 Val MSE: 0.002473\nFold 4 Val MSE: 0.002472\nFold 5 Val MSE: 0.002456\nOOF MSE: 0.002471\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (SWA): 0.002350\nFold 2 Val MSE (SWA): 0.002391\nFold 3 Val MSE (SWA): 0.002398\nFold 4 Val MSE (SWA): 0.002365\nFold 5 Val MSE (SWA): 0.002386\nOOF MSE (SWA): 0.002378\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (SWA+Clipping): 0.002350\nFold 2 Val MSE (SWA+Clipping): 0.002389\nFold 3 Val MSE (SWA+Clipping): 0.002399\nFold 4 Val MSE (SWA+Clipping): 0.002365\nFold 5 Val MSE (SWA+Clipping): 0.002386\nOOF MSE (SWA+Clipping): 0.002378\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.002478\nFold 2 Val MSE: 0.002505\nFold 3 Val MSE: 0.002532\nFold 4 Val MSE: 0.002496\nFold 5 Val MSE: 0.002507\nOOF MSE: 0.002503\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (SWA + clipping): 0.002350\nFold 2 Val MSE (SWA + clipping): 0.002389\nFold 3 Val MSE (SWA + clipping): 0.002399\nFold 4 Val MSE (SWA + clipping): 0.002365\nFold 5 Val MSE (SWA + clipping): 0.002386\nOOF MSE (SWA + clipping): 0.002378\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (SWA + Clipping): 0.002350\nFold 2 Val MSE (SWA + Clipping): 0.002389\nFold 3 Val MSE (SWA + Clipping): 0.002399\nFold 4 Val MSE (SWA + Clipping): 0.002365\nFold 5 Val MSE (SWA + Clipping): 0.002386\nOOF MSE (SWA + Clipping): 0.002378\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (SWA+Poly): 0.002266\nFold 2 Val MSE (SWA+Poly): 0.002275\nFold 3 Val MSE (SWA+Poly): 0.002291\nFold 4 Val MSE (SWA+Poly): 0.002262\nFold 5 Val MSE (SWA+Poly): 0.002283\nOOF MSE (SWA+Poly): 0.002275\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (with noise): 0.002275\nFold 2 Val MSE (with noise): 0.002276\nFold 3 Val MSE (with noise): 0.002287\nFold 4 Val MSE (with noise): 0.002274\nFold 5 Val MSE (with noise): 0.002299\nOOF MSE (with noise): 0.002282\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (SWA+Poly+Noise): 0.002275\nFold 2 Val MSE (SWA+Poly+Noise): 0.002276\nFold 3 Val MSE (SWA+Poly+Noise): 0.002287\nFold 4 Val MSE (SWA+Poly+Noise): 0.002274\nFold 5 Val MSE (SWA+Poly+Noise): 0.002299\nOOF MSE (SWA+Poly+Noise): 0.002282\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.002275\nFold 2 Val MSE: 0.002276\nFold 3 Val MSE: 0.002287\nFold 4 Val MSE: 0.002274\nFold 5 Val MSE: 0.002299\nOOF MSE: 0.002282\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (mixup+SWA+Poly): 0.002545\nFold 2 Val MSE (mixup+SWA+Poly): 0.002542\nFold 3 Val MSE (mixup+SWA+Poly): 0.002560\nFold 4 Val MSE (mixup+SWA+Poly): 0.002510\nFold 5 Val MSE (mixup+SWA+Poly): 0.002550\nOOF MSE (mixup+SWA+Poly): 0.002541\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.002526\nFold 2 Val MSE: 0.002537\nFold 3 Val MSE: 0.002566\nFold 4 Val MSE: 0.002537\nFold 5 Val MSE: 0.002557\nOOF MSE: 0.002545\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (SWA+Poly3): 0.002270\nFold 2 Val MSE (SWA+Poly3): 0.002289\nFold 3 Val MSE (SWA+Poly3): 0.002282\nFold 4 Val MSE (SWA+Poly3): 0.002292\nFold 5 Val MSE (SWA+Poly3): 0.002310\nOOF MSE (SWA+Poly3): 0.002289\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Traceback (most recent call last):\n  File \"runfile.py\", line 104, in <module>\n    for xb, yb in train_loader:\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 708, in __next__\n    data = self._next_data()\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1480, in _next_data\n    return self._process_data(data)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1505, in _process_data\n    data.reraise()\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/site-packages/torch/_utils.py\", line 733, in reraise\n    raise exception\nRuntimeError: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/site-packages/torch/utils/data/dataset.py\", line 211, in __getitem__\n    return tuple(tensor[index] for tensor in self.tensors)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/site-packages/torch/utils/data/dataset.py\", line 211, in <genexpr>\n    return tuple(tensor[index] for tensor in self.tensors)\n                 ~~~~~~^^^^^^^\nRuntimeError: CUDA error: initialization error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n\nExecution time: 14 seconds seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.002291\nFold 2 Val MSE: 0.002283\nFold 3 Val MSE: 0.002288\nFold 4 Val MSE: 0.002269\nFold 5 Val MSE: 0.002298\nOOF MSE: 0.002286\nSaved test predictions to ./working/submission.csv\nExecution time: 6 minutes seconds (time limit is 10 hours).", "Traceback (most recent call last):\n  File \"runfile.py\", line 93, in <module>\n    swa_model = AveragedModel(model)\n                ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/site-packages/torch/optim/swa_utils.py\", line 230, in __init__\n    self.module = deepcopy(model)\n                  ^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/copy.py\", line 172, in deepcopy\n    y = _reconstruct(x, memo, *rv)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/copy.py\", line 271, in _reconstruct\n    state = deepcopy(state, memo)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/copy.py\", line 146, in deepcopy\n    y = copier(x, memo)\n        ^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/copy.py\", line 231, in _deepcopy_dict\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\n                             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/copy.py\", line 146, in deepcopy\n    y = copier(x, memo)\n        ^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/copy.py\", line 231, in _deepcopy_dict\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\n                             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/copy.py\", line 172, in deepcopy\n    y = _reconstruct(x, memo, *rv)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/copy.py\", line 271, in _reconstruct\n    state = deepcopy(state, memo)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/copy.py\", line 146, in deepcopy\n    y = copier(x, memo)\n        ^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/copy.py\", line 231, in _deepcopy_dict\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\n                             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/copy.py\", line 146, in deepcopy\n    y = copier(x, memo)\n        ^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/copy.py\", line 231, in _deepcopy_dict\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\n                             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/copy.py\", line 172, in deepcopy\n    y = _reconstruct(x, memo, *rv)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/copy.py\", line 271, in _reconstruct\n    state = deepcopy(state, memo)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/copy.py\", line 146, in deepcopy\n    y = copier(x, memo)\n        ^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/copy.py\", line 231, in _deepcopy_dict\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\n                             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/copy.py\", line 153, in deepcopy\n    y = copier(memo)\n        ^^^^^^^^^^^^\n  File \"/home/dl370/anaconda3/envs/aideml_real/lib/python3.11/site-packages/torch/_tensor.py\", line 114, in __deepcopy__\n    raise RuntimeError(\nRuntimeError: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment.  If you were attempting to deepcopy a module, this may be because of a torch.nn.utils.weight_norm usage, see https://github.com/pytorch/pytorch/pull/103001\nExecution time: 13 seconds seconds (time limit is 10 hours).", "Fold 1 Validation MSE: 0.002266\nFold 2 Validation MSE: 0.002275\nFold 3 Validation MSE: 0.002291\nFold 4 Validation MSE: 0.002262\nFold 5 Validation MSE: 0.002283\nOOF MSE: 0.002275\nSaved test predictions to ./working/submission.csv\nExecution time: 6 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (SWA+GELU): 0.002159\nFold 2 Val MSE (SWA+GELU): 0.002142\nFold 3 Val MSE (SWA+GELU): 0.002177\nFold 4 Val MSE (SWA+GELU): 0.002157\nFold 5 Val MSE (SWA+GELU): 0.002188\nOOF MSE (SWA+GELU): 0.002165\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.002317\nFold 2 Val MSE: 0.002295\nFold 3 Val MSE: 0.002314\nFold 4 Val MSE: 0.002300\nFold 5 Val MSE: 0.002316\nOOF MSE: 0.002308\nSaved test predictions to ./working/submission.csv\nExecution time: 6 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (noise aug): 0.002160\nFold 2 Val MSE (noise aug): 0.002154\nFold 3 Val MSE (noise aug): 0.002176\nFold 4 Val MSE (noise aug): 0.002157\nFold 5 Val MSE (noise aug): 0.002171\nOOF MSE (noise aug): 0.002163\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.002314\nFold 2 Val MSE: 0.002306\nFold 3 Val MSE: 0.002323\nFold 4 Val MSE: 0.002313\nFold 5 Val MSE: 0.002318\nOOF MSE: 0.002315\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (with pre-SWA Cosine): 0.002428\nFold 2 Val MSE (with pre-SWA Cosine): 0.002417\nFold 3 Val MSE (with pre-SWA Cosine): 0.002449\nFold 4 Val MSE (with pre-SWA Cosine): 0.002442\nFold 5 Val MSE (with pre-SWA Cosine): 0.002458\nOOF MSE (with pre-SWA Cosine): 0.002439\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE with TTA: 0.002160\nFold 2 Val MSE with TTA: 0.002155\nFold 3 Val MSE with TTA: 0.002164\nFold 4 Val MSE with TTA: 0.002162\nFold 5 Val MSE with TTA: 0.002174\nOverall OOF MSE with TTA: 0.002163\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (TTA): 0.002160\nFold 2 Val MSE (TTA): 0.002155\nFold 3 Val MSE (TTA): 0.002164\nFold 4 Val MSE (TTA): 0.002162\nFold 5 Val MSE (TTA): 0.002174\nOOF MSE (TTA): 0.002163\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (TTA): 0.002160\nFold 2 Val MSE (TTA): 0.002155\nFold 3 Val MSE (TTA): 0.002164\nFold 4 Val MSE (TTA): 0.002162\nFold 5 Val MSE (TTA): 0.002174\nOOF MSE (with TTA): 0.002163\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (TTA): 0.002160\nFold 2 Val MSE (TTA): 0.002155\nFold 3 Val MSE (TTA): 0.002164\nFold 4 Val MSE (TTA): 0.002162\nFold 5 Val MSE (TTA): 0.002174\nOOF MSE (TTA): 0.002163\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (TTA): 0.002160\nFold 2 Val MSE (TTA): 0.002155\nFold 3 Val MSE (TTA): 0.002164\nFold 4 Val MSE (TTA): 0.002162\nFold 5 Val MSE (TTA): 0.002174\nOOF MSE (TTA): 0.002163\nSaved test predictions to ./working/submission.csv\nExecution time: 8 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (TTA): 0.002160\nFold 2 Val MSE (TTA): 0.002155\nFold 3 Val MSE (TTA): 0.002164\nFold 4 Val MSE (TTA): 0.002162\nFold 5 Val MSE (TTA): 0.002174\nOOF MSE (TTA): 0.002163\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.002428\nFold 2 Val MSE: 0.002417\nFold 3 Val MSE: 0.002449\nFold 4 Val MSE: 0.002442\nFold 5 Val MSE: 0.002458\nOOF MSE: 0.002439\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE with TTA: 0.002160\nFold 2 Val MSE with TTA: 0.002155\nFold 3 Val MSE with TTA: 0.002164\nFold 4 Val MSE with TTA: 0.002162\nFold 5 Val MSE with TTA: 0.002174\nOOF MSE with TTA: 0.002163\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (TTA): 0.002160\nFold 2 Val MSE (TTA): 0.002185\nFold 3 Val MSE (TTA): 0.002181\nFold 4 Val MSE (TTA): 0.002177\nFold 5 Val MSE (TTA): 0.002194\nOOF MSE (TTA): 0.002179\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (PCA targets): 0.002180\nFold 2 Val MSE (PCA targets): 0.002178\nFold 3 Val MSE (PCA targets): 0.002175\nFold 4 Val MSE (PCA targets): 0.002166\nFold 5 Val MSE (PCA targets): 0.002169\nOOF MSE (PCA targets): 0.002174\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (BatchNorm): 0.002414\nFold 2 Val MSE (BatchNorm): 0.002415\nFold 3 Val MSE (BatchNorm): 0.002405\nFold 4 Val MSE (BatchNorm): 0.002396\nFold 5 Val MSE (BatchNorm): 0.002420\nOOF MSE (BatchNorm): 0.002410\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (MC Dropout TTA): 0.002192\nFold 2 Val MSE (MC Dropout TTA): 0.002178\nFold 3 Val MSE (MC Dropout TTA): 0.002199\nFold 4 Val MSE (MC Dropout TTA): 0.002201\nFold 5 Val MSE (MC Dropout TTA): 0.002219\nOOF MSE (MC Dropout TTA): 0.002198\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (PCA): 0.002416\nFold 2 Val MSE (PCA): 0.002360\nFold 3 Val MSE (PCA): 0.002379\nFold 4 Val MSE (PCA): 0.002374\nFold 5 Val MSE (PCA): 0.002393\nOOF MSE (PCA): 0.002385\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (TTA): 0.002160\nFold 2 Val MSE (TTA): 0.002155\nFold 3 Val MSE (TTA): 0.002164\nFold 4 Val MSE (TTA): 0.002162\nFold 5 Val MSE (TTA): 0.002174\nOOF MSE (TTA): 0.002163\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (TTA): 0.002160\nFold 2 Val MSE (TTA): 0.002155\nFold 3 Val MSE (TTA): 0.002164\nFold 4 Val MSE (TTA): 0.002162\nFold 5 Val MSE (TTA): 0.002174\nOOF MSE (TTA): 0.002163\nSaved test predictions to ./working/submission.csv\nExecution time: 7 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001965\nFold 2 Val MSE: 0.001945\nFold 3 Val MSE: 0.001960\nFold 4 Val MSE: 0.001940\nFold 5 Val MSE: 0.001967\nOOF MSE: 0.001956\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (TTA): 0.001966\nFold 2 Val MSE (TTA): 0.001950\nFold 3 Val MSE (TTA): 0.001946\nFold 4 Val MSE (TTA): 0.001940\nFold 5 Val MSE (TTA): 0.001997\nOOF MSE (TTA): 0.001960\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (TTA): 0.001966\nFold 2 Val MSE (TTA): 0.001950\nFold 3 Val MSE (TTA): 0.001946\nFold 4 Val MSE (TTA): 0.001940\nFold 5 Val MSE (TTA): 0.001997\nOOF MSE (TTA): 0.001960\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001966\nFold 2 Val MSE: 0.001950\nFold 3 Val MSE: 0.001946\nFold 4 Val MSE: 0.001940\nFold 5 Val MSE: 0.001997\nOOF MSE: 0.001960\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001989\nFold 2 Val MSE: 0.001965\nFold 3 Val MSE: 0.001968\nFold 4 Val MSE: 0.001962\nFold 5 Val MSE: 0.001995\nOOF MSE: 0.001976\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE with TTA: 0.001966\nFold 2 Val MSE with TTA: 0.001950\nFold 3 Val MSE with TTA: 0.001946\nFold 4 Val MSE with TTA: 0.001940\nFold 5 Val MSE with TTA: 0.001997\nOverall OOF MSE with TTA: 0.001960\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (TTA): 0.001966\nFold 2 Val MSE (TTA): 0.001950\nFold 3 Val MSE (TTA): 0.001946\nFold 4 Val MSE (TTA): 0.001940\nFold 5 Val MSE (TTA): 0.001997\nOOF MSE (TTA): 0.001960\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.002257\nFold 2 Val MSE: 0.002268\nFold 3 Val MSE: 0.002255\nFold 4 Val MSE: 0.002262\nFold 5 Val MSE: 0.002272\nOOF MSE: 0.002263\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001966\nFold 2 Val MSE: 0.001959\nFold 3 Val MSE: 0.001963\nFold 4 Val MSE: 0.001933\nFold 5 Val MSE: 0.001961\nOOF MSE: 0.001957\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001990\nFold 2 Val MSE: 0.001969\nFold 3 Val MSE: 0.001978\nFold 4 Val MSE: 0.001963\nFold 5 Val MSE: 0.001988\nOOF MSE: 0.001978\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE with TTA: 0.001966\nFold 2 Val MSE with TTA: 0.001959\nFold 3 Val MSE with TTA: 0.001963\nFold 4 Val MSE with TTA: 0.001933\nFold 5 Val MSE with TTA: 0.001961\nOOF MSE with TTA: 0.001957\nSaved test predictions with TTA to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001966\nFold 2 Val MSE: 0.001950\nFold 3 Val MSE: 0.001946\nFold 4 Val MSE: 0.001940\nFold 5 Val MSE: 0.001997\nOOF MSE: 0.001960\nSaved test predictions to ./working/submission.csv\nExecution time: 15 minutes seconds (time limit is 10 hours).", "Fold 1 TTA Val MSE: 0.001966\nFold 2 TTA Val MSE: 0.001959\nFold 3 TTA Val MSE: 0.001963\nFold 4 TTA Val MSE: 0.001933\nFold 5 TTA Val MSE: 0.001961\nOOF MSE with TTA: 0.001957\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001966\nFold 2 Val MSE: 0.001950\nFold 3 Val MSE: 0.001946\nFold 4 Val MSE: 0.001940\nFold 5 Val MSE: 0.001997\nOOF MSE: 0.001960\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.002011\nFold 2 Val MSE: 0.001981\nFold 3 Val MSE: 0.002006\nFold 4 Val MSE: 0.001995\nFold 5 Val MSE: 0.002021\nOOF MSE: 0.002003\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (TTA): 0.001966\nFold 2 Val MSE (TTA): 0.001950\nFold 3 Val MSE (TTA): 0.001946\nFold 4 Val MSE (TTA): 0.001940\nFold 5 Val MSE (TTA): 0.001997\nOOF MSE (TTA): 0.001960\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (TTA): 0.001966\nFold 2 Val MSE (TTA): 0.001959\nFold 3 Val MSE (TTA): 0.001963\nFold 4 Val MSE (TTA): 0.001933\nFold 5 Val MSE (TTA): 0.001961\nOOF MSE (with TTA): 0.001957\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001966\nFold 2 Val MSE: 0.001950\nFold 3 Val MSE: 0.001946\nFold 4 Val MSE: 0.001940\nFold 5 Val MSE: 0.001997\nOOF MSE: 0.001960\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.002257\nFold 2 Val MSE: 0.002268\nFold 3 Val MSE: 0.002255\nFold 4 Val MSE: 0.002262\nFold 5 Val MSE: 0.002272\nOOF MSE: 0.002263\nSaved test predictions to ./working/submission.csv\nExecution time: 15 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE with TTA: 0.001966\nFold 2 Val MSE with TTA: 0.001950\nFold 3 Val MSE with TTA: 0.001946\nFold 4 Val MSE with TTA: 0.001940\nFold 5 Val MSE with TTA: 0.001997\nOverall OOF MSE with TTA: 0.001960\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001966\nFold 2 Val MSE: 0.001950\nFold 3 Val MSE: 0.001946\nFold 4 Val MSE: 0.001940\nFold 5 Val MSE: 0.001997\nOOF MSE: 0.001960\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE with TTA: 0.001966\nFold 2 Val MSE with TTA: 0.001950\nFold 3 Val MSE with TTA: 0.001946\nFold 4 Val MSE with TTA: 0.001940\nFold 5 Val MSE with TTA: 0.001997\nOOF MSE with TTA: 0.001960\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.002011\nFold 2 Val MSE: 0.001981\nFold 3 Val MSE: 0.002006\nFold 4 Val MSE: 0.001995\nFold 5 Val MSE: 0.002021\nOOF MSE: 0.002003\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001966\nFold 2 Val MSE: 0.001959\nFold 3 Val MSE: 0.001963\nFold 4 Val MSE: 0.001933\nFold 5 Val MSE: 0.001961\nOOF MSE: 0.001957\nSaved test predictions to ./working/submission.csv\nExecution time: 17 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001966\nFold 2 Val MSE: 0.001940\nFold 3 Val MSE: 0.001961\nFold 4 Val MSE: 0.001940\nFold 5 Val MSE: 0.001967\nOOF MSE: 0.001955\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001967\nFold 2 Val MSE: 0.001949\nFold 3 Val MSE: 0.001946\nFold 4 Val MSE: 0.001941\nFold 5 Val MSE: 0.001996\nOOF MSE: 0.001960\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE with TTA: 0.001967\nFold 2 Val MSE with TTA: 0.001949\nFold 3 Val MSE with TTA: 0.001946\nFold 4 Val MSE with TTA: 0.001941\nFold 5 Val MSE with TTA: 0.001996\nOOF MSE with TTA: 0.001960\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.002254\nFold 2 Val MSE: 0.002269\nFold 3 Val MSE: 0.002256\nFold 4 Val MSE: 0.002263\nFold 5 Val MSE: 0.002273\nOOF MSE: 0.002263\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001967\nFold 2 Val MSE: 0.001949\nFold 3 Val MSE: 0.001946\nFold 4 Val MSE: 0.001941\nFold 5 Val MSE: 0.001996\nOOF MSE: 0.001960\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE with TTA: 0.001967\nFold 2 Val MSE with TTA: 0.001949\nFold 3 Val MSE with TTA: 0.001946\nFold 4 Val MSE with TTA: 0.001941\nFold 5 Val MSE with TTA: 0.001996\nOOF MSE with TTA: 0.001960\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001967\nFold 2 Val MSE: 0.001949\nFold 3 Val MSE: 0.001946\nFold 4 Val MSE: 0.001941\nFold 5 Val MSE: 0.001996\nOOF MSE: 0.001960\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (TTA): 0.001967\nFold 2 Val MSE (TTA): 0.001949\nFold 3 Val MSE (TTA): 0.001946\nFold 4 Val MSE (TTA): 0.001941\nFold 5 Val MSE (TTA): 0.001996\nOOF MSE (TTA): 0.001960\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.002011\nFold 2 Val MSE: 0.001981\nFold 3 Val MSE: 0.002006\nFold 4 Val MSE: 0.001994\nFold 5 Val MSE: 0.002019\nOOF MSE: 0.002002\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (TTA): 0.001967\nFold 2 Val MSE (TTA): 0.001949\nFold 3 Val MSE (TTA): 0.001946\nFold 4 Val MSE (TTA): 0.001941\nFold 5 Val MSE (TTA): 0.001996\nOOF MSE (TTA): 0.001960\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.002011\nFold 2 Val MSE: 0.001981\nFold 3 Val MSE: 0.002006\nFold 4 Val MSE: 0.001994\nFold 5 Val MSE: 0.002019\nOOF MSE: 0.002002\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.002254\nFold 2 Val MSE: 0.002269\nFold 3 Val MSE: 0.002256\nFold 4 Val MSE: 0.002263\nFold 5 Val MSE: 0.002273\nOOF MSE: 0.002263\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001836\nFold 2 Val MSE: 0.001826\nFold 3 Val MSE: 0.001815\nFold 4 Val MSE: 0.001830\nFold 5 Val MSE: 0.001840\nOOF MSE: 0.001830\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001837\nFold 2 Val MSE: 0.001827\nFold 3 Val MSE: 0.001820\nFold 4 Val MSE: 0.001818\nFold 5 Val MSE: 0.001825\nOOF MSE: 0.001825\nSaved test predictions to ./working/submission.csv\nExecution time: 16 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001794\nFold 2 Val MSE: 0.001794\nFold 3 Val MSE: 0.001747\nFold 4 Val MSE: 0.001700\nFold 5 Val MSE: 0.001727\nOOF MSE: 0.001752\nSaved test predictions to ./working/submission.csv\nExecution time: 20 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001746\nFold 2 Val MSE: 0.001793\nFold 3 Val MSE: 0.001723\nFold 4 Val MSE: 0.001715\nFold 5 Val MSE: 0.001760\nOOF MSE: 0.001748\nSaved test predictions to ./working/submission.csv\nExecution time: 20 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001729\nFold 2 Val MSE: 0.001738\nFold 3 Val MSE: 0.001705\nFold 4 Val MSE: 0.001697\nFold 5 Val MSE: 0.001718\nOOF MSE: 0.001717\nSaved test predictions to ./working/submission.csv\nExecution time: 20 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001702\nFold 2 Val MSE: 0.001701\nFold 3 Val MSE: 0.001665\nFold 4 Val MSE: 0.001646\nFold 5 Val MSE: 0.001715\nOOF MSE: 0.001686\nSaved test predictions to ./working/submission.csv\nExecution time: 20 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001688\nFold 2 Val MSE: 0.001675\nFold 3 Val MSE: 0.001664\nFold 4 Val MSE: 0.001657\nFold 5 Val MSE: 0.001719\nOOF MSE: 0.001680\nSaved test predictions to ./working/submission.csv\nExecution time: 20 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001687\nFold 2 Val MSE: 0.001669\nFold 3 Val MSE: 0.001684\nFold 4 Val MSE: 0.001679\nFold 5 Val MSE: 0.001676\nOOF MSE: 0.001679\nSaved test predictions to ./working/submission.csv\nExecution time: 19 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001658\nFold 2 Val MSE: 0.001712\nFold 3 Val MSE: 0.001694\nFold 4 Val MSE: 0.001709\nFold 5 Val MSE: 0.001677\nOOF MSE: 0.001690\nSaved test predictions to ./working/submission.csv\nExecution time: 20 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001681\nFold 2 Val MSE: 0.001685\nFold 3 Val MSE: 0.001668\nFold 4 Val MSE: 0.001670\nFold 5 Val MSE: 0.001671\nOOF MSE: 0.001675\nSaved test predictions to ./working/submission.csv\nExecution time: 19 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001681\nFold 2 Val MSE: 0.001665\nFold 3 Val MSE: 0.001671\nFold 4 Val MSE: 0.001680\nFold 5 Val MSE: 0.001680\nOOF MSE: 0.001676\nSaved test predictions to ./working/submission.csv\nExecution time: 20 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001646\nFold 2 Val MSE: 0.001642\nFold 3 Val MSE: 0.001646\nFold 4 Val MSE: 0.001640\nFold 5 Val MSE: 0.001612\nOOF MSE: 0.001637\nSaved test predictions to ./working/submission.csv\nExecution time: 21 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001525\nFold 2 Val MSE: 0.001553\nFold 3 Val MSE: 0.001517\nFold 4 Val MSE: 0.001537\nFold 5 Val MSE: 0.001526\nOOF MSE: 0.001532\nSaved test predictions to ./working/submission.csv\nExecution time: 21 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001542\nFold 2 Val MSE: 0.001553\nFold 3 Val MSE: 0.001502\nFold 4 Val MSE: 0.001522\nFold 5 Val MSE: 0.001514\nOOF MSE: 0.001527\nSaved test predictions to ./working/submission.csv\nExecution time: 21 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001540\nFold 2 Val MSE: 0.001542\nFold 3 Val MSE: 0.001518\nFold 4 Val MSE: 0.001488\nFold 5 Val MSE: 0.001491\nOOF MSE: 0.001516\nSaved test predictions to ./working/submission.csv\nExecution time: 22 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001530\nFold 2 Val MSE: 0.001531\nFold 3 Val MSE: 0.001525\nFold 4 Val MSE: 0.001497\nFold 5 Val MSE: 0.001494\nOOF MSE: 0.001516\nSaved test predictions to ./working/submission.csv\nExecution time: 23 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001533\nFold 2 Val MSE: 0.001538\nFold 3 Val MSE: 0.001530\nFold 4 Val MSE: 0.001500\nFold 5 Val MSE: 0.001495\nOOF MSE: 0.001519\nSaved test predictions to ./working/submission.csv\nExecution time: 22 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001538\nFold 2 Val MSE: 0.001551\nFold 3 Val MSE: 0.001520\nFold 4 Val MSE: 0.001532\nFold 5 Val MSE: 0.001470\nOOF MSE: 0.001522\nSaved test predictions to ./working/submission.csv\nExecution time: 26 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001704\nFold 2 Val MSE: 0.001713\nFold 3 Val MSE: 0.001635\nFold 4 Val MSE: 0.001688\nFold 5 Val MSE: 0.001735\nOOF MSE: 0.001695\nSaved test predictions to ./working/submission.csv\nExecution time: 24 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001510\nFold 2 Val MSE: 0.001517\nFold 3 Val MSE: 0.001511\nFold 4 Val MSE: 0.001503\nFold 5 Val MSE: 0.001490\nOOF MSE: 0.001506\nSaved test predictions to ./working/submission.csv\nExecution time: 24 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001700\nFold 2 Val MSE: 0.001696\nFold 3 Val MSE: 0.001713\nFold 4 Val MSE: 0.001703\nFold 5 Val MSE: 0.001671\nOOF MSE: 0.001697\nSaved test predictions to ./working/submission.csv\nExecution time: 24 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001510\nFold 2 Val MSE: 0.001539\nFold 3 Val MSE: 0.001511\nFold 4 Val MSE: 0.001513\nFold 5 Val MSE: 0.001501\nOOF MSE: 0.001515\nSaved test predictions to ./working/submission.csv\nExecution time: 23 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001522\nFold 2 Val MSE: 0.001475\nFold 3 Val MSE: 0.001452\nFold 4 Val MSE: 0.001483\nFold 5 Val MSE: 0.001473\nOOF MSE: 0.001481\nSaved test predictions to ./working/submission.csv\nExecution time: 22 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001555\nFold 2 Val MSE: 0.001520\nFold 3 Val MSE: 0.001493\nFold 4 Val MSE: 0.001519\nFold 5 Val MSE: 0.001513\nOOF MSE: 0.001520\nSaved test predictions to ./working/submission.csv\nExecution time: 22 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001555\nFold 2 Val MSE: 0.001520\nFold 3 Val MSE: 0.001493\nFold 4 Val MSE: 0.001519\nFold 5 Val MSE: 0.001513\nOOF MSE: 0.001520\nSaved test predictions to ./working/submission.csv\nExecution time: 22 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001521\nFold 2 Val MSE: 0.001496\nFold 3 Val MSE: 0.001465\nFold 4 Val MSE: 0.001456\nFold 5 Val MSE: 0.001481\nOOF MSE: 0.001484\nSaved test predictions to ./working/submission.csv\nExecution time: 24 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001522\nFold 2 Val MSE: 0.001475\nFold 3 Val MSE: 0.001452\nFold 4 Val MSE: 0.001483\nFold 5 Val MSE: 0.001473\nOOF MSE: 0.001481\nSaved smoothed test predictions to ./working/submission.csv\nExecution time: 22 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001555\nFold 2 Val MSE: 0.001520\nFold 3 Val MSE: 0.001493\nFold 4 Val MSE: 0.001519\nFold 5 Val MSE: 0.001513\nOOF MSE: 0.001520\nSaved test predictions to ./working/submission.csv\nExecution time: 22 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001486\nFold 2 Val MSE: 0.001505\nFold 3 Val MSE: 0.001480\nFold 4 Val MSE: 0.001484\nFold 5 Val MSE: 0.001507\nOOF MSE: 0.001492\nSaved test predictions to ./working/submission.csv\nExecution time: 22 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (smoothed): 0.001522\nFold 2 Val MSE (smoothed): 0.001475\nFold 3 Val MSE (smoothed): 0.001452\nFold 4 Val MSE (smoothed): 0.001483\nFold 5 Val MSE (smoothed): 0.001473\nOOF MSE (smoothed): 0.001481\nSaved smoothed test predictions to ./working/submission.csv\nExecution time: 22 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001507\nFold 2 Val MSE: 0.001526\nFold 3 Val MSE: 0.001501\nFold 4 Val MSE: 0.001505\nFold 5 Val MSE: 0.001528\nOOF MSE: 0.001513\nSaved test predictions to ./working/submission.csv\nExecution time: 22 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001382\nFold 2 Val MSE: 0.001389\nFold 3 Val MSE: 0.001350\nFold 4 Val MSE: 0.001345\nFold 5 Val MSE: 0.001347\nOOF MSE: 0.001362\nSaved test predictions to ./working/submission.csv\nExecution time: 22 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001403\nFold 2 Val MSE: 0.001410\nFold 3 Val MSE: 0.001371\nFold 4 Val MSE: 0.001366\nFold 5 Val MSE: 0.001368\nOOF MSE: 0.001383\nSaved test predictions to ./working/submission.csv\nExecution time: 22 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001359\nFold 2 Val MSE: 0.001355\nFold 3 Val MSE: 0.001348\nFold 4 Val MSE: 0.001356\nFold 5 Val MSE: 0.001366\nOOF MSE: 0.001357\nSaved test predictions to ./working/submission.csv\nExecution time: 22 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001486\nFold 2 Val MSE: 0.001505\nFold 3 Val MSE: 0.001480\nFold 4 Val MSE: 0.001484\nFold 5 Val MSE: 0.001507\nOOF MSE: 0.001492\nSaved test predictions to ./working/submission.csv\nExecution time: 22 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (smoothed): 0.001359\nFold 2 Val MSE (smoothed): 0.001355\nFold 3 Val MSE (smoothed): 0.001348\nFold 4 Val MSE (smoothed): 0.001356\nFold 5 Val MSE (smoothed): 0.001366\nOOF MSE (smoothed): 0.001357\nSaved smoothed test predictions to ./working/submission.csv\nExecution time: 22 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001359\nFold 2 Val MSE: 0.001355\nFold 3 Val MSE: 0.001348\nFold 4 Val MSE: 0.001356\nFold 5 Val MSE: 0.001366\nOOF MSE: 0.001357\nSaved test predictions to ./working/submission.csv\nExecution time: 22 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001359\nFold 2 Val MSE: 0.001355\nFold 3 Val MSE: 0.001348\nFold 4 Val MSE: 0.001356\nFold 5 Val MSE: 0.001366\nOOF MSE: 0.001357\nSaved test predictions to ./working/submission.csv\nExecution time: 22 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001380\nFold 2 Val MSE: 0.001376\nFold 3 Val MSE: 0.001369\nFold 4 Val MSE: 0.001377\nFold 5 Val MSE: 0.001387\nOOF MSE: 0.001378\nSaved test predictions to ./working/submission.csv\nExecution time: 22 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001372\nFold 2 Val MSE: 0.001384\nFold 3 Val MSE: 0.001356\nFold 4 Val MSE: 0.001386\nFold 5 Val MSE: 0.001379\nOOF MSE: 0.001375\nSaved test predictions to ./working/submission.csv\nExecution time: 22 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001277\nFold 2 Val MSE: 0.001303\nFold 3 Val MSE: 0.001263\nFold 4 Val MSE: 0.001320\nFold 5 Val MSE: 0.001262\nOOF MSE: 0.001285\nSaved test predictions to ./working/submission.csv\nExecution time: 28 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001277\nFold 2 Val MSE: 0.001303\nFold 3 Val MSE: 0.001263\nFold 4 Val MSE: 0.001320\nFold 5 Val MSE: 0.001262\nOOF MSE: 0.001285\nSaved test predictions to ./working/submission.csv\nExecution time: 28 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001277\nFold 2 Val MSE: 0.001303\nFold 3 Val MSE: 0.001263\nFold 4 Val MSE: 0.001320\nFold 5 Val MSE: 0.001262\nOOF MSE: 0.001285\nSaved smoothed test predictions to ./working/submission.csv\nExecution time: 27 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.015202\nFold 2 Val MSE: 0.015213\nFold 3 Val MSE: 0.015263\nFold 4 Val MSE: 0.015239\nFold 5 Val MSE: 0.015221\nOOF MSE: 0.015228\nSaved test predictions to ./working/submission.csv\nExecution time: 39 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001276\nFold 2 Val MSE: 0.001304\nFold 3 Val MSE: 0.001276\nFold 4 Val MSE: 0.001729\nFold 5 Val MSE: 0.001255\nOOF MSE: 0.001368\nSaved test predictions to ./working/submission.csv\nExecution time: 30 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001277\nFold 2 Val MSE: 0.001303\nFold 3 Val MSE: 0.001263\nFold 4 Val MSE: 0.001320\nFold 5 Val MSE: 0.001262\nOOF MSE: 0.001285\nSaved test predictions to ./working/submission.csv\nExecution time: 27 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE (smoothed): 0.001311\nFold 2 Val MSE (smoothed): 0.001338\nFold 3 Val MSE (smoothed): 0.001297\nFold 4 Val MSE (smoothed): 0.001354\nFold 5 Val MSE (smoothed): 0.001296\nOOF MSE (smoothed): 0.001319\nSaved test predictions to ./working/submission.csv\nExecution time: 27 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001474\nFold 2 Val MSE: 0.001389\nFold 3 Val MSE: 0.001431\nFold 4 Val MSE: 0.001439\nFold 5 Val MSE: 0.001436\nOOF MSE: 0.001434\nSaved test predictions to ./working/submission.csv\nExecution time: 27 minutes seconds (time limit is 10 hours).", "Fold 1 Val MSE: 0.001298\nFold 2 Val MSE: 0.001324\nFold 3 Val MSE: 0.001284\nFold 4 Val MSE: 0.001341\nFold 5 Val MSE: 0.001283\nOOF MSE: 0.001306\nSaved test predictions to ./working/submission.csv\nExecution time: 27 minutes seconds (time limit is 10 hours)."], "analysis": ["A 3-layer MLP with two hidden layers of size 512 trained for 50 epochs achieved a validation MSE of approximately 2.98e-3. The training converged without errors, demonstrating reasonable predictive performance on the multivariate regression task.", "The residual MLP model demonstrates consistent learning, reducing training MSE from 0.027658 at epoch 1 to 0.003276 at epoch 50. On the validation set, it achieves an MSE of 0.002894, indicating solid generalization performance. The architecture and training setup appear effective for this multivariate regression task.", "The 1D CNN regression model converged smoothly over 50 epochs, steadily reducing validation MSE and achieving a best validation MSE of 0.005785. Most improvements occurred in the first 30 epochs, with minor refinements thereafter, indicating stable training dynamics.", "The final MLP model with four hidden layers and dropout achieved a validation MSE of 0.004699 after 50 epochs of training using the Adam optimizer. The training was stable and converged within the given time budget. Further hyperparameter tuning or architectural modifications may yield incremental improvements.", "The 5-layer MLP with dropout trained for 40 epochs achieved a validation MSE of 0.003407, indicating the network effectively learned the mapping from 14D geometry to 2001D spectra. This performance suggests the chosen architecture and training setup are suitable, though further hyperparameter tuning or architecture exploration could potentially lower the error even more.", "The ResMLP architecture converged smoothly, achieving a train MSE of 0.00498 and a validation MSE of 0.00462 after 50 epochs. Batch normalization, dropout, and weight decay helped regularize the model, yielding good generalization on the validation set. Test predictions were successfully saved for submission.", "The Residual MLP converged smoothly over 50 epochs, reducing validation MSE from 0.008699 to 0.002894, demonstrating effective learning and generalization. The architecture with batch normalization, dropout, and a shortcut connection helped stabilize training and improve performance. Test predictions were successfully generated and saved for submission.", "The residual MLP steadily improved from an initial validation MSE of 0.008699 at epoch 1 to a final MSE of 0.002894 after 50 epochs, demonstrating stable convergence with the chosen architecture and hyperparameters. Batch normalization, residual connections, and the ReduceLROnPlateau scheduler effectively reduced the error. Further exploration (e.g., CNN or Transformer-based blocks) might yield additional gains.", "The residual MLP model converged smoothly, achieving a final training MSE of ~0.00498 and a validation MSE of 0.00462 after 50 epochs. The architecture with batch normalization, dropout, and a shortcut connection provided stable learning and low error on the spectrum prediction task.", "The residual MLP model achieved strong convergence, with training MSE decreasing steadily to 0.003326 by epoch 50 and a validation MSE of 0.003017. The OneCycleLR scheduler and dropout helped regularize the network without any runtime errors. Test predictions were successfully saved for submission.", "The ResMLP model converged smoothly over 50 epochs, steadily reducing validation MSE from 0.0087 at epoch\u00a01 to a final best of 0.002846. Training and validation curves show consistent improvement without overfitting. Test predictions were successfully saved for submission.", "The ResMLP model converged smoothly over 50 epochs, with training MSE decreasing from 0.0247 to 0.0035 and validation MSE improving from 0.0073 to a best value of 0.003053. The final checkpointed model achieves a low validation error, indicating strong regression performance on this spectrum prediction task.", "A 3-layer residual MLP was trained with batch normalization and dropout over 5-fold CV for 50 epochs, achieving consistent fold-wise validation MSEs around 0.0029 and an overall CV MSE of 0.002900.", "The ResMLP model trained for 50 epochs converged smoothly, showing steady improvements in validation MSE from 0.0087 at epoch 1 to a best of 0.002963. Training and validation losses remained consistent, indicating stable learning without overfitting. The final test predictions were successfully saved.", "The ResMLP model converged smoothly, reducing validation MSE from 0.0091 to a best of 0.002646 over 50 epochs. Training and validation losses decreased steadily, indicating good fit without overfitting. Test predictions were successfully saved.", "The ResMLP model yielded stable and low validation MSE across all 5 folds, with fold MSEs ranging from 0.002625 to 0.002664. The overall out-of-fold MSE was 0.002642, demonstrating good generalization. Test predictions were generated and saved successfully.", "The ResMLP architecture with a residual connection achieved consistent validation performance across 5 folds, yielding fold MSEs between 0.002606 and 0.002665. The overall out\u2011of\u2011fold MSE stabilized at 0.002640, demonstrating robust generalization. Final test predictions were averaged and saved successfully.", "The ResMLP model with residual connections and layer normalization achieved stable performance across 5 folds, with per-fold validation MSEs ranging from 0.002371 to 0.002424. The overall out-of-fold MSE is 0.002403, indicating consistent predictive accuracy for the 2001D spectrum outputs.", "The ResMLP model with 3 residual blocks and OneCycleLR training achieved consistent performance across all 5 folds, yielding an average OOF MSE of 0.002471. Validation MSEs ranged narrowly between 0.002456 and 0.002482, demonstrating stable generalization.", "We trained a ResMLP with SWA across 5 folds, achieving validation MSEs around 0.00235-0.00240 and an overall OOF MSE of 0.002378.", "A ResMLP architecture with SWA and gradient clipping achieved consistent fold validation MSEs around 0.00235\u20130.00240 and an overall OOF MSE of 0.002378, indicating stable and effective regression performance. Test predictions were successfully saved for submission. No errors were encountered during training or evaluation.", "The ResMLP model with SWA training achieved stable performance across five folds, with individual validation MSEs around 0.00248\u20130.00253. The overall out-of-fold MSE was 0.002503, indicating consistent generalization. Test predictions were generated and saved successfully.", "The SWA-averaged ResMLP with gradient clipping achieved stable performance across folds, with per-fold validation MSE around 0.00235\u20130.00240 and an overall OOF MSE of 0.002378. This indicates strong generalization and stable convergence under the chosen architecture and training regimen.", "The ResMLP with SWA and gradient clipping achieved stable performance across 5 folds, yielding validation MSEs in the narrow range of 0.00235\u20130.00240. The overall out-of-fold MSE of 0.002378 indicates good generalization for the multivariate regression task.", "The ResMLP model combined with degree-2 polynomial feature expansion and SWA produced consistent validation MSEs around 0.00227 across all 5 folds, resulting in an overall OOF MSE of 0.002275. Training completed successfully without any errors or warnings, and the averaged test predictions were saved as expected. These results indicate stable and reliable performance of the chosen architecture and training setup.", "The ResMLP model with polynomial feature expansion, input noise augmentation, and SWA produced consistent validation MSEs around 0.00228 across all 5 folds, yielding an overall OOF MSE of 0.002282. Training completed successfully within the time limit, and test predictions were generated and saved.", "The ResMLP model with polynomial feature expansion, SWA, and input noise augmentation achieved highly consistent validation performance across all 5 folds, with per-fold MSEs around 0.00227. The overall out\u2011of\u2011fold MSE is 0.002282, indicating strong generalization and stability of the approach.", "The ResMLP model augmented with degree-2 polynomial features and SWA achieved consistent validation MSEs (~0.00228 across folds) and an overall OOF MSE of 0.002282, indicating stable performance and good generalization.", "The ResMLP model with polynomial feature expansion, mixup, and SWA achieved consistent validation MSEs between 0.00251 and 0.00256 across folds, resulting in an overall OOF MSE of 0.002541. This indicates stable and strong predictive performance on the regression task.", "The ResMLP architecture with degree-2 polynomial feature expansion, layer normalization, dropout, and SWA training achieved consistent validation MSEs across folds (around 0.00253) and an overall OOF MSE of 0.002545, demonstrating strong generalization. Test predictions were saved successfully.", "The ResMLP model with polynomial feature expansion (degree 3) and SWA training achieved consistent performance across all 5 CV folds, with fold MSEs ranging from 0.002270 to 0.002310. The overall out-of-fold MSE was 0.002289, demonstrating low variance and strong generalization.", "The training process crashed with a CUDA initialization error because the DataLoader workers attempted to handle CUDA tensors. The dataset tensors were moved to the GPU before loading, which is not supported with multiple workers. To fix this, keep data tensors on CPU (use device='cpu') in the TensorDataset and move batches to the GPU inside the training loop, or set num_workers=0 to avoid worker processes accessing CUDA tensors.", "The ResMLP model with polynomial feature expansion (degree 3) and SWA training achieved consistent validation MSEs around 0.00227\u20130.00230 across five folds, yielding an overall OOF MSE of 0.002286. Test set predictions were averaged across folds and successfully saved to submission.csv.", "The script crashes when constructing the SWA averaged model due to the weight_norm wrappers not supporting deepcopy. A simple fix is to remove the weight_norm reparameterizations (e.g., by calling torch.nn.utils.remove_weight_norm on each layer) before passing the model into AveragedModel, or to defer applying weight normalization until after SWA model construction. Alternatively, disable SWA or implement a manual moving-average of the parameters.", "The Residual MLP with polynomial feature expansion, layer normalization, dropout, and SWA produced stable validation MSE around 0.00227 across five folds, achieving an overall OOF MSE of 0.002275. The model\u2019s architecture and training regimen effectively minimized the regression error consistently.", "The SWA\u2011enhanced ResMLP with GELU activations and degree\u20112 polynomial features achieved consistent performance across 5 folds, yielding individual validation MSEs between 0.00214 and 0.00219 and an overall OOF MSE of 0.002165. This indicates strong generalization on the validation set and stable convergence under the chosen architecture and training schedule.", "The ResMLP with polynomial feature expansion and SWA achieved consistent validation performance across all 5 folds (MSE ~0.00230) and an overall OOF MSE of 0.002308, demonstrating stable generalization. The model saved test predictions successfully.", "The ResMLP model with polynomial feature expansion, Gaussian noise augmentation, and SWA achieved consistent cross-validated performance with validation MSEs around 0.00216. Overall out-of-fold MSE was 0.002163, demonstrating stable and low error across folds.", "The ResMLP model with degree\u20112 polynomial feature expansion, layer normalization, dropout, and SWA produced stable validation MSEs around 0.00231 across all 5 folds, yielding an overall OOF MSE of 0.002315. This indicates good generalization and consistent performance.", "The ResMLP with second-degree polynomial features and SWA achieved consistent fold validation MSEs around 0.00242\u20130.00245, yielding an overall OOF MSE of 0.002439. The model demonstrates stable generalization across folds and effective performance for this regression task.", "The ResMLP with second-degree polynomial features, SWA, and TTA achieved a stable validation MSE around 0.00216 across all five folds, with an overall OOF MSE of 0.002163. Test predictions were saved successfully within the time limit, demonstrating robust and consistent performance.", "The ResMLP with polynomial feature expansion, SWA and TTA achieved highly stable performance, yielding foldwise validation MSEs between 0.002155 and 0.002174 and an overall OOF MSE of 0.002163. This indicates the model generalizes well on this regression task under the given setup.", "The ResMLP model with polynomial feature expansion, SWA, and TTA achieved consistent validation MSE around 0.00216 across five folds and an overall OOF MSE of 0.002163. The approach stabilized predictions effectively, and the final test predictions were saved successfully.", "The ResMLP model with 2nd-degree polynomial expansion, SWA, and TTA achieved stable performance across folds with validation MSEs around 0.00216, yielding an overall OOF MSE of 0.002163. The consistent fold results indicate good generalization with this architecture and training strategy.", "The ResMLP model with SWA and TTA achieved consistent validation MSE around 0.00216 across all 5 folds, indicating stable performance. The out-of-fold MSE was 0.002163, demonstrating effective generalization. Test predictions were saved successfully.", "The ResMLP with polynomial feature expansion, SWA, and TTA achieved consistent validation MSE across 5 folds (~0.00216) and an overall OOF MSE of 0.002163. The setup appears stable with minimal variance between folds. Test predictions have been saved for submission.", "The ResMLP model with second-degree polynomial features, LayerNorm, dropout, and SWA produced consistent validation MSEs across folds (around 0.00242) and an overall OOF MSE of 0.002439. The training procedure including cosine annealing and SWA yielded stable predictions, and test predictions were saved successfully.", "The ResMLP model combined with 2nd-degree polynomial feature expansion, LayerNorm, SWA, and test-time augmentation achieved consistently low validation MSE (~0.00216) across all 5 folds. The final out\u2011of\u2011fold MSE was 0.002163, indicating stable generalization performance.", "The ResMLP model with polynomial feature augmentation, SWA, and TTA achieved consistent validation MSEs across folds (~0.00216\u20130.00219) and an overall OOF MSE of 0.002179, demonstrating stable and low-error regression performance.", "The ResMLP with PCA on the targets and SWA training achieved stable and low MSE across folds, with an overall OOF MSE of 0.002174. The model uses a 14\u2192256-512-256 residual MLP architecture with SWA starting at epoch 25 and PCA dimension 100.", "The ResMLP with polynomial feature expansion, batch normalization, and stochastic weight averaging achieved consistent validation MSE around 0.0024 across all 5 folds, yielding an overall OOF MSE of 0.002410. The model demonstrates stable and robust regression performance on the electromagnetic spectrum prediction task.", "The ResMLP with SWA, MC Dropout, and TTA achieved stable fold-wise validation MSE around 0.00219, resulting in an overall OOF MSE of 0.002198. The model training and inference pipeline executed correctly and produced consistent, low-error predictions. Test predictions were saved successfully.", "The Residual MLP with polynomial feature expansion, PCA reduction, LayerNorm, GELU activations, and Stochastic Weight Averaging achieved consistent performance across 5 folds, yielding fold MSEs around 0.00236\u20130.00242. The overall out\u2011of\u2011fold MSE is 0.002385, demonstrating effective variance reduction and stable generalization.", "The ResMLP model augmented with 2nd-degree polynomial features, SWA, and TTA achieved consistent performance across 5 folds, yielding fold MSEs around 0.00216 and an overall OOF MSE of 0.002163. The pipeline ran smoothly and produced test predictions without errors.", "A ResMLP with polynomial feature expansion, Gaussian noise TTA, and SWA achieved stable validation performance across 5 folds, with individual fold MSEs around 0.00216. The overall out\u2011of\u2011fold MSE is 0.002163, indicating robust generalization.", "The TabTransformerReg (Transformer + ResMLP head) trained with SWA achieved stable performance across 5 folds with validation MSEs around 0.00194\u20130.00197, yielding an OOF MSE of 0.001956. Training completed without errors and predictions were saved successfully.", "The TabTransformer-based model with SWA and TTA achieved consistent performance across folds, with per-fold validation MSEs around 0.00194\u20130.00199 and an overall OOF MSE of 0.001960. The use of residual MLP heads, stochastic weight averaging, and test\u2010time noise augmentation contributed to stable and low-error predictions.", "The TabTransformerReg model with SWA and TTA achieved consistently low per\u2010fold validation MSE (0.00194\u20130.001997) and an overall OOF MSE of 0.001960, indicating strong predictive performance. The 5-fold ensemble predictions were saved successfully for test submission.", "The TabTransformerReg model with a ResMLP head trained with SWA and TTA achieved strong performance, yielding consistent fold-wise validation MSEs around 0.00194\u20130.00199 and an overall OOF MSE of 0.001960. The training procedure was stable and completed successfully in under the time limit.", "The model\u2014a TabTransformer-based network with SWA and TTA\u2014achieved consistently low MSE across all 5 folds, with per-fold validation MSEs around 0.00196\u20130.00199 and an overall OOF MSE of 0.001976. The combination of stochastic weight averaging, Monte Carlo dropout, and feature noise appears to have stabilized predictions. Test predictions were successfully saved for submission.", "We trained a TabTransformer-based model with a ResMLP head, SWA and TTA, achieving consistent fold MSEs around 0.00194\u20130.00199 and an overall OOF MSE of 0.00196. These results demonstrate that the architecture effectively predicts the 2001\u2011dimensional spectrum from 14\u2011dimensional geometry inputs.", "The TabTransformer-based model with SWA and TTA achieved consistently low validation MSE across folds, with individual fold MSEs around 0.00194\u20130.00199 and an overall OOF MSE of 0.00196. Test predictions were saved successfully.", "The TabTransformerReg model with a ResMLP head, mixup augmentation, noise injection, and SWA produced stable results with fold validation MSEs tightly clustered around 0.00226. The overall OOF MSE of 0.002263 indicates consistent performance across splits. Test predictions were saved successfully.", "The TabTransformerReg model with a residual MLP head, combined with SWA and TTA, achieved consistent validation performance across 5 folds, yielding an average MSE of 0.001957. The training procedure demonstrated stable convergence and effective generalization. Final test predictions were generated by averaging fold outputs.", "The TabTransformerReg (transformer + ResMLP head) achieved consistently low validation MSE across 5 folds (ranging 0.001963\u20130.001990), with an overall OOF MSE of 0.001978. This demonstrates stable and strong regression performance in predicting the 2001\u2011dimensional spectrum from 14\u2011dimensional geometry inputs.", "Using the TabTransformer with a residual MLP head, SWA, and TTA produced highly stable performance across folds\u2014validation MSE ranged from 0.001933 to 0.001966 with an overall OOF MSE of 0.001957. The model generalizes well, and final test predictions have been generated and saved.", "The TabTransformerReg model with ResMLP head and SWA achieved consistent fold validation MSEs around 0.00194\u20130.00199, yielding an overall OOF MSE of 0.001960. Test predictions were successfully generated and saved without errors.", "The TabTransformerReg with ResMLP head, SWA, and TTA attained consistent validation MSEs between 0.001933 and 0.001966, leading to an overall OOF MSE of 0.001957. The use of data standardization, random noise augmentation, and Stochastic Weight Averaging improved performance and stability. Test predictions were generated and saved successfully.", "The TabTransformer-based model with a residual MLP head, SWA, and TTA achieved stable and low validation MSE across all folds (0.00194\u20130.001997) with an overall OOF MSE of ~0.00196. The combination of SWA and test-time noise augmentation contributed to consistent generalization.", "The TabTransformer-based regression model with residual MLP head and SWA combined with Monte Carlo Dropout TTA achieved consistent performance across all five folds, yielding validation MSEs in the narrow range of 0.00198\u20130.00202. The overall out-of-fold MSE was 0.002003, indicating stable and low error in predicting the 2001-dimensional spectra.", "The TabTransformer-based regressor with residual MLP head, SWA, and TTA achieved consistently low fold validation MSEs (~0.00194\u20130.00199) and an overall OOF MSE of 0.00196. This indicates stable and strong generalization performance across the 5-fold CV.", "The model training completed successfully without any errors. Using a TabTransformerReg with SWA and five\u2010fold CV plus TTA yielded stable validation MSEs around 0.00193\u20130.00197 and an overall OOF MSE of 0.001957, indicating good generalization.", "We trained a TabTransformer-based regressor with a ResMLP head using 5-fold cross-validation, SWA starting at epoch 25, and 5x TTA during inference. The per-fold validation MSE ranged from 0.00194 to 0.001997, yielding an overall OOF MSE of 0.001960. Test predictions were generated and saved successfully.", "The TabTransformer-based model with a ResMLP head and SWA achieved consistently low validation MSE around 0.00226 across all folds, yielding an overall OOF MSE of 0.002263. The training procedure with mixup, noise augmentation, and SWA provided robust generalization. Test predictions were successfully generated and saved.", "We trained a TabTransformer-style model with a ResMLP head using SWA and test-time augmentation. Across 5-fold CV, the per-fold validation MSE ranged from 0.00194 to 0.001997, yielding an overall OOF MSE of 0.001960, indicating stable and strong regression performance.", "The SWA-augmented TabTransformer regression model produced consistent validation results, achieving per-fold MSEs between 0.00194 and 0.001997 and an overall OOF MSE of 0.001960. Test predictions were successfully generated and saved for submission.", "The TabTransformer-based model with a ResMLP head, combined with SWA and TTA, achieved consistent fold MSEs around 0.00194\u20130.00199 and an overall OOF MSE of 0.00196. Training across 5 folds with data augmentation and SWA improved stability, and final test predictions were saved successfully.", "The TabTransformer with SWA and TTA achieved stable and low validation MSE across folds (around 0.002). The final out\u2011of\u2011fold MSE was 0.002003, indicating consistent predictive performance on the spectrum regression task. Test predictions were saved successfully.", "The TabTransformerReg with SWA and TTA achieved consistently low validation MSE across all folds (~0.00196) and an overall OOF MSE of 0.001957, demonstrating stable performance. Gaussian noise augmentation at both training and inference contributed to robust generalization with minimal fold-to-fold variance.", "The model achieved consistent performance across all 5 folds with validation MSEs around 0.00194\u20130.00197 and an overall OOF MSE of 0.001955. The TabTransformer encoder followed by a residual MLP head and SWA training strategy yields strong and stable regression results on this spectrum prediction task.", "The TabTransformerReg model with a ResMLP head, trained with SWA and TTA, achieved consistent validation performance across the 5 folds, yielding per-fold MSEs around 0.00194\u20130.00199 and an overall OOF MSE of 0.001960. Test predictions were successfully saved.", "The TabTransformer-based model with SWA and TTA achieved consistent validation MSEs across 5 folds, ranging from 0.00194 to 0.001996, and an overall OOF MSE of 0.00196. Test predictions were saved successfully.", "A TabTransformer-based model with a residual MLP head, mixup augmentation, smoothness penalty, and SWA achieved consistent validation MSEs around 0.00226 across 5 folds, with an overall OOF MSE of 0.002263. This indicates strong generalization on the held\u2011out validation data.", "The TabTransformerReg model with SWA and TTA achieved consistent performance across all folds, yielding an average OOF MSE of 0.00196. The use of smoothness regularization and test-time augmentation resulted in stable and low-error predictions. The training and inference pipeline executed successfully without errors.", "The TabTransformer-based regression model with SWA, smoothness regularization, and TTA achieved consistent per-fold validation MSEs around 0.00194\u20130.00199, yielding an overall OOF MSE of 0.00196. Test predictions were generated successfully and saved without errors.", "We trained a TabTransformer-style model with a residual MLP head, applying SWA and TTA. Validation MSE across folds was consistently around 0.00194\u20130.00199, yielding an overall OOF MSE of 0.001960. Test predictions were saved for submission.", "The TabTransformer-based model with a ResMLP head, SWA, and TTA achieved consistent validation MSEs across folds (~0.00194\u20130.00199) and an overall OOF MSE of 0.00196. This indicates stable performance and effective regularization from smoothness penalty and stochastic weight averaging.", "The TabTransformer-based residual MLP achieved consistent performance across folds, with individual validation MSEs around 0.002 and an overall out-of-fold MSE of 0.002002. Monte Carlo dropout TTA and SWA further stabilized predictions, and test predictions were successfully generated.", "Using a TabTransformer backbone with SWA and TTA delivered consistent validation MSEs around 0.00194\u20130.00199 across five folds, with a final OOF MSE of 0.00196. Test predictions were saved successfully.", "The TabTransformerReg with ResMLP head and SWA achieved stable performance across 5-fold CV, yielding validation MSEs around 0.002. The overall out-of-fold MSE is 0.002002, indicating consistent generalization. Test predictions were generated and saved successfully.", "The TabTransformer-based model with a ResMLP head and SWA achieved consistent validation performance across 5 folds, with fold MSEs around 0.00225 and an overall OOF MSE of 0.002263. The use of mixup, smoothness penalty, and stochastic weight averaging stabilized training and improved generalization. Test predictions were successfully saved for submission.", "The TabTransformerReg model with a ResMLP head and SWA produced consistently low validation MSE across all 5 folds (~0.00182\u20130.00184) and an overall OOF MSE of 0.001830, indicating stable performance. Test predictions have been saved successfully.", "We trained a TabTransformer with a ResMLP head using 5\u2011fold cross\u2011validation and SWA + TTA. The model achieved consistently low per\u2011fold validation MSE around 0.00182 and an overall OOF MSE of 0.001825, demonstrating good generalization.", "The TabTransformerReg with SWA and TTA achieved consistently low validation MSEs around 0.0017 across all 5 folds, yielding an overall OOF MSE of 0.001752. This indicates stable generalization and effective smoothing and noise augmentation during training.", "The TabTransformer-based model with a ResMLP head, combined with SWA and Monte Carlo Dropout TTA, achieved consistent per-fold validation MSEs around 0.00172\u20130.00179 and an overall OOF MSE of 0.001748. The training pipeline ran smoothly without errors, and the final predictions were saved as expected.", "The TabTransformerReg model with Switched Weight Averaging (SWA), Cosine Annealing LR, and Monte Carlo Dropout TTA achieved consistent fold-wise validation MSE around 0.0017, yielding an overall OOF MSE of 0.001717. The approach proves stable across the 5 folds and should generalize well to test data.", "The TabTransformer-based regressor with a ResMLP head, combined with SWA, cosine annealing, and TTA, achieved stable fold-wise performance with validation MSEs around 0.00165\u20130.00172. The out-of-fold MSE was 0.001686, indicating consistent generalization. Test predictions have been saved for submission.", "The TabTransformer-based regressor with a residual MLP head, combined with SWA, cosine annealing, and TTA (20 augmentations), achieved consistent validation MSEs across 5 folds (~0.00166\u20130.00172) and an overall OOF MSE of 0.00168. The training procedure ran smoothly without errors and the model yields reliable predictions on the test set.", "The TabTransformer-based model with SWA, TTA, and PCA on targets achieved consistent validation MSE across five folds (~0.00167), yielding an overall OOF MSE of 0.001679. This indicates stable performance and effective use of smoothing, ensemble, and noise augmentation strategies for this regression task.", "The TabTransformerReg model with SWA, PCA on targets (200 components), and TTA achieved consistent validation MSE across 5 folds, with individual fold MSEs around 0.00166\u20130.00171 and an overall OOF MSE of 0.00169. This demonstrates stable performance and effective regularization and ensembling strategies on the regression task.", "The TabTransformer-based model with SWA, PCA on targets, and extensive TTA achieved consistent validation fold MSEs around 0.00167 and an overall OOF MSE of 0.001675, indicating strong generalization. The pipeline ran smoothly and generated stable test predictions.", "The TabTransformer-based model with PCA target reduction, SWA and TTA achieved consistently low validation MSE (~0.00167 across all folds) and an overall OOF MSE of 0.001676.", "Using a 3\u2011layer TabTransformer with PCA on targets, SWA, and extensive test/time TTA resulted in very consistent fold MSEs around 0.00164. The final out\u2011of\u2011fold MSE was 0.001637, demonstrating stable and low prediction error across all splits.", "Across 5 folds, the TabTransformerReg + SWA model achieved very consistent validation MSEs around 0.00152\u20130.00155, yielding an out\u2011of\u2011fold MSE of 0.001532. These results indicate the model generalizes well and the training pipeline is stable.", "The PCA\u2011based dimensionality reduction coupled with a TabTransformer backbone and ResMLP head, trained with SWA, TTA, and a smoothness penalty, achieved stable performance across folds with validation MSEs in the 0.00150\u20130.00155 range. The final out\u2011of\u2011fold MSE was 0.001527, indicating the model\u2019s strong ability to predict the 2001\u2011dimensional spectrum from 14 geometry features.", "The TabTransformer-based regression model with PCA, SWA, and TTA achieved stable performance across folds, with validation MSEs between 0.001488 and 0.001542 and an overall OOF MSE of 0.001516. Predictions for the test set were successfully saved.", "The TabTransformerReg model with PCA-based target compression and SWA produced stable results, achieving fold-wise validation MSEs around 0.0015. Incorporating smoothness regularization, TTA, and cosine annealing with SWA improved consistency across folds. The overall out-of-fold MSE was 0.001516.", "We successfully trained a TabTransformer-based regression model with PCA reduction (100 dims), SWA, and TTA. Across 5 folds, validation MSE ranged between 0.001495 and 0.001538, yielding an overall OOF MSE of 0.001519. The results indicate strong performance for predicting the 2001D spectrum, and test predictions were saved.", "The TabTransformerReg model with PCA-based target decomposition, SWA, and test\u2010time augmentation achieved consistent validation MSEs of around 0.0015 across 5 folds, yielding an overall OOF MSE of 0.001522. Training proceeded without errors and test predictions were saved successfully.", "We performed 5-fold cross-validation using a TabTransformer regression model with SWA, smoothness regularization, and TTA, compressing targets via PCA. Each fold achieved validation MSE around 0.00164\u20130.00174, leading to an overall OOF MSE of approximately 0.001695. Test predictions were saved successfully.", "The model training completed successfully with consistent validation performance across folds, achieving an average OOF MSE of approximately 0.001506. The use of PCA, SWA, and TTA contributed to stable and low-error predictions.", "The TabTransformerReg model with PCA-reduced targets and SWA achieved very low validation MSE (~0.0017) consistently across all 5 folds, yielding an overall OOF MSE of 0.001697. The use of TTA, smoothness penalty, and SWA improved stability and produced smooth, accurate spectrum predictions.", "The TabTransformerReg architecture with PCA output compression, SWA, Cosine Annealing LR, and TTA achieved consistent validation MSE around 0.00151 across folds, resulting in an overall OOF MSE of 0.001515.", "The TabTransformer-based regression model with PCA dimensionality reduction, SWA, and TTA achieved consistent per-fold validation MSEs between 0.001452 and 0.001522, resulting in an overall OOF MSE of 0.001481. The approach demonstrates stable and robust generalization for the 14D-to-2001D spectrum prediction task.", "Using a TabTransformer-based regression with PCA compression, SWA, and extensive TTA, the model achieved consistently low MSE across folds (\u22480.0015) and an overall OOF MSE of 0.00152. The pipeline ran without errors, demonstrating stable performance and effective utilization of smoothness regularization, PCA, and learning rate schedules.", "The TabTransformer-based regression model with SWA and TTA achieved consistent and low validation MSE across all folds, with individual fold MSEs between 0.001493 and 0.001555. The overall out-of-fold MSE was 0.001520, indicating robust generalization.", "The TabTransformer-based model with PCA compression and SWA achieved consistent fold validation MSEs between 0.001456 and 0.001521, yielding an overall OOF MSE of 0.001484. Techniques like TTA, smoothness regularization, and noise augmentation contributed to stable performance. A final ensemble over folds saved test predictions accordingly.", "The TabTransformer-based regression model with PCA, SWA, TTA, and Gaussian smoothing achieved consistent performance across folds, with validation MSEs between 0.001452 and 0.001522. The overall out-of-fold MSE of 0.001481 demonstrates good generalization for predicting the electromagnetic spectrum from geometry parameters.", "The 5-fold cross\u2011validation yielded stable per\u2011fold MSEs between 0.001493 and 0.001555, resulting in an overall OOF MSE of 0.001520. The PCA\u2011based output compression combined with SWA and test\u2011time augmentation provided consistent and low\u2011variance predictions.", "Across five folds, the TabTransformer-based model with PCA (200 components), SWA, and TTA achieved consistent validation MSEs around 0.00148\u20130.00151, yielding an overall OOF MSE of 0.001492. The training procedure converged stably, and the final smoothed test predictions were successfully saved.", "The TabTransformerReg model with PCA dimensionality reduction, SWA, TTA, and spectral smoothing achieved consistent validation MSE across folds (0.00145\u20130.00152), yielding an overall OOF MSE of 0.001481. The training and prediction pipeline ran without errors and produced a final test submission.", "The TabTransformerReg model with SWA, PCA-based dimensionality reduction, smoothness regularization, and TTA achieved stable performance across folds, with validation MSEs around 0.00150. Final out-of-fold MSE was 0.001513, indicating consistent generalization on the 2001D spectral regression task.", "The TabTransformer with residual MLP head, combined with PCA dimensionality reduction, TTA, and SWA, achieved consistently low validation MSE across 5 folds (~0.00135\u20130.00139) and an overall OOF MSE of 0.001362. The training pipeline ran smoothly without errors, and test predictions were successfully generated and saved.", "The TabTransformerReg model with PCA-based target compression, SWA, and TTA achieved consistent validation performance across folds (MSE ~0.00137\u20130.00141) and an overall OOF MSE of 0.001383. The inclusion of a smoothness penalty and post-hoc spectral smoothing helped stabilize predictions.", "The TabTransformerReg model with PCA compression, SWA, and TTA achieved consistent performance across 5 folds, yielding fold validation MSEs between 0.001348 and 0.001366 and an overall OOF MSE of 0.001357. The approach demonstrates stable generalization on the validation set.", "The TabTransformerReg combined with PCA and SWA achieved consistent validation MSE across folds (~0.00148) and an overall OOF MSE of 0.001492, indicating stable performance and effective generalization.", "The TabTransformer-based model with SWA, TTA, and triangular smoothing achieved consistent validation performance across folds, yielding per-fold MSE around 0.00135 and an overall OOF MSE of 0.001357. Incorporating PCA on targets and smoothness penalties helped stabilize and improve predictions.", "Across 5-fold cross validation, the Transformer\u2011based regressor with SWA, TTA, target PCA, and output smoothing achieved highly consistent validation MSEs (~0.00135 per fold) and an overall OOF MSE of 0.001357. The pipeline successfully saved test predictions.", "The TabTransformerReg model with SWA, TTA, PCA on targets, and triangular smoothing achieved consistent performance across folds, with fold MSEs around 0.00135 and an overall OOF MSE of 0.001357. The approach demonstrates stable and low-error predictions for the 2001D spectrum.", "We leveraged a TabTransformer-based regression model with a ResMLP output head, SWA, TTA and triangular smoothing. Across 5-fold CV the average validation MSE was ~0.001378, indicating stable and low error. Final OOF MSE of 0.001378 suggests the pipeline is well-tuned for this regression task.", "The TabTransformer-based model with PCA compression, SWA, and TTA achieved stable performance across folds, yielding per-fold validation MSEs around 0.00136\u20130.00139. The overall out-of-fold MSE was 0.001375, indicating consistent generalization. Test predictions were successfully saved.", "The TabTransformer-based model with PCA compression and SWA produced consistent validation MSE between 0.00126 and 0.00132 across five folds, yielding an overall OOF MSE of 0.001285. Test-time augmentation and a smoothness penalty further stabilized predictions, and final test outputs were generated by averaging TTA ensembles.", "The TabTransformer-based model with PCA dimensionality reduction, stochastic weight averaging, test\u2010time augmentation, and post\u2010prediction smoothing achieved very consistent performance, yielding fold validation MSEs between 0.00126 and 0.00132 and an overall out\u2010of\u2010fold MSE of 0.001285. This indicates the chosen architecture and training strategy effectively model the 14\u21922001 regression task.", "The TabTransformerReg model with PCA dimensionality reduction, SWA, and extensive TTA achieved consistent validation MSEs around 0.00127\u20130.00132 across 5 folds, yielding an overall OOF MSE of ~0.001285. The combination of smoothness regularization and post\u2010training smoothing produced stable predictions. Final test predictions were saved successfully.", "The TabTransformerReg model with PCA compression and SWA achieved consistent validation MSE across folds (~0.0152), and an overall OOF MSE of 0.015228. TTA and smoothness regularization contributed to stable predictions and low error.", "The TabTransformer with PCA-based target reduction, SWA, and extensive TTA achieved consistent fold MSEs around 0.0013, yielding an overall OOF MSE of 0.001368. This architecture and training pipeline appear stable and effective for the multivariate spectrum regression task.", "The TabTransformerReg model with PCA-reduced targets, SWA training, TTA and triangular smoothing achieved consistent fold validation MSEs (~0.00126\u20130.00132), resulting in an overall OOF MSE of 0.001285. Training completed without errors and test predictions were successfully saved.", "Across 5 cross-validation folds, the TabTransformer-based model with SWA, PCA reconstruction, TTA, and post-prediction smoothing achieved smoothed validation MSEs ranging from 0.001297 to 0.001354, yielding an overall OOF MSE of 0.001319. The approach produced stable and low-error spectrum predictions.", "We implemented a TabTransformer-based model with PCA on spectral outputs, SWA, and extensive TTA. Across 5 folds, validation MSE ranged from 0.001389 to 0.001474, with an overall OOF MSE of 0.001434. Test predictions were saved successfully.", "The TabTransformer-based regression model with PCA, SWA, and TTA achieved stable and low error across validation folds, with fold MSEs ranging from 0.001283 to 0.001341. The overall out-of-fold (OOF) MSE was 0.001306, indicating consistent and robust performance. Test predictions were generated successfully and saved for submission."], "exp_name": "2-imperial-nickel-oxpecker", "metrics": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}

let lastClick = 0;
let firstFrameTime = undefined;

let nodes = [];
let edges = [];

let lastScrollPos = 0;

setup = () => {
  canvas = createCanvas(...updateTargetDims());
};

class Node {
  x;
  y;
  size;
  xT;
  yT;
  xB;
  yB;
  treeInd;
  color;
  relSize;
  animationStart = Number.MAX_VALUE;
  animationProgress = 0;
  isStatic = false;
  hasChildren = false;
  isRootNode = true;
  isStarred = false;
  selected = false;
  renderSize = 10;
  edges = [];
  bgCol;

  constructor(x, y, relSize, treeInd) {
    const minSize = 35;
    const maxSize = 60;

    const maxColor = 10;
    const minColor = 125;

    this.relSize = relSize;
    this.treeInd = treeInd;
    this.size = minSize + (maxSize - minSize) * relSize;
    this.color = minColor + (maxColor - minColor) * relSize;
    this.bgCol = Math.round(Math.max(this.color / 2, 0));

    this.x = x;
    this.y = y;
    this.xT = x;
    this.yT = y - this.size / 2;
    this.xB = x;
    this.yB = y + this.size / 2;

    nodes.push(this);
  }

  startAnimation = (offset = 0) => {
    if (this.animationStart == Number.MAX_VALUE)
      this.animationStart = globalTime + offset;
  };

  child = (node) => {
    let edge = new Edge(this, node);
    this.edges.push(edge);
    edges.push(edge);
    this.hasChildren = true;
    node.isRootNode = false;
    return node;
  };

  render = () => {
    if (globalTime - this.animationStart < 0) return;

    const mouseXlocalCoords = (mouseX - width / 2) / scaleFactor;
    const mouseYlocalCoords = (mouseY - height / 2) / scaleFactor;
    const isMouseOver =
      dist(mouseXlocalCoords, mouseYlocalCoords, this.x, this.y) <
      this.renderSize / 1.5;
    if (isMouseOver) cursor(HAND);
    if (isMouseOver && mouseIsPressed) {
      nodes.forEach((n) => (n.selected = false));
      this.selected = true;
      setCodeAndPlan(
        treeStructData.code[this.treeInd],
        treeStructData.plan[this.treeInd],
      );
      manualSelection = true;
    }

    this.renderSize = this.size;
    if (!this.isStatic) {
      this.animationProgress = animEase(
        (globalTime - this.animationStart) / 1000,
      );
      if (this.animationProgress >= 1) {
        this.isStatic = true;
      } else {
        this.renderSize =
          this.size *
          (0.8 +
            0.2 *
              (-3.33 * this.animationProgress ** 2 +
                4.33 * this.animationProgress));
      }
    }

    fill(this.color);
    if (this.selected) {
      fill(accentCol);
    }

    noStroke();
    square(
      this.x - this.renderSize / 2,
      this.y - this.renderSize / 2,
      this.renderSize,
      10,
    );

    noStroke();
    textAlign(CENTER, CENTER);
    textSize(this.renderSize / 2);
    fill(255);
    // fill(lerpColor(color(accentCol), color(255), this.animationProgress))
    text("{ }", this.x, this.y - 1);
    // DEBUG PRINT:
    // text(round(this.relSize, 2), this.x, this.y - 1)
    // text(this.treeInd, this.x, this.y + 15)

    const dotAnimThreshold = 0.85;
    if (this.isStarred && this.animationProgress >= dotAnimThreshold) {
      let dotAnimProgress =
        (this.animationProgress - dotAnimThreshold) / (1 - dotAnimThreshold);
      textSize(
        ((-3.33 * dotAnimProgress ** 2 + 4.33 * dotAnimProgress) *
          this.renderSize) /
          2,
      );
      if (this.selected) {
        fill(0);
        stroke(0);
      } else {
        fill(accentCol);
        stroke(accentCol);
      }
      strokeWeight((-(dotAnimProgress ** 2) + dotAnimProgress) * 2);
      text("*", this.x + 20, this.y - 11);
      noStroke();
    }

    if (!this.isStatic) {
      fill(bgCol);
      const progressAnimBaseSize = this.renderSize + 5;
      rect(
        this.x - progressAnimBaseSize / 2,
        this.y -
          progressAnimBaseSize / 2 +
          progressAnimBaseSize * this.animationProgress,
        progressAnimBaseSize,
        progressAnimBaseSize * (1 - this.animationProgress),
      );
    }
    if (this.animationProgress >= 0.9) {
      this.edges
        .sort((a, b) => a.color() - b.color())
        .forEach((e, i) => {
          e.startAnimation((i / this.edges.length) ** 2 * 1000);
        });
    }
  };
}

class Edge {
  nodeT;
  nodeB;
  animX = 0;
  animY = 0;
  animationStart = Number.MAX_VALUE;
  animationProgress = 0;
  isStatic = false;
  weight = 0;

  constructor(nodeT, nodeB) {
    this.nodeT = nodeT;
    this.nodeB = nodeB;
    this.weight = 2 + nodeB.relSize * 1;
  }

  color = () => this.nodeB.color;

  startAnimation = (offset = 0) => {
    if (this.animationStart == Number.MAX_VALUE)
      this.animationStart = globalTime + offset;
  };

  render = () => {
    if (globalTime - this.animationStart < 0) return;

    if (!this.isStatic) {
      this.animationProgress = animEase(
        (globalTime - this.animationStart) / 1000,
      );
      if (this.animationProgress >= 1) {
        this.isStatic = true;
        this.animX = this.nodeB.xT;
        this.animY = this.nodeB.yT;
      } else {
        this.animX = bezierPoint(
          this.nodeT.xB,
          this.nodeT.xB,
          this.nodeB.xT,
          this.nodeB.xT,
          this.animationProgress,
        );

        this.animY = bezierPoint(
          this.nodeT.yB,
          (this.nodeT.yB + this.nodeB.yT) / 2,
          (this.nodeT.yB + this.nodeB.yT) / 2,
          this.nodeB.yT,
          this.animationProgress,
        );
      }
    }
    if (this.animationProgress >= 0.97) {
      this.nodeB.startAnimation();
    }

    strokeWeight(this.weight);
    noFill();
    stroke(
      lerpColor(color(bgCol), color(accentCol), this.nodeB.relSize * 1 + 0.7),
    );
    bezier(
      this.nodeT.xB,
      this.nodeT.yB,
      this.nodeT.xB,
      (this.nodeT.yB + this.nodeB.yT) / 2,
      this.animX,
      (this.nodeT.yB + this.nodeB.yT) / 2,
      this.animX,
      this.animY,
    );
  };
}

draw = () => {
  cursor(ARROW);
  frameRate(120);
  if (!firstFrameTime && frameCount <= 1) {
    firstFrameTime = millis();
  }
  // ---- update global animation state ----
  const initialSpeedScalingEaseIO =
    (cos(min((millis() - firstFrameTime) / 8000, 1.0) * PI) + 1) / 2;
  const initialSpeedScalingEase =
    (cos(min((millis() - firstFrameTime) / 8000, 1.0) ** (1 / 2) * PI) + 1) / 2;
  const initAnimationSpeedFactor = 1.0 - 0.4 * initialSpeedScalingEaseIO;
  // update global scaling-aware clock
  globalTime += globalAnimSpeed * initAnimationSpeedFactor * deltaTime;

  if (nodes.length == 0) {
    const spacingHeight = height * 1.3;
    const spacingWidth = width * 1.3;
    treeStructData.layout.forEach((lay, index) => {
      new Node(
        spacingWidth * lay[0] - spacingWidth / 2,
        20 + spacingHeight * lay[1] - spacingHeight / 2,
        1 - treeStructData.metrics[index],
        index,
      );
    });
    treeStructData.edges.forEach((ind) => {
      nodes[ind[0]].child(nodes[ind[1]]);
    });
    nodes.forEach((n) => {
      if (n.isRootNode) n.startAnimation();
    });
    nodes[0].selected = true;
    setCodeAndPlan(
      treeStructData.code[0],
      treeStructData.plan[0],
    )
  }

  const staticNodes = nodes.filter(
    (n) => n.isStatic || n.animationProgress >= 0.7,
  );
  if (staticNodes.length > 0) {
    const largestNode = staticNodes.reduce((prev, current) =>
      prev.relSize > current.relSize ? prev : current,
    );
    if (!manualSelection) {
      if (!largestNode.selected) {
        setCodeAndPlan(
          treeStructData.code[largestNode.treeInd],
          treeStructData.plan[largestNode.treeInd],
        );
      }
      staticNodes.forEach((node) => {
        node.selected = node === largestNode;
      });
    }
  }
  background(bgCol);
  // global animation transforms
  translate(width / 2, height / 2);
  scale(scaleFactor);

  
  // ---- fg render ----
  edges.forEach((e) => e.render());
  nodes.forEach((n) => n.render());
  
};

    </script>
    <title>AIDE Run Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
        overflow: scroll;
      }
      body {
        background-color: #f2f0e7;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 40vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
    </style>
  </head>
  <body>
    <pre
      id="text-container"
    ><div id="plan"></div><hr><code id="code" class="language-python"></code></pre>
  </body>
</html>
